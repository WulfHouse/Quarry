DOCUMENT HISTORY:
- First completed: December 18th, 2025
- Last updated: December 20th, 2025

This document is a HIGH-LEVEL MASTER SPECIFICATION OVERVIEW and HYPOTHETICAL
WISHLIST for what the Pyrite programming language and Quarry development suite
are intended to eventually become.

================================================================================
                  PYRITE PROGRAMMING LANGUAGE SPECIFICATION
================================================================================

1. DESIGN GOALS AND PHILOSOPHY
================================================================================

Pyrite is a compiled systems programming language designed to combine the 
low-level power and performance of C with the readability and ease-of-use of 
Python, while integrating the safety of Rust and the simplicity of Zig. 

The core design goals of Pyrite are outlined below:

1.1 Simplicity and Minimalism by Default
--------------------------------------------------------------------------------

Pyrite's syntax and feature set are kept minimal and straightforward. A beginner 
should find Pyrite easy to learn and read, akin to Python. Advanced features are 
opt-in rather than mandatory, meaning they introduce no complexity or cost unless 
explicitly used. 

This follows the Python philosophy that "Explicit is better than implicit" and 
"Simple is better than complex". In practice, Pyrite favors clear, transparent 
constructs over "magic" or implicit behavior. There are no hidden surprises - 
what the code looks like is exactly what it will do when executed.

1.2 C-Level Performance, Zero Runtime Overhead
--------------------------------------------------------------------------------

Pyrite programs compile to efficient native machine code with performance on par 
with C. Every high-level construct in the language is designed as a zero-cost 
abstraction, compiling down to equivalent low-level code with no extra overhead 
beyond what a skilled C programmer might write. 

There is no heavyweight runtime or VM: no global interpreter loop, no JIT 
compilation, and no stop-the-world garbage collector. Pyrite has a minimal 
runtime footprint suitable for resource-constrained environments. Features that 
involve dynamic memory allocation or other expensive operations are never 
implicit - they only occur if the programmer explicitly uses them. If you don't 
use a feature, it imposes zero cost on the binary. 

This approach is validated by other modern languages: for example, Rust programs 
achieve memory-safe, high-level abstractions without sacrificing speed or 
introducing a GC, and emerging languages like Mojo have demonstrated that it is 
possible to achieve performance comparable to C++/Rust even with a high-level, 
Pythonic syntax by relying on static typing and compile-time optimization.

1.3 Memory Safety by Default (Manual Control Optional)
--------------------------------------------------------------------------------

A primary objective of Pyrite is to eliminate common memory bugs (buffer 
overflows, use-after-free, null pointer dereferences, data races, etc.) at 
compile time, without sacrificing performance. By default, Pyrite ensures memory 
safety through strict compile-time checks inspired by Rust's ownership model, so 
that well-typed Pyrite code cannot perform invalid memory accesses. 

In other words, safe Pyrite code is memory-safe and data race-free by 
construction, similar to Rust's guarantees. However, unlike purely managed 
languages, Pyrite also permits manual memory management when needed. Developers 
can drop down to low-level pointer manipulation or explicit allocate/free control 
for specialized use cases, but such code must be marked as unsafe or use special 
APIs. 

This provides an "escape hatch" for power users (much like Rust's unsafe blocks), 
while keeping safe code free of memory errors. In short, safe by default, unsafe 
by choice.

1.4 Pythonic, Readable Syntax
--------------------------------------------------------------------------------

Pyrite's syntax is heavily influenced by Python to lower the barrier to entry for 
beginners. Code uses indentation for blocks (significant whitespace) instead of 
curly braces or heavy punctuation. Keywords and control flow structures read like 
English. 

The goal is that someone with minimal coding experience can quickly grasp Pyrite 
code structure. For example, function definitions, loops, and conditionals use a 
clean, uncluttered syntax similar to Python. This emphasis on readability follows 
the Zen of Python's maxim that "Readability counts." At the same time, the syntax 
has been adapted to a statically-typed, compiled context in a way that feels 
natural and intuitive for systems programming.

1.5 Intuitive Memory Model for Learners
--------------------------------------------------------------------------------

Pyrite is designed to make low-level concepts like memory allocation, lifetimes, 
and data structure performance characteristics as transparent as possible, so 
that even beginners can understand what the code is doing under the hood. 

It should be apparent to the programmer whether a given data type or variable is 
allocated on the stack or on the heap, and what the implications are for 
performance. For instance, the language makes a clear distinction between value 
types allocated on the stack vs. heap-allocated objects, using syntax and 
semantics that convey the difference. 

Expensive operations (like copying a large structure or growing a dynamic array) 
are not hidden - they require an explicit action or are documented, so a learner 
can reason about cost. This explicitness echoes Python's clarity and Zig's 
philosophy of not hiding costly operations from the programmer. 

By designing the language in this way, even developers new to systems programming 
can intuitively learn how memory management works (whether via an ownership 
system or via manual malloc/free), and can develop a performance intuition from 
the start.

1.6 Complete Systems Programming Capabilities
--------------------------------------------------------------------------------

Pyrite is intended as a "do-anything" systems language. Anything you can do in 
C, you can do in Pyrite. This includes low-level hardware manipulation in 
embedded systems and OS kernels, as well as high-level application, game engine, 
and web server programming. 

The language imposes no runtime or library requirements that would hinder writing 
an OS kernel or interfacing directly with memory and devices. You can write 
bare-metal code with no operating system at all, or, at the other extreme, use 
Pyrite for high-level scripting and application development. 

This versatility is a key goal: Pyrite is meant to scale from microcontrollers to 
large distributed applications. Interfacing with existing C/C++ code is 
straightforward - Pyrite provides a foreign function interface (FFI) to call C 
functions directly, and Pyrite's compiler can produce binaries that conform to C 
ABI conventions for easy linking. 

In practice, this means you can gradually adopt Pyrite in existing C/C++ 
projects, or use Pyrite as a safer alternative for new modules, with minimal 
friction. (Interoperability is considered so important that Pyrite is designed to 
make conforming to the C ABI straightforward, similar to Zig's approach of 
treating C interop as a first-class requirement.)

1.7 Modern Features, Optional Complexity
--------------------------------------------------------------------------------

While keeping the core language simple, Pyrite doesn't shy away from powerful 
features that improve safety or developer ergonomics - it simply makes them 
optional. This includes things like generics (parametric polymorphism), algebraic 
data types and pattern matching, compile-time code execution (for metaprogramming 
and optimization), a built-in package/module system, and advanced concurrency 
primitives. 

These features are inspired by the successes of Rust and Zig: for example, Rust's 
trait system and pattern matching, or Zig's compile-time evaluation and explicit 
allocator model. Such features are available for use when needed to write 
high-level abstractions or libraries, but the average user can start with a much 
simpler subset of the language. 

Crucially, no feature is "magic" - each is designed to have predictable cost and 
behavior, aligning with the principle of explicitness. For instance, there are no 
implicit class destructors or unexpected operator overloads that hide control 
flow or allocate memory behind the scenes. If something significant (like 
allocation or locking) is happening, the code makes it obvious. 

This means the language can grow with the programmer: beginners can stick to the 
simple core (like variables, loops, and basic structs), while experts can opt-in 
to advanced patterns without runtime overhead. 

By adhering to these principles, Pyrite aims to be a language that developers 
truly enjoy using. It strives to be as approachable and fun as Python, as safe 
and robust as Rust, and as lightweight and transparent as Zig. In spirit, Pyrite 
tries to embody what developers admire most in those languages - for example, 
Rust's strong safety guarantees without needing a garbage collector, Python's 
elegant syntax and philosophy, and Zig's emphasis on explicit control - in one 
unified toolset.

1.8 Core Language Subset for Learning
--------------------------------------------------------------------------------

To support Pyrite's goal of being approachable to beginners while remaining 
powerful for experts, the language defines a semantic "Core" subset. This is NOT 
a separate syntax or dialect, but rather a well-defined subset of features that:

â€¢ Provides a complete, practical programming environment for learners
â€¢ Compiles to identical machine code as full Pyrite (no performance penalty)
â€¢ Desugars into full Pyrite semantics (zero runtime differences)
â€¢ Can be enforced via tooling (linter modes, compiler flags) rather than 
  language fragmentation

The Core subset philosophy:
  - Forbids or warns about advanced features: unsafe blocks, manual allocators, 
    complex lifetime annotations, advanced generic patterns
  - Includes all fundamental features: basic types, structs, enums, pattern 
    matching, ownership/borrowing basics, standard collections, defer, with
  - Enables real systems programming: Core code is production-ready, not a toy
  - Grows with the programmer: Code written in Core remains valid as developers 
    adopt advanced features

This is implemented through:
  â€¢ Compiler mode: pyritec --core-only rejects advanced features
  â€¢ Linter levels: quarry lint --beginner warns about complexity
  â€¢ Standard library tiers: core:: namespace vs std:: full library
  â€¢ Documentation paths: "Core Pyrite" learning track vs full language reference

By keeping Core as a semantic subset rather than syntactic variation, developers 
learn "real Pyrite" from day one, and all code remains compatible as skills 
advance. This avoids ecosystem fragmentation while maintaining approachability.

1.9 Target Audiences
--------------------------------------------------------------------------------

Pyrite is optimized primarily for **Python-first beginners with systems 
programming aspirations**, with a strong secondary audience of **Rust-curious 
developers seeking an easier path** to systems programming.

Primary Audience: Python Beginners
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

These developers know Python's syntax and philosophy, but need:
  â€¢ C-level performance without C-level complexity
  â€¢ Understanding of memory management and performance
  â€¢ Type safety and compile-time error catching
  â€¢ Ability to write OS kernels, embedded systems, game engines

Pyrite's Pythonic syntax, explicit design, and teaching compiler make systems 
concepts accessible to this audience.

Secondary Audience: Rust-Curious Developers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Developers who bounced off Rust's learning curve but appreciate its goals:
  â€¢ Already understand the value of ownership and memory safety
  â€¢ Found Rust's syntax or borrow checker too complex initially
  â€¢ Want the same safety guarantees with gentler onboarding
  â€¢ Appreciate Rust's performance but need faster productivity

Pyrite offers the same safety model with more familiar syntax and better 
pedagogical tooling (auto-fix, visual diagrams, explicit-by-default).

Design Tradeoff Philosophy
~~~~~~~~~~~~~~~~~~~~~~~~~~~

When design decisions conflict between audiences, favor beginners. However, 
never alienate Rust-familiar developers:
  â€¢ Keep &str working alongside Text alias
  â€¢ Ownership model identical to Rust (familiar to experts)
  â€¢ Advanced features available but optional (growth path)

This ensures beginners get gentle onboarding while experts don't feel 
the language has been oversimplified at their expense.

1.10 Ultimate Vision
--------------------------------------------------------------------------------

Pyrite's ultimate goal is to become one of the most widely adopted and desired 
programming languages of its era - the kind of language that consistently ranks 
highly in developer surveys for satisfaction and preference. This goal underpins its 
design: by blending power and simplicity in a distinctive way, Pyrite aims to attract 
both low-level systems programmers and high-level application developers. 

(Notably, the Rust language has ranked as the most "loved" language on the Stack 
Overflow survey for multiple years, largely due to its combination of performance 
and safety. Pyrite seeks to match or exceed that level of developer admiration by 
offering similar benefits with an easier learning curve and more familiar syntax.)

The key to achieving widespread developer adoption lies not just in language features, 
but in the complete developer experience: world-class compiler diagnostics 
(including auto-fix and visual learning tools), frictionless tooling (Quarry 
build system with cost analysis), comprehensive standard library, and a thriving 
ecosystem. These elements transform Pyrite from merely a good language into one 
developers actively love using and recommend to others.

The following sections provide a thorough specification of Pyrite's syntax, 
semantics, and features - detailed enough to serve as a foundation for 
implementing a compiler and using the language effectively.

1.11 Unique Differentiators: What Makes Pyrite Special
--------------------------------------------------------------------------------

Pyrite combines proven ideas from multiple languages, but adds distinctive features 
that few other systems languages provide in combination. This section highlights the 
distinctive capabilities that position Pyrite to achieve widespread developer adoption.

Features No Other Systems Language Has
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. **Interactive REPL with Ownership Visualization (Section 8.7)**
   - Real-time ownership state displayed as you type
   - :cost, :type, :ownership commands for exploration
   - Makes abstract concepts tangible through direct interaction
   - Critical gap: Rust has third-party REPL, C/C++/Zig have none with ownership viz

2. **Energy Profiling Built-In (Section 8.20)**
   - quarry energy shows power consumption and battery impact
   - Optimize for sustainability and battery life
   - Few other systems languages provide built-in energy profiling
   - Growing importance: green software, mobile, IoT

3. **Two-Tier Closure Model with Explicit Syntax (Section 7.5)**
   - fn[...] = compile-time (zero-cost) vs fn(...) = runtime (may allocate)
   - Visual syntax makes cost explicit, not optimization-dependent
   - Enables verifiable --no-alloc mode
   - Mojo has similar concept but less explicit syntax

4. **Call-Graph Blame Tracking for Performance Contracts (Section 4.5)**
   - @noalloc violations show complete call chain
   - "Function A called B called C which allocated"
   - Actionable fixes at every level
   - Unique: performance contracts that compose across boundaries

5. **Community Transparency Dashboard (Section 8.25)**
   - Public real-time metrics: performance, safety, learning, adoption
   - Makes developer adoption metrics measurable, not just aspirational
   - Evidence-based advocacy (show data, not claims)
   - No competitor has comprehensive public metrics

6. **Internationalized Compiler Errors (Section 2.7)**
   - Native language diagnostics (Chinese, Spanish, Hindi, Japanese, etc.)
   - Professional translations, not machine translation
   - Critical for global adoption (60% non-native English)
   - Almost no compilers do this well

7. **Performance Lockfile with Regression Root Cause (Section 8.13)**
   - Perf.lock commits performance baseline to version control
   - CI fails on regression with assembly diff and root cause
   - "SIMD width changed from 8 to 4 due to alignment"
   - Prevents "death by 1000 cuts" performance decay

8. **Design by Contract Integrated with Ownership (Section 7.3)**
   - @requires, @ensures, @invariant for logical correctness
   - Compose with ownership (memory safety) and @cost_budget (performance)
   - First systems language with ownership + contracts + performance contracts
   - Enables highest safety certification levels

Features That Match Best-in-Class (Competitive Parity)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  â€¢ Memory safety (equal to Rust)
  â€¢ Zero-cost abstractions (equal to Rust/C++)
  â€¢ Compile-time evaluation (equal to Zig)
  â€¢ Borrow checking (equal to Rust, better error messages)
  â€¢ Cross-compilation (equal to Zig)
  â€¢ Package manager (equal to Cargo)
  â€¢ Fuzzing + sanitizers (equal to Rust/Go)
  â€¢ Supply-chain security (equal to or better than Rust cargo-vet)

Features That Exceed Competitors
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  â€¢ **Learning curve:** Gentler than Rust (Pythonic syntax, interactive tools)
  â€¢ **Error messages:** More visual than Rust (ownership flow diagrams)
  â€¢ **Cost transparency:** More explicit than Zig (multi-level reporting)
  â€¢ **Tooling integration:** More comprehensive than any competitor (50+ features)
  â€¢ **Binary size tools:** Equal to Rust cargo bloat (critical for embedded)
  â€¢ **Deterministic builds:** Equal to Rust (essential for security)
  â€¢ **Formal semantics:** Better than Rust (mathematical specification)

The "Most Admired" Formula
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Most languages excel in 2-3 dimensions. Pyrite excels in 8:

  1. **Safety** â†’ Ownership + contracts + formal verification
  2. **Performance** â†’ Zero-cost + profiling + optimization + energy
  3. **Learning** â†’ REPL + Playground + exercises + visual errors + native language
  4. **Productivity** â†’ Incremental builds + hot reload + auto-fix + great diagnostics
  5. **Transparency** â†’ Cost + size + energy + public metrics
  6. **Security** â†’ Audit + vet + sign + reproducible + formal methods
  7. **Production** â†’ Observability + testing + debugging + certification
  8. **Global** â†’ Internationalization + accessibility + evidence

This represents the difference between a good language and one that achieves 
widespread developer adoption and satisfaction.

**Complete, not fragmented.** No external tools required. No complex configuration 
requirements. No need to piece together solutions from multiple sources. Everything 
is integrated, consistent, and well-designed.

This approach to improving developer experience focuses on removing every friction 
point, delivering every expected feature, and adding unique capabilities that 
competitors may not provide. Pyrite aims to match or exceed the best systems 
languages while being more accessible.

Quick Feature Comparison Matrix
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

| Feature                        | Pyrite | Rust | Zig | C/C++ | Go  | Mojo |
|--------------------------------|--------|------|-----|-------|-----|------|
| **Core Language**              |        |      |     |       |     |      |
| Memory safety by default       | Y      | Y    | N   | N     | Y   | Y    |
| Zero-cost abstractions         | Y      | Y    | Y   | Y     | N   | Y    |
| No garbage collector           | Y      | Y    | Y   | Y     | N   | N    |
| No runtime overhead            | Y      | Y    | Y   | Y     | N   | N    |
| Pythonic syntax                | Y      | N    | N   | N     | P   | Y    |
| **Learning & Productivity**    |        |      |     |       |     |      |
| Interactive REPL               | Y      | 3rd  | N   | N     | N   | Y    |
| Ownership visualization        | Y      | N    | N/A | N/A   | N/A | N    |
| Interactive exercises          | Y      | 3rd  | N   | N     | N   | N    |
| Multilingual errors            | Y      | P    | N   | N     | N   | N    |
| Auto-fix suggestions           | Y      | Y    | N   | N     | Y   | N    |
| **Performance & Analysis**     |        |      |     |       |     |      |
| Cost transparency              | Y      | P    | Y   | N     | N   | P    |
| Binary size profiling          | Y      | Y    | Y   | N     | N   | N    |
| Energy profiling               | Y      | N    | N   | N     | N   | N    |
| Performance lockfile           | Y      | N    | N   | N     | N   | N    |
| Call-graph blame tracking      | Y      | N    | N   | N     | N   | N    |
| Incremental compilation        | Y      | Y    | P   | P     | Y   | Y    |
| **Security & Verification**    |        |      |     |       |     |      |
| Deterministic builds           | Y      | Y    | Y   | N     | P   | N    |
| Supply-chain security          | Y      | Y    | N   | N     | P   | N    |
| Fuzzing built-in               | Y      | Y    | N   | N     | Y   | N    |
| Sanitizers integrated          | Y      | Y    | P   | 3rd   | Y   | N    |
| Design by Contract             | Y      | N    | N   | N     | N   | N    |
| Formal semantics               | Y      | P    | N   | Y     | N   | N    |
| **Production & Deployment**    |        |      |     |       |     |      |
| Built-in observability         | Y      | 3rd  | N   | N     | Y   | N    |
| Hot reloading                  | Y      | 3rd  | N   | N     | N   | N    |
| Cross-compilation              | Y      | Y    | Y   | P     | Y   | N    |
| No-alloc verification          | Y      | P    | N   | N     | N   | N    |
| **Ecosystem & Community**      |        |      |     |       |     |      |
| Official package manager       | Y      | Y    | P   | N     | Y   | P    |
| Metrics dashboard              | Y      | N    | N   | N     | N   | N    |
| License compliance             | Y      | 3rd  | N   | N     | 3rd | N    |
| Dead code analysis             | Y      | 3rd  | N   | N     | N   | N    |

**Legend:**
  â€¢ Y = Built-in first-class feature (Yes)
  â€¢ 3rd = Available through third-party tools
  â€¢ P = Partial support or limited implementation
  â€¢ N = Not available or not applicable (No)
  â€¢ N/A = Concept doesn't apply (no ownership system)

**Unique Features (8):** REPL with ownership viz, energy profiling, performance 
lockfile, call-graph blame, contracts, formal semantics (comprehensive), 
community dashboard, internationalized errors (comprehensive)

**Best-in-Class (5):** Memory safety, supply-chain security, cost transparency, 
compiler diagnostics, binary size tools

**Result:** 8 distinctive features + 5 best-in-class capabilities + Pythonic syntax 
= comprehensive differentiation strategy

1.12 Getting Started: Developer Journey
--------------------------------------------------------------------------------

To illustrate how all these features work together, here's the typical developer 
journey from first install to production deployment.

Day 1: Installation and First Program
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    # Install Pyrite (single command)
    $ curl --sSf https://raw.githubusercontent.com/WulfHouse/Quarry/main/scripts/setup/install.sh | sh
    
    # Write first program (hello.pyrite)
    fn main():
        print("Hello, Pyrite!")
    
    # Run immediately (script mode, zero config)
    $ pyrite run hello.pyrite
    Hello, Pyrite!
    
    # Or use REPL for exploration
    $ pyrite repl
    >>> let x = 5 + 3
    8
    >>> let data = List[int]([1, 2, 3])
    [Heap] [Move] Stack: 24B, Heap: 12B

Week 1: Learning Ownership Interactively
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    # Start interactive learning
    $ quarry learn ownership
    
    Exercise: Fix the ownership error...
    [Work through 12 exercises with progressive hints]
    
    # Practice in REPL
    $ pyrite repl --explain
    >>> let data = List[int]([1, 2, 3])
    >>> process(data)
    >>> data.length()  # See instant ownership error with visualization

Week 2: First Real Project
~~~~~~~~~~~~~~~~~~~~~~~~~~~

    # Create project
    $ quarry new my-cli-tool
    $ cd my-cli-tool
    
    # Write code, hit ownership error
    $ quarry build
    error[P0234]: cannot use moved value 'config'
    
    # Interactive fix
    $ quarry fix --interactive
    Select a fix:
      1. Pass a reference (recommended)
      2. Clone the value
      3. Restructure to return ownership
    Choice: 1
    
    âœ“ Fixed! Run 'quarry build' to verify.

Week 3: Performance Optimization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    # Profile the code
    $ quarry cost
    Warning: List allocated in loop (1000 times)
    
    $ quarry perf
    Hot spot: parse_data (34% of runtime)
    
    $ quarry tune
    Suggestions:
      1. Pre-allocate list with_capacity(1000) â†’ 15% speedup
    [Apply] âœ“ Applied
    
    $ quarry perf --check
    âœ“ 15.3% faster than baseline

Month 2: Production Deployment
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    # Add dependencies
    $ quarry add http-server json
    
    # Write server with observability
    import std::http
    import std::log
    
    fn main():
        log::info("server_starting", {"port": 8080})
        let server = HttpServer::new("0.0.0.0:8080", handler)
        server.run()
    
    # Security audit
    $ quarry audit
    âœ“ No vulnerabilities
    
    # Build for production
    $ quarry build --release --lto --pgo=bench
    
    # Generate SBOM for compliance
    $ quarry sbom --format=spdx
    
    # Deploy!

Month 6: Embedded Project
~~~~~~~~~~~~~~~~~~~~~~~~~~

    # Target embedded device
    $ quarry new --embedded stm32-firmware
    $ cd stm32-firmware
    
    # Build with no-alloc verification
    $ quarry build --target=thumbv7em-none-eabi --no-alloc
    
    # Check binary size
    $ quarry bloat
    Total: 47 KB (73% of 64 KB flash)
    
    # Optimize
    $ quarry build --optimize=size --strip-all
    Total: 32 KB (50% of flash) âœ“
    
    # Flash to device
    $ quarry flash --target=stm32f4

Year 1: Contributing to Ecosystem
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    # Publish your library
    $ quarry publish my-awesome-lib
    
    # Track adoption on dashboard
    <!-- Aspirational: does not exist yet -->
    https://quarry.dev/packages/my-awesome-lib
    Downloads: 1,247 this month
    
    # See your impact on metrics
    <!-- Aspirational: does not exist yet -->
    https://quarry.dev/metrics
    Ecosystem: 15,247 packages (+1 yours!)

This Journey Demonstrates
~~~~~~~~~~~~~~~~~~~~~~~~~~

  â€¢ **Day 1 productivity:** Write and run code immediately (script mode + REPL)
  â€¢ **Week 1 learning:** Interactive tools teach ownership organically
  â€¢ **Week 2 building:** Auto-fix removes friction, quarry just works
  â€¢ **Week 3 optimizing:** Profiling + tuning make performance systematic
  â€¢ **Month 2 production:** All production features built-in (observability, 
    security, compliance)
  â€¢ **Month 6 embedded:** Same language, different constraints, still safe
  â€¢ **Year 1 ecosystem:** Contribute back, see impact, feel part of community

Every stage is supported by first-class tooling. No external dependencies, no 
complex configuration requirements, no need to piece together solutions from 
multiple sources. This approach focuses on removing every friction point from 
beginner to expert to contributor.

================================================================================
2. COMPILER DIAGNOSTICS AND ERROR MESSAGES
================================================================================

One of Pyrite's highest-priority design goals is to make the compiler a teacher, 
not just an error reporter. Exceptional compiler diagnostics are the primary 
mechanism for delivering on Pyrite's promise of transparency and approachability.

Research and developer surveys consistently show that compiler error quality is 
among the top factors in language satisfaction. Rust's success is strongly tied 
to rustc's famous diagnostics, which teach ownership and borrowing through clear, 
actionable error messages.

2.1 Error Message Design Principles
--------------------------------------------------------------------------------

Every Pyrite compiler error follows a structured format:

1. WHAT HAPPENED: Clear statement of the error
2. WHY IT'S A PROBLEM: Explanation of the underlying issue
3. WHAT TO DO NEXT: Concrete suggestions for fixes (often multiple options)
4. LOCATION CONTEXT: Precise source highlighting with multi-line context

Example error format:

    error[P0234]: cannot use moved value 'data'
      ----> main.py:15:10
       |
    12 |     let data = List[int]([1, 2, 3])
       |         -------- value allocated here
    13 |     process(data)
       |             -------- value moved here (ownership transferred to 'process')
    14 |     # ... other code ...
    15 |     print(data.length())
       |          ^^^^ cannot use 'data' after it was moved
       |
       = note: 'data' was moved on line 13 when passed to 'process'
       = help: if you want to use 'data' again, consider:
               1. Pass a reference instead: process(&data)
               2. Clone the data: process(data.clone())
               3. Have 'process' return ownership after using it
       = explain: Run 'pyritec --explain P0234' for detailed explanation

2.2 Ownership and Borrowing Diagnostics
--------------------------------------------------------------------------------

Because ownership and borrowing are novel concepts for many programmers, Pyrite 
provides specialized diagnostics with timeline visualizations:

Borrow conflict example:

    error[P0502]: cannot borrow 'config' as mutable while also borrowed as immutable
      ----> main.py:23:5
       |
    20 |     let reader = &config           # immutable borrow starts
       |                  -------------- immutable borrow occurs here
    21 |     
    22 |     print(reader.get_value())      # immutable borrow used
    23 |     config.update("key", "value")  # mutable borrow attempted
       |     ^^^^^^ mutable borrow occurs here
    24 |     
    25 |     print(reader.get_value())      # immutable borrow used again
       |           ------------ immutable borrow later used here
       |
       = Timeline of borrows:
         Line 20: 'config' immutably borrowed â†’ stored in 'reader'
         Line 23: Attempted to mutably borrow 'config' (CONFLICT)
         Line 25: 'reader' still needs immutable access
       
       = Ownership flow visualization:
         
         config (owner)
           â”‚
           â”œâ”€[L20]â”€â†’ &config â”€â†’ reader (immutable borrow active)
           â”‚                      â”‚
           â”‚                      â”‚ (borrow still alive)
           â”‚                      â”‚
           â”œâ”€[L23]â”€â†’ &mut config â”€â”€X ERROR: conflicts with existing borrow
           â”‚
           â””â”€[L25]â”€â†’ reader.get_value() (uses immutable borrow)
       
       = help: The compiler cannot allow mutable access while 'reader' exists
               because 'reader' expects 'config' to remain unchanged.
               
               Possible fixes:
               1. Complete all reads before modifying:
                  print(reader.get_value())
                  drop(reader)  # or let it go out of scope
                  config.update("key", "value")
               
               2. Restructure to avoid overlapping borrows
       
       = explain: Run 'pyritec --explain P0502' for borrowing rules
       = visual: Run 'pyritec --explain P0502 --visual' for interactive diagram

2.3 Memory and Performance Diagnostics
--------------------------------------------------------------------------------

Pyrite's cost-transparency philosophy extends to compiler warnings about 
expensive operations:

Allocation warning example:

    warning[P1050]: heap allocation in loop body
      ----> main.py:45:13
       |
    43 |     for i in 0..1000:
    44 |         var text = String.new()  # allocated on heap
    45 |         text.push_str("item ")   # potential reallocation
       |              ^^^^^^^^ string may reallocate here
    46 |         text.push_str(i.to_string())  # likely reallocation
       |              ^^^^^^^^ string may reallocate here
       |
       = performance: This loop may allocate 1000+ times
       = help: Consider allocating once before the loop:
               
               var text = String.with_capacity(50)
               for i in 0..1000:
                   text.clear()
                   text.push_str("item ")
                   text.push_str(i.to_string())
       
       = note: Use #[allow(heap_in_loop)] to suppress this warning

Copy warning example:

    warning[P1051]: large value copied implicitly
      ----> main.py:78:18
       |
    76 |     struct ImageBuffer:
    77 |         data: [u8; 1_000_000]  # 1 MB fixed-size array
    78 |     
    79 |     fn process(img: ImageBuffer):  # takes ownership by copy
       |                     ^^^^^^^^^^^ 1 MB copied on each call
       |
       = performance: ImageBuffer is 1,000,000 bytes and will be copied when 
                      passed by value
       = help: Consider taking a reference instead:
               
               fn process(img: &ImageBuffer):  # borrows, no copy
               
               Or if mutation is needed:
               
               fn process(img: &mut ImageBuffer):

2.4 Explain System
--------------------------------------------------------------------------------

Every compiler error and warning has a unique error code (e.g., P0234, P1050) 
that maps to detailed explanations accessible via:

    pyritec --explain P0234

This displays:

â€¢ Full conceptual explanation of the issue
â€¢ Multiple code examples (both incorrect and correct versions)
â€¢ Link to relevant language documentation chapter
â€¢ Common pitfalls and patterns
â€¢ Performance implications (if applicable)

The explain system serves as interactive documentation, teaching language 
concepts at the moment developers encounter problems.

Enhanced Visual Mode
~~~~~~~~~~~~~~~~~~~~~

For ownership and borrowing errors, which are the hardest concepts for 
newcomers, Pyrite offers enhanced visualizations:

    pyritec --explain P0502 --visual

This generates an interactive diagram showing:

1. **Ownership timeline:** When values are created, moved, borrowed, and dropped
2. **Borrow scope visualization:** Start and end points of each borrow
3. **Conflict points:** Exactly where and why incompatible accesses occur
4. **Data flow graph:** How ownership flows through function calls

Example visual output (ASCII art in terminal, interactive in IDE):

    Ownership Flow for 'data'
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Line 12: let data = List[int]([1,2,3])
             â”Œâ”€â”€â”€â”€â”€â”
             â”‚data â”‚ â† OWNER
             â””â”€â”€â”€â”€â”€â”˜
                â”‚
                â”‚ (owns heap allocation)
                â”‚
    Line 13: process(data)
                â”‚
                â””â”€â”€â†’ MOVED to process() parameter
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚process()â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ (now owns the list)
                         â”‚
                     [list freed here when process() returns]
    
    Line 15: print(data.length())
             â”Œâ”€â”€â”€â”€â”€â”
             â”‚data â”‚ â† INVALID (moved away on line 13)
             â””â”€â”€â”€â”€â”€â”˜
                â”‚
                X ERROR: Cannot use moved value

    Solution paths:
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Path 1: Clone before moving
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Line 13: process(data.clone())  â† data remains valid
    
    Path 2: Use a reference
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Line 13: process(&data)  â† borrow instead of move
    
    Path 3: Get ownership back
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Line 13: data = process(data)  â† function returns ownership

For complex scenarios with multiple borrows:

    Borrow Conflict Analysis
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Variable: config
    
    Timeline:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Line 20: let reader = &config
             â”‚
             â”œâ”€â”€â†’ Immutable borrow starts (&config)
             â”‚    Stored in 'reader'
             â”‚
    Line 22: print(reader.get_value())
             â”‚    Uses immutable borrow âœ“
             â”‚
    Line 23: config.update("key", "value")
             â”‚
             X    Attempted mutable borrow (&mut config)
             â”‚    CONFLICT: Immutable borrow still active!
             â”‚
    Line 25: print(reader.get_value())
                  Uses immutable borrow (why it's still active)

    Conflict Resolution:
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    The mutable borrow on Line 23 cannot coexist with the
    immutable borrow in 'reader' because:
    
      â€¢ 'reader' expects 'config' to remain unchanged
      â€¢ Mutable access could invalidate 'reader's assumptions
      â€¢ Rust's/Pyrite's safety: no aliasing + mutation
    
    Fix: End the immutable borrow before mutating:
    
      let reader = &config
      print(reader.get_value())
      drop(reader)           â† Explicitly end borrow
      config.update(...)     â† Now safe to mutate

IDE Integration:
  â€¢ VSCode/IntelliJ plugins render interactive visualizations
  â€¢ Hover over variables to see lifetime spans
  â€¢ Click error codes to open visual explain mode
  â€¢ Animated flow for move/borrow operations

This visual enhancement transforms the hardest part of Pyrite (ownership) from 
"read text and imagine" to "see the flow directly." It's the difference between 
reading about chess moves vs. seeing the board.

2.5 IDE Hover: Ownership and Performance Metadata
--------------------------------------------------------------------------------

To make memory management and performance characteristics immediately visible, 
Pyrite's Language Server Protocol (LSP) implementation provides rich hover 
tooltips that show ownership state, memory layout, and cost implications.

Variable Hover Information
~~~~~~~~~~~~~~~~~~~~~~~~~~

Hovering over any variable shows:

    let data = List[int]([1, 2, 3])
         ^^^^
         Hover shows:
         
         Type: List[int]
         Badges: [Heap] [Move] [MayAlloc]
         
         Memory:
           â€¢ Stack: 24 bytes (ptr + len + cap)
           â€¢ Heap: 12 bytes (3 Ã— 4-byte integers)
           â€¢ Total: 36 bytes
         
         Ownership:
           â€¢ Owner: 'data' (owned value)
           â€¢ Moved: No
           â€¢ Borrowed: Not currently borrowed
         
         Next risks:
           âš ï¸  Passing to function will move (data becomes invalid)
           âœ“  Use &data to borrow instead

After a move:

    process(data)
    print(data.length())  # Error
          ^^^^
          Hover shows:
          
          Type: List[int] (MOVED)
          
          Ownership:
           â€¢ Owner: None (moved on line 5 to 'process')
           â€¢ Moved: Line 5, process(data)
           â€¢ Cannot be used
          
          Fix:
           â€¢ Option 1: Pass &data to borrow
           â€¢ Option 2: Clone: process(data.clone())
           â€¢ Run 'quarry fix' for automatic correction

Function Parameter Hover
~~~~~~~~~~~~~~~~~~~~~~~~~

Hovering over function signatures shows parameter behavior:

    fn process(list: List[int]) -> int:
                     ^^^^^^^^^^
                     Hover shows:
                     
                     Parameter: list
                     Type: List[int]
                     Ownership: Takes ownership (consumes)
                     
                     âš ï¸  Warning: This function takes ownership
                     Caller loses access to the value after call
                     
                     Consider: &List[int] to borrow instead

For reference parameters:

    fn sum(numbers: &[int]) -> int:
                    ^^^^^^
                    Hover shows:
                    
                    Parameter: numbers
                    Type: &[int] (borrowed slice)
                    Ownership: Borrows (read-only)
                    
                    âœ“  Caller retains ownership
                    âœ“  No allocation or copy
                    âœ“  Zero-cost abstraction

Performance Cost Hover
~~~~~~~~~~~~~~~~~~~~~~

Hovering over operations shows their performance characteristics:

    let copy = large_buffer
               ^^^^^^^^^^^^
               Hover shows:
               
               Operation: Copy
               Cost: 4096 bytes copied
               
               Type: ImageBuffer
               Size: 4 KB
               
               âš ï¸  Large copy warning
               Consider: Use reference (&large_buffer)
               Estimated impact: 4 KB memcpy (~500 cycles)

For allocations:

    let list = List[int].new()
               ^^^^^^^^^^^^^^^
               Hover shows:
               
               Operation: Heap allocation
               Cost: Initial capacity: 0 bytes
                     Growth: Will reallocate on first push
               
               ðŸ’¡ Tip: Use with_capacity(n) to avoid reallocation
               Example: List[int].with_capacity(10)

Borrow Conflict Context
~~~~~~~~~~~~~~~~~~~~~~~

When hovering in a borrow conflict scenario:

    let reader = &config
    config.update("key", "val")  # Error
    ^^^^^^
    Hover shows:
    
    Variable: config
    Type: Config
    
    Borrow Status:
      âš ï¸  Immutably borrowed by 'reader' (line 20)
      âš ï¸  Cannot mutate while borrowed
    
    Active borrows:
      â€¢ Line 20: &config â†’ reader (immutable)
      â€¢ Used on line 25: reader.get_value()
    
    To fix:
      â€¢ End 'reader' borrow before mutating
      â€¢ Use drop(reader) or let it go out of scope

Type Information Hover
~~~~~~~~~~~~~~~~~~~~~~

Hovering over type names shows memory characteristics:

    struct Point:
           ^^^^^
           Hover shows:
           
           Type: Point
           Badges: [Stack] [Copy] [ThreadSafe]
           
           Memory Layout:
             â€¢ Size: 8 bytes (2 Ã— i32)
             â€¢ Alignment: 4 bytes
             â€¢ Location: Stack (inline in structs)
           
           Behavior:
             â€¢ Copy: Cheap bitwise copy
             â€¢ Drop: No-op (no destructor)
             â€¢ Thread-safe: Yes (no shared state)
           
           Fields:
             â€¢ x: i32 (offset 0, 4 bytes)
             â€¢ y: i32 (offset 4, 4 bytes)
           
           Run 'quarry explain-type Point' for details

Integration with Cost Analysis
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

IDE hover information integrates with static cost analysis:

    for i in 0..1000:
        let v = List[int].new()  # In loop
                ^^^^^^^^^^^^^^^
                Hover shows:
                
                âš ï¸  Allocation in loop
                
                Static analysis:
                  â€¢ Called 1000 times
                  â€¢ 1000 heap allocations
                  â€¢ Estimated cost: ~24 KB
                
                Suggestion:
                  Move allocation before loop:
                  
                  let v = List[int].with_capacity(10)
                  for i in 0..1000:
                      v.clear()
                      # use v...

Configuration
~~~~~~~~~~~~~

IDE hover detail levels are configurable:

    # .vscode/settings.json or IDE config
    {
      "pyrite.hover.level": "intermediate",  # beginner/intermediate/advanced
      "pyrite.hover.showMemoryLayout": true,
      "pyrite.hover.showPerformanceCosts": true,
      "pyrite.hover.showOwnershipFlow": true
    }

Levels:
  â€¢ Beginner: Simple ownership state, basic warnings
  â€¢ Intermediate: Memory layout, costs, suggestions (default)
  â€¢ Advanced: Full call chains, assembly hints, cache implications

Why This Matters
~~~~~~~~~~~~~~~~~

IDE hover transforms abstract concepts into visible reality:

  â€¢ **Ownership becomes tangible:** See when values move, when borrows conflict
  â€¢ **Performance becomes predictable:** See costs before profiling
  â€¢ **Learning becomes interactive:** Immediate feedback while coding
  â€¢ **Debugging becomes faster:** Understand state without println debugging

Research suggests that visual feedback can accelerate learning of ownership concepts 
compared to text-only error messages (specific studies show improvements in the 40-60% 
range, though results vary by study methodology). Making ownership visible in real-time 
while coding (not just at compile time) addresses a significant gap in teaching systems 
programming.

Implementation: Beta Release, integrated with pyrite-lsp server. Requires coordination 
with static analyzer and quarry cost system to provide accurate information.

2.6 Diagnostic Quality Standards
--------------------------------------------------------------------------------

All compiler messages must meet these standards:

â€¢ Actionable: Suggest concrete fixes, not just describe the problem
â€¢ Contextual: Show enough source code to understand the issue
â€¢ Beginner-friendly: Avoid jargon or explain it inline
â€¢ Multi-solution: Present multiple approaches when applicable
â€¢ Consistent: Use uniform formatting and terminology
â€¢ Precise: Highlight exact problematic code spans

The compiler team maintains diagnostic quality as a first-class requirement, with 
dedicated review gates for error message clarity before feature acceptance.

2.7 Internationalized Error Messages (Stable Release)
--------------------------------------------------------------------------------

To achieve truly global adoption, Pyrite provides compiler
diagnostics in multiple languages, making systems programming accessible to 
non-native English speakers worldwide.

Command Usage
~~~~~~~~~~~~~

    pyritec --language=zh            # Chinese error messages
    pyritec --language=es            # Spanish error messages  
    pyritec --language=ja            # Japanese error messages
    pyritec --language=ko            # Korean error messages
    pyritec --language=pt            # Portuguese error messages
    pyritec --language=de            # German error messages
    pyritec --language=fr            # French error messages

Configuration:

    # .pyrite/config.toml or environment variable
    [compiler]
    language = "es"                  # Default to Spanish
    
    # Or environment variable
    export PYRITE_LANG=zh

Example Error Message (English):

    error[P0234]: cannot use moved value 'data'
      ----> main.py:15:10
       |
    12 |     let data = List[int]([1, 2, 3])
       |         -------- value allocated here
    13 |     process(data)
       |             -------- value moved here
    15 |     print(data.length())
       |          ^^^^ cannot use 'data' after it was moved
       |
       = help: Consider:
               1. Pass a reference: process(&data)
               2. Clone the data: process(data.clone())

Example Error Message (Chinese):

    é”™è¯¯[P0234]: æ— æ³•ä½¿ç”¨å·²ç§»åŠ¨çš„å€¼ 'data'
      ----> main.py:15:10
       |
    12 |     let data = List[int]([1, 2, 3])
       |         -------- å€¼åœ¨æ­¤åˆ†é…
    13 |     process(data)
       |             -------- å€¼åœ¨æ­¤ç§»åŠ¨
    15 |     print(data.length())
       |          ^^^^ å€¼ç§»åŠ¨åŽæ— æ³•ä½¿ç”¨ 'data'
       |
       = å¸®åŠ©: è€ƒè™‘ä»¥ä¸‹é€‰é¡¹:
               1. ä¼ é€’å¼•ç”¨: process(&data)
               2. å…‹éš†æ•°æ®: process(data.clone())

Example Error Message (Spanish):

    error[P0234]: no se puede usar el valor movido 'data'
      ----> main.py:15:10
       |
    12 |     let data = List[int]([1, 2, 3])
       |         -------- valor asignado aquÃ­
    13 |     process(data)
       |             -------- valor movido aquÃ­
    15 |     print(data.length())
       |          ^^^^ no se puede usar 'data' despuÃ©s de moverlo
       |
       = ayuda: Considera:
               1. Pasar una referencia: process(&data)
               2. Clonar los datos: process(data.clone())

Translation Quality Standards
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pyrite enforces high translation quality through:

1. **Native speaker translators:** Professional translations, not machine translation
2. **Technical accuracy:** Preserve precise technical meanings
3. **Consistency:** Shared terminology across all error messages
4. **Community review:** Open process for translation improvements
5. **Maintainability:** Translation infrastructure integrated with compiler

Supported Languages (Priority Order)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Stable Release (Initial Internationalization):
  â€¢ English (en) - Default
  â€¢ Chinese (zh) - 1.4B speakers, huge developer population
  â€¢ Spanish (es) - 500M speakers, Latin America growing
  â€¢ Hindi (hi) - 600M speakers, India's massive dev community

Stable Release Expansion:
  â€¢ Japanese (ja) - Major tech economy
  â€¢ Portuguese (pt) - Brazil's growing tech sector
  â€¢ Korean (ko) - Strong programming culture
  â€¢ German (de) - European tech hub
  â€¢ French (fr) - Francophone Africa + Europe
  â€¢ Russian (ru) - Eastern Europe tech community

Translation Infrastructure
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Messages stored in structured format:

    # errors/en/P0234.toml
    code = "P0234"
    title = "cannot use moved value '{name}'"
    
    [message]
    primary = "cannot use '{name}' after it was moved"
    note = "value moved here"
    help = [
        "Pass a reference: {suggestion_ref}",
        "Clone the data: {suggestion_clone}"
    ]

Community contribution workflow:

    quarry translate --language=es       # Check translation coverage
    quarry translate --validate          # Verify translation quality
    quarry translate --submit            # Submit translation PR

IDE Integration
~~~~~~~~~~~~~~~

Language selection respects IDE/OS settings:

    # VSCode/IntelliJ automatically use system language
    # Override in IDE settings:
    "pyrite.diagnostics.language": "auto"  # Match system locale
    "pyrite.diagnostics.language": "en"    # Force English
    "pyrite.diagnostics.language": "zh"    # Force Chinese

Why This Matters
~~~~~~~~~~~~~~~~~

Internationalized error messages address a critical adoption barrier:

**Global reach:**
  â€¢ 60% of programmers are non-native English speakers
  â€¢ Educational institutions in non-English countries
  â€¢ Open source contributors worldwide
  â€¢ Government contracts requiring local language support

**Accessibility:**
  â€¢ Language barriers prevent understanding complex concepts (ownership)
  â€¢ Error messages are WHERE learning happens
  â€¢ Native language reduces cognitive load for hard concepts
  â€¢ Enables instructors to teach in students' first language

**Differentiation:**
  â€¢ Almost NO compilers do this well (GCC/Clang have minimal translations)
  â€¢ Rust has basic translation but inconsistent
  â€¢ Go, Zig, Mojo: English only
  â€¢ Pyrite: First-class multilingual support from day one

**Evidence:**
  â€¢ Microsoft DevTools survey: 78% prefer native language errors
  â€¢ Educational research suggests faster concept mastery in first language (specific 
    studies show approximately 2x improvement, though results vary)
  â€¢ Adoption data: Python's success partly due to global accessibility

**Achieving widespread developer adoption requires global reach, not just 
English-speaking countries.**

Translation coverage tracking:

    $ quarry translate --language=zh --coverage
    
    Chinese Translation Coverage
    =============================
    
    Error codes: 234 / 250 (93.6%)
    Warning codes: 89 / 92 (96.7%)
    Compiler hints: 156 / 160 (97.5%)
    
    Missing translations:
      â€¢ P0891 - Advanced lifetime error
      â€¢ P0923 - Generic associated type issue
      â€¢ W1234 - Performance hint for GPU kernel

This infrastructure positions Pyrite to be among the first systems programming languages 
with comprehensive global 
language, accessible to the world's 26 million developers regardless of native 
language.

Implementation: Stable Release (core languages and expansion)
Priority: High (required for global adoption)

================================================================================
3. SYNTAX OVERVIEW
================================================================================

Pyrite's syntax is designed to be familiar to Python users while operating in a 
compiled, statically-typed setting. This section describes the basic lexical 
structure and grammar of Pyrite code.

3.1 Lexical Elements and Formatting
--------------------------------------------------------------------------------

Whitespace and Indentation
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pyrite uses indentation-based block structure, similar to Python. Blocks of code 
(such as the body of a function, loop, or conditional) are defined by their 
indent level rather than explicit braces. Consistent indentation is required; a 
mix of tab and spaces or inconsistent indent widths will be a compile-time error. 

This design ensures that the visual layout of the code always matches its logical 
structure - a feature proven by Python to reduce bugs and improve clarity. 

For example: 

    if x > 0:
        print("Positive")
    elif x < 0:
        print("Negative")
    else:
        print("Zero")

In this snippet, the bodies of the if and else branches are determined by 
indentation. The layout makes the flow obvious. (If the indentation were 
misaligned, the compiler would reject it.) This contributes to code that is easy 
to read and reason about.

End-of-Line Termination
~~~~~~~~~~~~~~~~~~~~~~~

Statements are terminated by newline characters. Pyrite does not require a 
semicolon at the end of each statement. You typically put one statement per line. 
(An optional semicolon may be allowed if multiple statements are written on a 
single line or to explicitly terminate a statement, but this is rarely needed in 
practice.) 

This follows Python's convention and avoids cluttering code with unnecessary 
punctuation. Long expressions can be continued across lines either by using a 
backslash or (more commonly) by open parentheses/brackets, similar to Python.

Comments
~~~~~~~~

Pyrite supports both single-line and multi-line comments. Single-line comments 
start with # and continue to the end of the line (just like Python). 

For example: 

    x = 5  # this is a single-line comment

Multi-line (block) comments can be written by enclosing the text in triple quotes 
""" ... """ (again, similar to Python's docstring style). Alternatively, C-style 
/* ... */ block comments may be supported for convenience with certain tools, but 
the idiomatic way is to use """ for multi-line documentation comments. 

By convention, a triple-quoted string literal that is not assigned to any 
variable (placed standalone in code) can serve as a documentation comment for the 
following code element.

Identifiers
~~~~~~~~~~~

Identifiers (names for variables, functions, types, etc.) may consist of letters 
(including Unicode letters), digits, and underscores, but must not begin with a 
digit. Pyrite is case-sensitive, so for example foo, Foo, and FOO are distinct 
identifiers. 

By convention, snake_case is used for variable and function names (following 
Python's style), and CamelCase is used for type names (e.g. struct or enum names, 
similar to Rust and Zig naming conventions). Identifiers cannot clash with 
reserved keywords (the compiler will error if you try to name a variable if or 
while, for instance).

Keywords
~~~~~~~~

Pyrite reserves certain keywords for language constructs. Many keywords are 
borrowed from Python for familiarity. For example: fn (for function definition), 
let and var (for variable binding, see mutability below), if, elif, else, for, 
while, break, continue, return, struct, enum, union, true, false, None, unsafe, 
etc. 

Note that we use fn to define functions (similar to Rust's fn or Swift's func) 
instead of Python's def. This is to clearly distinguish function definitions and 
to remind that functions in Pyrite are statically typed and compiled. Overall, 
the goal is a light syntax reminiscent of Python, but with some tweaks to suit a 
compiled language.

Literals
~~~~~~~~

Pyrite supports various literal forms:

â€¢ Integer literals can be written in decimal (e.g. 123), hexadecimal (prefix 0x, 
  e.g. 0x7B), binary (prefix 0b), or octal (prefix 0o). You can include 
  underscores in numeric literals for readability (e.g. 1_000_000 for one 
  million). By default, an unsuffixed integer literal is inferred to an 
  appropriate integer type based on context (or defaults to the type int if 
  there's no context). Pyrite does not implicitly widen or narrow integers without 
  a cast - arithmetic on mixed integer types either requires an explicit cast or 
  will use an automatic promotion to a larger type when well-defined (similar to 
  C's usual arithmetic conversions but simpler, since Pyrite discourages implicit 
  narrowing). Integer overflow is handled by the build mode: in debug builds, 
  arithmetic overflow is checked and will raise an error or panic if it occurs, 
  whereas in optimized release builds it will wrap around using two's complement 
  arithmetic by default (with an option to enable checked arithmetic if desired). 
  This follows Rust and Zig's approach of having overflow checks in debug mode 
  but zero overhead in release unless explicitly requested for safety.

â€¢ Floating-point literals use standard decimal or scientific notation (e.g. 3.14, 
  1e-9). By default a floating literal like 3.0 is inferred as a double-precision 
  64-bit float (f64) for precision, unless a context or suffix indicates 
  otherwise. A suffix (like 1.2f32) can denote a specific type. No implicit 
  conversion between floats and ints occurs - an explicit cast is required to 
  convert between numeric types, to avoid accidental loss of data.

â€¢ Boolean literals are written as true and false. Only booleans can be used in 
  conditional contexts (if, while, etc.); there is no automatic truthiness 
  conversion from other types (e.g. an integer 0 vs non-zero) - this prevents 
  certain classes of bugs (like writing if (x = 0) in C when == was meant). 
  Instead, you must explicitly compare or convert to bool if needed. Logical 
  operations use the keywords and, or, not (in place of C's &&, ||, !) for 
  readability, with short-circuit evaluation semantics identical to Python (e.g. 
  A and B evaluates B only if A was true).

â€¢ Character and string literals: A char in Pyrite represents a single Unicode 
  code point (likely a 32-bit value, like Rust's char). Character literals are 
  written with single quotes, e.g. 'A'. Text strings are handled by an immutable 
  string type (e.g. String), with string literals written in double quotes, e.g. 
  "Hello, world". Standard escape sequences are supported within strings (\n, \t, 
  \", etc.). Strings in Pyrite are sequences of Unicode characters and are 
  immutable (their content cannot be changed once created). To modify text, one 
  would use a separate mutable buffer type or convert a string to a mutable array 
  of chars. Concatenating string literals at compile time is allowed (adjacent 
  string literals are merged, or a special operator can be evaluated at compile 
  time), but runtime string concatenation will allocate new memory. Pyrite avoids 
  hidden heap allocations by default, so using + to concatenate two strings will 
  likely allocate behind the scenes - the language encourages using an explicit 
  string builder or formatting utility for efficiency when building strings in a 
  loop, for example. (In fact, Pyrite may only allow string + in a context where 
  it can be evaluated at compile time, to avoid misleading performance costs.)

â€¢ The None literal: None represents a null or absent value. This is similar to 
  Python's None or to the concept of null/nullptr in other languages, but in 
  Pyrite it is used in a controlled way: primarily as the value of an optional 
  type or sentinel. Pyrite does not have null pointers by default; None is 
  actually an instance of an Optional type (see composite types below). You 
  cannot assign None to an arbitrary variable unless that variable's type permits 
  it (e.g. Optional[T]). This design prevents null-pointer dereferences in safe 
  code - you can only get a "null" if you explicitly use an optional, and you 
  must check for None before using the value.

3.2 Code Structure
--------------------------------------------------------------------------------

Modules and Imports
~~~~~~~~~~~~~~~~~~~

Pyrite source code is organized into modules, typically one module per file. 
Every source file is a module, and modules can import other modules. The syntax 
for importing is Python-like: the import keyword is used. For example, import 
math will import the module named math. Imported module names act as namespaces, 
so names defined in math would be accessed with a math. prefix (e.g. 
math.sin(x)). 

Pyrite supports a simple package system that allows grouping modules into package 
hierarchies using dot notation (for example, import graphics.image would import 
the image sub-module of the graphics package, assuming a directory structure to 
match). The exact details of module lookup (how the compiler finds 
graphics/image.pyrite or similar) are defined by the compiler and build system, 
but cyclic imports are either disallowed or handled gracefully (the compiler will 
detect circular dependencies and produce an error or resolution order). Module 
imports are resolved at compile time; there is no dynamic importing at runtime by 
default.

Entry Point
~~~~~~~~~~~

Like C (and Rust), Pyrite programs start execution from a special main function. 
A Pyrite program must have exactly one function defined as fn main(): ... which 
serves as the entry point. The main function takes no arguments (or, 
alternatively, it may take an array of strings as an argument to represent 
command-line arguments, similar to argv in C, depending on the final design) and 
returns an integer as an exit code (conventionally 0 for success). 

For example, a minimal program might be:

    fn main():
        print("Hello from Pyrite!")
        return 0

If no explicit return is given in main, reaching the end of the function 
implicitly returns 0. Pyrite's build system will produce an executable that 
begins at main. There is no requirement for a runtime library to start up (unlike 
languages that need a VM initialization); Pyrite's startup is as minimal as C's - 
  it just calls main.

File Extension and Compilation Units
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

(By convention, assume Pyrite source files use a .pyrite extension, though 
this is not a language rule per se.) Each file can be compiled as part of a 
project. The compiler and package manager handle how multiple modules are linked 
together into an application or library. A package in Pyrite might consist of 
many modules, with a manifest for dependencies (similar to Rust's Cargo or Zig's 
build system). 

However, the specifics of the build and package system are part of the ecosystem 
rather than the core language syntax, so they are not detailed here. The language 
ensures that separate modules have explicit imports for any names they use from 
other modules (no implicit global scope), which aids clarity and compiler 
optimization.

================================================================================
4. TYPES AND TYPE SYSTEM
================================================================================

Pyrite has a static, strong type system that enforces type safety at compile 
time (preventing type errors and many classes of runtime errors). However, the 
type system is designed to be as unobtrusive as possible for the programmer. 
Type inference is used extensively so that in many cases the programmer doesn't 
need to explicitly write types - the compiler can deduce them. 

This section describes Pyrite's types, including primitives, composite types, and 
how mutability and ownership affect types.

4.1 Primitive Types
--------------------------------------------------------------------------------

Integers
~~~~~~~~

Pyrite supports a range of integer types for low-level control, similar to C and 
Rust. This includes fixed-width signed and unsigned integers of various bit 
widths (e.g. i8, i16, i32, i64 for signed 8/16/32/64-bit, and u8, u16, etc. for 
unsigned). There is also a native-sized integer type int (and uint or perhaps 
usize) whose bit width matches the platform's word size (32-bit on a 32-bit 
target, 64-bit on a 64-bit target) - this is analogous to Zig's usize or Rust's 
isize/usize. 

The default integer type for inference is a comfortable size (likely 32-bit or 
64-bit depending on platform) when the context doesn't constrain it. Pyrite 
avoids surprising implicit casts: it will not automatically narrow an integer 
(e.g. from 64-bit to 32-bit) and will only widen in cases that are well-defined. 
Mixed-type integer arithmetic either selects a larger type or yields a compile 
error requiring an explicit cast. 

Integer operations that overflow will, by default, throw an error in debug mode, 
but in optimized builds they wrap (two's complement wraparound) for performance, 
unless explicit checked arithmetic is requested. This strategy ensures safety 
during development without penalizing release performance, following Rust and 
Zig's lead on integer overflow handling.

Floating-Point
~~~~~~~~~~~~~~

Pyrite provides standard IEEE-754 floating point types: f32 (32-bit single 
precision) and f64 (64-bit double precision). By default, a float literal like 
3.14 is inferred as f64 for precision, but you can suffix or contextually infer 
f32 if needed. Floating-point operations follow IEEE semantics (with support for 
special values like NaN and infinity). 

As with ints, no implicit conversion occurs between floats and integers - a 
conversion must be done with an explicit cast function or operator to avoid 
unintended truncation or rounding. If necessary, a generic float keyword may be 
an alias for the default float type (like Python's float meaning 64-bit).

Boolean
~~~~~~~

The bool type represents a boolean value (true or false). Booleans in Pyrite are 
not implicitly convertible to or from numeric types. This means conditional 
statements require a bool expression explicitly. (This design, also used in 
languages like Rust and Pascal, prevents common errors like using = instead of == 
in C, and makes the code's intent clearer.) 

Boolean operations use the English words and, or, not as mentioned, and they 
short-circuit. There is no separate boolean and bitwise type - & and | are 
available as bitwise operators for integers, while and/or are logical for bools.

Character and String
~~~~~~~~~~~~~~~~~~~~

Pyrite's char type is a Unicode code point (32-bit). It can represent any 
Unicode character by code point value. Strings in Pyrite are of type String 
(immutable sequence of characters). A String can be any length and is stored in a 
contiguous memory buffer (likely heap-allocated for nontrivial lengths). Strings 
support typical operations like concatenation, slicing, and iteration. 

However, strings are immutable; you cannot change a character in an existing 
String. To build or modify text, you would use a StringBuilder or work with a 
mutable char array. String literals (quoted text in the source) produce String 
instances at compile time, which may be interned or stored in read-only memory 
  (as an optimization - but that's an implementation detail).

Pyrite avoids hidden allocations, so operations like concatenating two strings at 
runtime will allocate a new string for the result (and this is documented 
behavior). Programmers are encouraged to handle large text concatenation through 
explicit means (e.g. using a builder or formatting library) rather than relying 
on + repeatedly. In some cases, constant string concatenation can be evaluated at 
compile time (if the operands are known at compile time).

Beginner-Friendly Type Aliases
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To ease the learning curve for absolute beginners, Pyrite's standard library 
provides optional type aliases for common borrowed views:

    type Text = &str        # Borrowed string view (read-only text)
    type Bytes = &[u8]      # Borrowed byte slice (raw data)

These aliases make function signatures more readable for newcomers:

    # More beginner-friendly
    fn parse_config(content: Text) -> Result[Config, Error]
    
    # Equivalent to (both work)
    fn parse_config(content: &str) -> Result[Config, Error]

The underlying types (&str, &[u8]) remain the fundamental types and work 
everywhere. The Text/Bytes aliases are purely for readability in documentation 
and teaching materials. Experienced developers can use either form.

Note: These are type aliases, not new types - no runtime cost, no type system 
complexity. They exist solely to make "borrowed view of text" more intuitive 
for Python/JavaScript developers transitioning to systems programming.

Generic Reference Aliases
~~~~~~~~~~~~~~~~~~~~~~~~~

Beyond Text and Bytes, the standard library provides generic aliases for common 
reference patterns:

    type Ref[T] = &T        # Immutable reference to T
    type Mut[T] = &mut T    # Mutable reference to T
    type View[T] = &[T]     # Slice view of T elements

These are available in the prelude but entirely optional. They allow tutorial 
code to use familiar terms before introducing the underlying syntax:

    # Beginner-facing tutorial might use:
    fn sum(numbers: View[int]) -> int:
        var total = 0
        for n in numbers:
            total += n
        return total
    
    # Experienced developer writes:
    fn sum(numbers: &[int]) -> int:
        var total = 0
        for n in numbers:
            total += n
        return total
    
    # Both are identical (Ref[T] is literally &T)

Teaching Path
~~~~~~~~~~~~~

The suggested learning progression for references:

1. **Week 1:** Use `Ref[T]`, `Mut[T]`, `View[T]` in tutorials
   - "Pass a Ref[Config] to read the config"
   - "Pass a Mut[Buffer] to modify the buffer"
   - "Pass a View[int] to iterate over numbers"

2. **Week 2-3:** Reveal the underlying syntax
   - "Ref[T] is actually &T in Pyrite syntax"
   - "You can use either form"
   - Show both in examples side-by-side

3. **Week 4+:** Use underlying syntax primarily
   - Documentation uses &T, &mut T, &[T]
   - Generic aliases become optional shorthand
   - Beginners have learned the real syntax organically

Design Rationale
~~~~~~~~~~~~~~~~

This approach mirrors how humans learn natural languages:

  â€¢ Start with familiar vocabulary ("pass a reference")
  â€¢ Reveal native idioms gradually ("actually it's &T")
  â€¢ Achieve fluency through practice

The key insight: type aliases cost nothing at runtime and don't fragment the 
ecosystem. They're purely pedagogical sugar. Experienced developers never need 
to know they exist, but they smooth the first week for Python/JavaScript 
developers.

Alternatives considered and rejected:
  âœ— Teaching &T immediately: Too much syntax shock
  âœ— Hiding &T permanently: Creates two communities
  âœ“ Gradual revelation: Best of both worlds

These aliases are opt-in for documentation authors. Core Pyrite docs use the 
real syntax; community tutorials can choose their approach.

Unit (Void)
~~~~~~~~~~~

For functions that do not return a meaningful value (procedures), Pyrite has a 
unit type, analogous to void in C or NoneType in Python. The unit type can be 
spelled void or possibly as an empty tuple (). The unit type has exactly one 
possible value (no data); we can think of it like "nothing". 

A function with no return type specified is assumed to return void. The unit 
value can be ignored (not assigned to anything). This is mainly for type 
completeness - so that every function has some return type even if it's not useful. 

In Pyrite, the None literal is not of type void; instead None is typically a 
value of type Optional[T] (see below). Unit/void is used for functions that just 
have side effects and don't produce a value.

4.2 Composite Types
--------------------------------------------------------------------------------

Arrays
~~~~~~

Pyrite supports two kinds of arrays with explicit syntax to differentiate them:

â€¢ Fixed-size arrays - These are like C arrays, allocated on the stack (or inline 
  in structs) and with a size known at compile time. The syntax is [T; N], where 
  T is the element type and N is the compile-time size constant (following Rust's 
  syntax). For example, [int; 100] is an array of 100 integers, and [u8; 1024] 
  is a 1KB byte buffer. The semicolon visually distinguishes arrays from slices 
  (see below) and signals "fixed size" to readers. Fixed arrays have value 
  semantics: assigning or passing them will (by default) copy the entire array, 
  unless the type is non-copyable. They are useful for embedded programming or 
  performance-critical code where you want a true stack allocation.
  
  Examples:
      let buffer: [u8; 256]            # 256-byte buffer on stack
      let matrix: [[f32; 4]; 4]        # 4Ã—4 matrix (16 floats)
      let zeros = [0; 100]             # Array of 100 zeros (type inferred)

â€¢ Dynamic arrays (Vectors) - These are resizable arrays that manage a block of 
  memory on the heap. Pyrite's standard library provides a List[T] or Vector[T] 
  type to represent a dynamic array. A List[T] can grow or shrink, and 
  internally it allocates memory (using an allocator) to hold its elements. The 
  distinction between these two is explicit: a beginner will learn that List[int] 
  (for example) is a resizable array that uses the heap, whereas int[N] (an array 
  type of length N) is a fixed-size array on the stack. This makes the memory 
  behavior intuitive at a glance. 

â€¢ Slices: Pyrite supports slices as borrowed views into arrays or lists. The 
  syntax is &[T] for an immutable slice and &mut [T] for a mutable slice. A slice 
  is essentially a fat pointer (pointer to first element + length). For instance, 
  a function can take a &[T] parameter to accept either a fixed array, a List, or 
  a portion of either. Slices allow safe access to array segments without copying, 
  and come with bounds-checking (with potential to opt-out in unsafe code).
  
  Examples:
      fn sum(numbers: &[int]) -> int    # Accepts any contiguous integers
      
      let arr = [1, 2, 3, 4, 5; 5]
      let slice = &arr[1..4]            # Slice of elements [2, 3, 4]
      
      let list = List[int]([10, 20, 30])
      let list_slice = &list[..]        # Slice of entire list
  
  The type hierarchy is clear and teachable:
    â€¢ [T; N]   - Fixed-size array (stack, compile-time size)
    â€¢ &[T]     - Borrowed slice (view into array/list)
    â€¢ List[T]  - Growable vector (heap, runtime size)

Structs (Records)
~~~~~~~~~~~~~~~~~

A struct in Pyrite is a composite type that groups multiple named fields 
(possibly of different types) into one record, similar to a struct in C or a 
class with only data in many OOP languages. By default, Pyrite structs have value 
semantics (a struct value holds its own copy of fields, and copying the struct 
copies all its data). Structs are defined with the struct keyword. 

For example: 

    struct Point:
        x: int
        y: int

This defines a struct Point with two integer fields. You can create a Point with 
a literal syntax like Point { x: 3, y: 4 }. Structs can be nested (a struct can 
contain other structs or arrays, etc.). They do not have implicit constructors or 
destructors - any initialization beyond simple field assignment would be done via 
functions (or static methods, see advanced features). By default, all struct 
fields are public (exported) within the module unless a visibility modifier is 
introduced (the spec may introduce private or similar for encapsulation at module 
boundaries). 

Pyrite does not automatically generate "methods" or behaviors for structs; it 
keeps them simple. However, traits and implementation blocks (discussed later in 
advanced features) can be used to add methods or operator overloads in a 
controlled manner. 

Memory layout: The layout of struct fields in memory is deterministic (likely 
same as C layout for compatibility, with perhaps some padding rules for 
alignment). Pyrite aims to support repr(C) or similar annotations to guarantee 
binary layouts for FFI when needed.

Enumerations (Enums)
~~~~~~~~~~~~~~~~~~~~

Pyrite supports enumerated types and algebraic data types via the enum keyword. 
An enum in Pyrite can be a simple enumeration of named constants or a tagged 
union of variants with data (similar to Rust's enums or variants in functional 
languages). 

For example:

    enum Result[T, E]:
        Ok(value: T)
        Err(error: E)

This enum Result has two variants: Ok holding a value of type T, and Err holding 
an error of type E. Enums in Pyrite are tagged unions under the hood: they 
consist of a discriminator (tag) and a payload for the active variant. The 
compiler ensures that only the active variant's data is accessible at any time, 
and that all possible variants are handled in pattern matching (exhaustiveness 
checking). 

Enums allow naturally expressing optional values, error handling, and other 
variant-based logic safely. A simpler enum with no data for variants (like enum 
Color { Red, Green, Blue }) is also allowed, similar to a C/C++ enum but 
type-safe (it doesn't silently convert to int, though you can map to an int if 
needed). Enum variant names are namespaced under the enum type. You construct 
variants as Color.Red, Result.Ok(42), etc. and use pattern matching to extract 
values (see Control Flow for pattern matching).

Using enums eliminates many error-prone patterns (like using integer codes or 
sentinel values). For instance, Pyrite would encourage use of an Optional[T] 
(which could be an enum with variants Some(T) and None) for values that may or 
may not be present, rather than using null pointers. This way, the compiler 
forces you to handle the "none" case explicitly.

Optionals
~~~~~~~~~

Likely defined as an enum under the hood, Optional[T] (or Option[T]) would be 
the type for "maybe a T". You could declare var x: Option[int] = None to 
represent an integer that might not have a value. To use it, you must check if 
it's None or Some(val) - otherwise the compiler will not let you treat it as a 
concrete int. This prevents null usage errors.

Unions
~~~~~~

Pyrite also allows a form of union type (an untagged union, similar to C's union) 
for low-level programming where you need to interpret the same memory in 
different ways. However, accessing a union's fields is an unsafe operation unless 
you manually track which field is active, because the compiler cannot enforce 
correctness (an untagged union can lead to type punning and unsafe aliasing if 
misused). 

For example, you might define union IntOrFloat { i: int, f: f32 } to reuse memory 
for an int or float, but it's on the programmer to remember which one is stored. 
Reading from a union field that wasn't most recently written is undefined behavior 
in safe terms. Thus, union usage in Pyrite would require an unsafe block or 
function. The presence of union types ensures that any trick possible in C (for 
bit-level manipulation, etc.) is possible in Pyrite when absolutely needed, but 
safe code would prefer enums (tagged unions) for variant data.

4.3 References and Pointers
--------------------------------------------------------------------------------

Because Pyrite is a systems programming language, it provides pointer types - but 
in a way that separates safe references from raw, unsafe pointers.

References (Borrowed Pointers)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A reference is a pointer type that is managed by the compiler's borrow checker 
for safety. Syntax: &T denotes an immutable reference to a value of type T, and 
&mut T denotes a mutable reference. You can think of &T similar to a pointer to T 
that you promise not to modify (and through which you cannot mutate the value), 
while &mut T is a pointer that gives you exclusive mutable access. 

References must obey the borrowing rules (see Memory Management section) - 
essentially, you can have multiple &T to the same value (read-only aliasing is 
OK), but only one &mut T at a time to a given value (exclusive write access). 

References are always non-null (you cannot have a null &T in safe code; the 
compiler ensures a reference is valid). They also must not outlive the data they 
point to - the compiler's lifetime analysis ensures that a reference doesn't 
persist beyond the scope of the value it refers to, preventing dangling pointers. 

In summary, &T and &mut T provide the convenience of pointers but with 
compile-time guarantees: if your code compiles using only references (and no 
unsafe conversions), you won't have null-dereferences or use-after-free issues. 
References are the primary way to pass data around without copying in Pyrite's 
safe code.

Optional Argument Convention Aliases
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For educators and teaching materials, Pyrite provides optional argument convention 
keywords that desugar to standard reference syntax. These aliases make intent 
explicit for beginners learning ownership concepts:

    # Standard Pyrite syntax (always works)
    fn process(data: &Config) -> Result[(), Error]:
        # Immutable borrow
    
    fn update(data: &mut Config):
        # Mutable borrow
    
    fn consume(data: Config):
        # Takes ownership (moves)

    # Optional teaching aliases (desugar to above)
    fn process(borrow data: Config) -> Result[(), Error]:
        # Desugars to: data: &Config
    
    fn update(inout data: Config):
        # Desugars to: data: &mut Config
    
    fn consume(take data: Config):
        # Explicit: data moved (semantically identical, but self-documenting)

The keywords:
  â€¢ `borrow` â†’ desugars to `&T` (immutable reference)
  â€¢ `inout` â†’ desugars to `&mut T` (mutable reference)  
  â€¢ `take` â†’ semantic marker for ownership transfer (no desugaring, just documentation)

These are **pure syntax sugar** with zero runtime cost and no type system changes.

Design rationale:
  â€¢ Zero cost: Desugars during parsing, identical to &T
  â€¢ Optional: Never required, &T works everywhere
  â€¢ Teaching-focused: Makes intent crystal clear for newcomers
  â€¢ Self-limiting: Experienced developers naturally transition to &T

Example teaching progression:

    # Week 1 (Tutorial code with aliases)
    fn calculate_total(borrow items: List[Item]) -> f64:
        var sum = 0.0
        for item in items:
            sum += item.price
        return sum
    
    # Week 2 (Show equivalence)
    fn calculate_total(borrow items: List[Item]) -> f64:
        # Note: 'borrow' is sugar for &List[Item]
        # Both forms work identically
    
    # Week 3+ (Transition to standard syntax)
    fn calculate_total(items: &List[Item]) -> f64:
        # Standard Pyrite syntax
        # More compact, same meaning

When to use:
  â€¢ Teaching materials targeting absolute beginners
  â€¢ Introductory tutorials and workshops
  â€¢ Educational institutions with Python-first students
  â€¢ Code review where intent needs extra clarity

When NOT to use:
  â€¢ Production code (stdlib uses &T exclusively)
  â€¢ Advanced tutorials (learners should see real syntax)
  â€¢ Library APIs (ecosystem standardizes on &T)
  â€¢ After first 2-3 weeks of learning

Compiler support:
  â€¢ Parser recognizes keywords and desugars immediately
  â€¢ Error messages always show underlying &T syntax
  â€¢ quarry fmt can normalize to standard syntax (configurable)
  â€¢ No separate type checking (aliases resolved before analysis)

Configuration:

    # Quarry.toml - Teaching mode
    [learning]
    allow-argument-aliases = true
    
    # Quarry.toml - Production mode
    [learning]
    allow-argument-aliases = false         # Reject aliases, standardize on &T

Linter integration:

    # Suggest transitioning to standard syntax
    quarry lint --suggest-standard-syntax
    
    warning: Consider using standard syntax '&Config' instead of 'borrow Config'
      ----> tutorial.py:15:23
       |
    15 | fn process(borrow data: Config):
       |            ^^^^^^^^^^^^ teaching alias
       |
       = help: Standard syntax is more compact:
               fn process(data: &Config):
       = note: Teaching aliases are optional sugar for beginners
       = note: Use 'quarry fmt --normalize-syntax' to convert automatically

Why this approach works:
  â€¢ **Gentle onboarding:** borrow/inout/take read like English
  â€¢ **Zero fragmentation:** All code is valid Pyrite, just different syntax
  â€¢ **Natural graduation:** Learners see &T in errors, transition organically
  â€¢ **No ecosystem split:** Production code uses &T, tutorials can choose

This mirrors Pyrite's existing type alias strategy (Text = &str, Ref[T] = &T):
  â€¢ Provide familiar vocabulary for beginners
  â€¢ Reveal underlying syntax gradually
  â€¢ Enable fluency through practice
  â€¢ Keep ecosystem unified (aliases are optional)

Inspired by Mojo's argument conventions but implemented as **opt-in syntax sugar** 
rather than required syntax, maintaining Pyrite's "one obvious way" philosophy 
while accommodating teaching scenarios.

Implementation: Beta Release (low complexity, high teaching value)
Adoption: Optional (individual projects/educators decide)
Ecosystem impact: Zero (stdlib and community standardize on &T)

Raw Pointers
~~~~~~~~~~~~

For interoperability with C and for the cases where you truly need pointer 
arithmetic or want to opt out of the borrow rules, Pyrite has raw pointer types. 
A raw pointer is written as *T for a constant pointer or *mut T for a mutable 
pointer (similar to C's T* or Rust's *const T /*mut T). 

Raw pointers do not carry lifetime or nullability guarantees - you can cast an 
address to a *T, it might be null or dangling, and you are responsible for using 
it correctly. Raw pointers are unsafe to dereference; the compiler will only 
allow you to read or write through a *T inside an unsafe context (see Memory 
Management). 

Raw pointers support pointer arithmetic (e.g. you can do ptr + 1 to point to the 
next element if it's an array), whereas references do not allow arithmetic. Raw 
pointers are mainly used for low-level code, FFI calls (passing pointers to or 
from C), or implementing data structures where you need more control than the 
safe abstractions allow. In safe Pyrite code, you will rarely use raw pointers 
directly - they'll be encapsulated in libraries.

Pointer Conversions
~~~~~~~~~~~~~~~~~~~

You can obtain a raw pointer from a reference (for example, by casting &T to *T 
or using some standard library function) which essentially tells the compiler "I 
want to manage this pointer's rules manually." Going the other way (raw to 
reference) is unsafe because the compiler must assume you know that pointer is 
valid. Pyrite's design encourages you to use references whenever possible, and 
only use raw pointers in isolated low-level parts of your code, clearly marked as 
unsafe.

References and raw pointers give Pyrite the same expressive power as C for memory 
access, but by distinguishing the safe vs unsafe variants, Pyrite ensures that 
most of your code can be checked for memory errors at compile time. Only in the 
rare parts that truly need unchecked behavior do you have to drop down to raw 
pointers.

4.4 Mutability and Assignment
--------------------------------------------------------------------------------

Pyrite distinguishes between immutable and mutable variables at the language 
syntax level, taking inspiration from Rust's approach of making immutability the 
default. By default, a variable declared with let is immutable - once a value is 
bound to that name, it cannot be changed. If you need a variable's value to 
change (mutate), you use the var keyword to declare it as mutable. This design 
encourages using immutable values wherever possible, which can prevent a class of 
bugs and makes reasoning about code easier (especially in concurrency).

Example:

    let radius = 10     # immutable binding
    var counter = 0     # mutable binding
    counter = counter + 1  # OK, counter is mutable
    # radius = 5        # ERROR, radius is immutable

Immutability by default aligns with the idea that most data does not need to 
change after creation, and it helps catch unintended changes. Of course, you can 
opt into mutability with a clear var annotation.

Constants
~~~~~~~~~

For values that are known at compile time and never change, Pyrite provides a 
const declaration. 

For example: 

    const PI = 3.14159
    const SIZE = 1024 * 4

Constants in Pyrite are inlined at compile time - they do not occupy memory at 
runtime. You can use them in any context that a literal would be allowed. 
Constants must be initialized with a compile-time evaluable expression. They are 
implicitly immutable (you don't write var or let for constants, just const). This 
is analogous to #define in C or const in languages like C++ and Rust, and is 
useful for configuration values, array lengths, etc.

Assignment Semantics (Move vs Copy)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In Pyrite, assignment and passing variables to functions will either copy or move 
the value depending on its type. Types like primitive integers, floats, and other 
simple types implement a trait (interface) that marks them as Copy types - 
meaning a bitwise copy of the value is cheap and valid. For those types, 
assigning a = b makes a copy of b into a (and both are still usable). 

However, for types that manage resources (like a dynamic array, or any type with 
an owned heap allocation), Pyrite will treat them as Move-only by default. This 
means an assignment a = b will move the value from b to a, leaving b invalid (and 
it cannot be used unless reinitialized). This move semantics ensures that there 
is only ever one owner of each piece of resource, preventing double-free errors 
at compile time. If a type is move-only, you can still explicitly clone it if 
needed (if the type provides a clone operation). But by default, expensive or 
sensitive resources aren't implicitly duplicated.

For example, if List[int] is a dynamic array type and you do let a = 
List[int]([1,2,3]); let b = a;, this would transfer ownership of the list from a 
to b (after the line, a would be considered invalid or "moved"). If you try to use 
a again, the compiler will error, because using a moved value is not allowed (to 
prevent use-after-free). 

In contrast, if you had let x = 5; let y = x;, both x and y are fine because int 
is a Copy type (cheap to copy and doesn't represent a resource that needs 
freeing). The Pyrite compiler handles this automatically based on type traits - 
the beginner programmer doesn't need to deeply understand it initially, beyond 
noticing that some values can be reused after assignment and others cannot. The 
compiler's error messages (e.g. "value moved here and cannot be used again") will 
guide the programmer, effectively teaching them about ownership as they go.

Destruction and RAII
~~~~~~~~~~~~~~~~~~~~

When a variable goes out of scope (for example, at the end of a function or the 
end of a block in which it's defined), Pyrite will automatically destroy (clean 
up) that variable if it owns a resource. This is similar to RAII (Resource 
Acquisition Is Initialization) in C++ and is tied into the ownership model 
detailed in the next section. For instance, if you have a File object that 
acquires a file handle, when it goes out of scope Pyrite will ensure its 
destructor closes the file.

Simple types like ints or floats don't need special cleanup (and the compiler 
typically treats their destruction as a no-op). Complex types might free memory 
or other resources in their destructor. Importantly, Pyrite does not have a 
garbage collector - resources are deterministically freed at scope end, and 
memory is either stack-allocated or explicitly allocated/freed on the heap. 

If a type needs custom cleanup, it will implement a drop/destructor method as 
part of a trait (e.g. a trait similar to Disposable or Rust's Drop). But unlike 
C++, that destructor is not "hidden" - Pyrite does not implicitly generate code 
with side effects beyond freeing memory unless you've opted into it. This keeps 
control flow clear (no unexpected lengthy destructors running at scope exit 
unless you know the type does that).

In summary, Pyrite's type system offers the low-level control of C's types, the 
safety and expressiveness of Rust's ownership and algebraic types, and the 
convenience of Python's readability. Types are inferred where possible, and the 
rules about moves vs copies and lifetimes work mostly behind the scenes to 
prevent errors, with the programmer learning them gradually through clear 
compiler feedback.

4.5 Cost Transparency Attributes
--------------------------------------------------------------------------------

Pyrite provides compiler-enforced attributes to make performance guarantees 
explicit and verifiable. These attributes transform the language philosophy of 
"no hidden costs" from documentation into hard contracts.

@noalloc - No Heap Allocations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The @noalloc attribute guarantees a function (or block) performs zero heap 
allocations:

    @noalloc
    fn calculate_checksum(data: &[u8]) -> u32:
        var sum: u32 = 0
        for byte in data:
            sum = sum ^ byte
        return sum

If this function attempts any heap allocation (creating a List, allocating a 
String, calling a function that allocates), compilation fails:

    error[P0601]: heap allocation in @noalloc function
      ----> crypto.py:156:13
       |
    155 | @noalloc
    156 | fn calculate_checksum(data: &[u8]) -> u32:
    157 |     let buffer = List[u8].new()  # ERROR
       |                  ^^^^^^^^^^^^^^ heap allocation not allowed
       |
       = note: function is marked @noalloc
       = help: Use stack allocation or pass a pre-allocated buffer

Use cases:
  - Real-time systems (deterministic performance)
  - Embedded systems (limited heap or no allocator)
  - Cryptographic functions (constant-time requirements)
  - Kernel code (allocation restrictions)

@nocopy - No Implicit Copies
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The @nocopy attribute prevents large value copies:

    @nocopy
    fn process_image(img: &ImageData) -> Result[(), Error]:
        # Compiler ensures no copies of large values
        # All data passed by reference or move

    error[P0602]: implicit copy in @nocopy function
      ----> image.py:89:18
       |
    88 | @nocopy
    89 | fn process_image(img: &ImageData) -> Result[(), Error]:
    90 |     let backup = *img  # ERROR: copies large struct
       |                  ^^^^ implicit copy of 5 MB struct
       |
       = help: Use a reference instead: let backup = img

@nosyscall - No System Calls
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The @nosyscall attribute forbids operations that invoke system calls:

    @nosyscall
    fn pure_computation(x: int, y: int) -> int:
        return x * x + y * y  # OK
        # File I/O, network, threading â†’ compile error

Useful for:
  - Security-critical code (system call auditing)
  - Performance-critical paths (no kernel transitions)
  - Sandboxed execution

@bounds_checked / @no_bounds_check
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Explicit control over array bounds checking:

    @bounds_checked  # Always check, even in release mode
    fn safe_access(arr: &[int], idx: int) -> Optional[int]:
        if idx < arr.len():
            return Some(arr[idx])
        return None

    @no_bounds_check  # Requires unsafe block
    unsafe fn unchecked_access(arr: &[int], idx: int) -> int:
        return arr[idx]  # Caller guarantees idx is valid

@cost_budget - Performance Contracts
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For real-time and safety-critical systems, Pyrite allows specifying performance 
budgets as compile-time contracts:

    @cost_budget(cycles=100, allocs=0)
    fn parse_packet(data: &[u8]) -> Result[Packet, Error]:
        # Compiler enforces budget
        # Errors if exceeded
        var packet = Packet::default()  # Stack allocation OK
        packet.parse(data)?
        return Ok(packet)

Compilation error example:

    error[P0650]: cost budget exceeded
      ----> network.py:156:13
       |
    155 | @cost_budget(cycles=100, allocs=0)
    156 | fn parse_packet(data: &[u8]) -> Result[Packet, Error]:
    157 |     let buffer = List[u8].new()  # ERROR
        |                  ^^^^^^^^^^^^^^ heap allocation not allowed
        |
        = note: function specifies allocs=0 budget
        = note: List::new() allocates on heap
        = help: Use fixed-size array: [u8; N] or pass pre-allocated buffer

Budget parameters:
  â€¢ cycles=N: Maximum estimated instruction count
  â€¢ allocs=N: Maximum heap allocations allowed
  â€¢ stack=N: Maximum stack space (bytes)
  â€¢ syscalls=N: Maximum system calls allowed

Benefits:
  â€¢ **Guarantees for real-time systems:** Provable bounds
  â€¢ **Safety-critical certification:** Performance as correctness
  â€¢ **Executable documentation:** Budget is enforced, not hoped for
  â€¢ **Regression detection:** CI fails if budget exceeded

Integration with profiling:
  â€¢ quarry cost estimates actual cost vs budget
  â€¢ quarry perf measures runtime cost vs budget
  â€¢ quarry tune suggests how to meet budget

Use cases:
  â€¢ Real-time audio/video processing
  â€¢ Flight control software
  â€¢ Medical device firmware
  â€¢ High-frequency trading
  â€¢ Kernel interrupt handlers

Example: Constant-time cryptography

    @cost_budget(cycles=5000, allocs=0, branches=0)
    fn verify_signature(sig: &[u8; 64], msg: &[u8], key: &PublicKey) -> bool:
        # Compiler ensures constant-time execution
        # No data-dependent branches allowed
        # No heap allocation allowed
        ...

This transforms "performance requirements" from documentation into compiler-
verified contracts. Beta Release feature.

Call-Graph Blame Tracking
~~~~~~~~~~~~~~~~~~~~~~~~~~

Performance contracts compose across function boundaries with blame tracking. When 
a contract violation occurs, the compiler shows the complete call chain and 
identifies which callee caused the violation:

    @cost_budget(allocs=0)
    fn process_data(input: &[u8]) -> Result[Data, Error]:
        let parsed = parse_input(input)?
        return Ok(transform(parsed))

Compilation error with blame chain:

    error[P0650]: @cost_budget violation: heap allocation
      ----> src/processor.py:45:22
       |
    43 | @cost_budget(allocs=0)
    44 | fn process_data(input: &[u8]) -> Result[Data, Error]:
    45 |     let parsed = parse_input(input)?
       |                  ^^^^^^^^^^^^^^^^^^^ allocation occurs in this call
       |
       = note: Allocation chain:
         1. process_data() [marked @cost_budget(allocs=0)]
            â†’ calls parse_input()
         2. parse_input() 
            â†’ calls tokenize()
         3. tokenize() at line 234
            â†’ allocates List[Token].new() [VIOLATES BUDGET]
       
       = help: To fix, choose one approach:
         1. Pass pre-allocated buffer to parse_input:
            fn parse_input(input: &[u8], buffer: &mut List[Token])
         
         2. Mark parse_input as @may_alloc (documents allocation):
            @may_alloc
            fn parse_input(input: &[u8]) -> Result[Tokens, Error]
         
         3. Refactor tokenize() to use fixed-size array:
            const MAX_TOKENS = 100
            var tokens: [Token; MAX_TOKENS]
       
       = explain: Run 'pyritec --explain P0650' for performance contracts guide

Cross-Module Blame:

    error[P0601]: @noalloc violation in safe_compute()
      ----> src/main.py:67:15
       |
    65 | @noalloc
    66 | fn safe_compute(x: f64) -> f64:
    67 |     return math::advanced_sqrt(x)  # Calls external function
       |            ^^^^^^^^^^^^^^^^^^^^^^ heap allocation occurs here
       |
       = note: Allocation chain across modules:
         1. safe_compute() [your code, marked @noalloc]
            â†’ calls math::advanced_sqrt() [external crate 'advanced_math']
         2. advanced_math::advanced_sqrt() at advanced_math-1.2.3/src/lib.py:89
            â†’ allocates Vec[f64] for iterative approximation [VIOLATES @noalloc]
       
       = help: The external function 'advanced_sqrt' is not marked @noalloc.
               Options:
         1. Use standard library math::sqrt() instead (marked @noalloc)
         2. File issue with 'advanced_math' crate to add @noalloc variant
         3. Remove @noalloc from safe_compute() and document allocation

Benefits:
  â€¢ **Root cause analysis:** Know exactly which function violated the contract
  â€¢ **Transitive enforcement:** Contracts propagate through call graph
  â€¢ **Actionable fixes:** Compiler suggests specific solutions at each level
  â€¢ **Cross-crate tracking:** Works across dependency boundaries
  â€¢ **Refactoring confidence:** Change internals, contract violations caught

Why This Matters:

Call-graph blame transforms performance contracts from "catch violations" to 
"understand why." Instead of "allocation occurred somewhere," you get "allocation 
occurred in tokenize() at line 234, called from parse_input(), called from your 
@cost_budget function." This makes contracts practical for large codebases and 
external dependencies.

Beta Release flagship feature.

Cost Transparency Warnings
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Even without explicit attributes, the compiler warns about potentially expensive 
operations (configurable via lint levels):

    # Warn on heap allocations
    [warn(heap_alloc)]
    fn my_function():
        let v = List[int].new()  # warning: heap allocation
    
    # Warn on copies > 1KB
    [warn(large_copy, threshold=1024)]
    fn another_function():
        let big = large_struct  # warning if > 1KB

    # Warn on dynamic dispatch
    [warn(dynamic_dispatch)]
    fn uses_trait_object(obj: &dyn Trait):
        obj.method()  # warning: indirect call via vtable

Integration with Linter
~~~~~~~~~~~~~~~~~~~~~~~~

Cost transparency is also enforced through quarry lint levels:

    quarry lint --level=performance

Reports:
  â€¢ All heap allocations with size estimates
  â€¢ All implicit copies with byte counts
  â€¢ Dynamic dispatch call sites
  â€¢ Potential reallocation points (e.g., List growth)
  â€¢ Lock contention points in concurrent code

Output format:

    performance: heap allocation (estimated 1024 bytes)
      ----> src/parser.py:234:9
      |
    234 |     let tokens = List[Token].new()
      |
      = help: Consider pre-allocating with with_capacity(estimated_size)
      = impact: May allocate multiple times as list grows

    performance: large copy (4096 bytes)
      ----> src/renderer.py:567:14
      |
    567 |     process_buffer(buffer)  # buffer is 4KB struct
      |
      = help: Pass by reference: process_buffer(&buffer)

Summary
~~~~~~~

Cost transparency attributes make Pyrite's "no hidden costs" philosophy 
enforceable:

  â€¢ Beginners learn performance intuition through warnings
  â€¢ Experts gain hard guarantees for critical code
  â€¢ Auditors can verify performance requirements
  â€¢ Documentation becomes executable contracts

No other systems language provides this level of provable cost control while 
maintaining high-level ergonomics.

================================================================================
5. MEMORY MANAGEMENT AND OWNERSHIP
================================================================================

One of Pyrite's cornerstone features is its approach to memory management: 
memory-safe by default with the ability to manually manage memory when necessary, 
all achieved without a garbage collector or runtime penalty. This section details 
Pyrite's ownership model, how allocation and deallocation work, and how we 
achieve safety without runtime overhead.

5.1 Ownership Model and RAII
--------------------------------------------------------------------------------

Pyrite uses a system of ownership and lifetimes reminiscent of Rust's borrow 
checker to ensure memory safety at compile time. However, the aim is to present 
it in a simplified, more Pythonic way to make it easier for newcomers to grasp. 

The key idea is that every value in Pyrite is owned by some variable (or 
temporary), or is borrowed from one. There is always a clear owner responsible 
for freeing the value when it's no longer needed.

Resource Ownership
~~~~~~~~~~~~~~~~~~

When we say a variable owns an object, it means that variable is responsible for 
the object's lifetime. For stack-allocated values (those without an explicit heap 
allocation), ownership is simple: the value lives until the variable goes out of 
scope (end of the block or function), at which point it is automatically cleaned 
up (like a stack frame being popped). For heap-allocated resources, ownership 
means the owner variable is responsible for freeing that memory when it goes out 
of scope. 

In Pyrite, when an owner goes out of scope, the language will invoke code to 
release any resources it owned (memory, file handles, etc.). This is analogous to 
RAII in C++ and is how Rust's ownership model works as well. In practical terms, 
the Pyrite compiler will automatically insert calls to destructors or free 
functions at the end of scope for owned objects.

Moves and Scope-Based Destruction
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

As mentioned in the type system, when an owning variable goes out of scope, 
Pyrite ensures the resource it owns is freed exactly once. For example, consider:

    {
        let data = List[int]([1,2,3])
        # ... use data ...
    }  # scope ends here
    # data is automatically freed here (memory reclaimed)

Inside the block, data owns a dynamic array allocated on the heap. When the block 
exits, because data is going out of scope, the compiler generates code to free 
the array's memory. This happens deterministically at the end of the scope, not 
at some unpredictable time in the future (there is no garbage collector pausing 
the program). 

The compile-time ownership analysis guarantees that in safe code, every 
allocation has a matching free at the right time: no leaks (each allocated 
resource has an owner that eventually frees it) and no double frees (because 
ownership prevents two owners of the same resource). If a resource needs to live 
longer than the scope in which it's created, you must explicitly move it to an 
outer scope (e.g. by returning it from a function or assigning it to a variable 
in an enclosing scope).

Transfer of Ownership (Move semantics)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ownership can be transferred by moving values between variables or across 
function calls. In Pyrite, assigning an object to a new variable or passing it as 
an argument will move it by default, unless the type is marked Copy. 

For instance:

    fn process_list(lst: List[int]):
        print("Length:", lst.length())
        # lst goes out of scope here, so it will be freed automatically
    
    var mylist = List[int]([10,20,30])
    process_list(mylist)  # moves ownership of mylist into the function
    # after this call, mylist is invalid (cannot be used) unless it was returned back

Here, process_list takes a List[int] by value, meaning it will own the list 
while in the function. When we call process_list(mylist), we move the list into 
the function. The local variable mylist in the caller loses ownership - 
essentially, it's emptied. If we try to use mylist after that call, the compiler 
will give an error, because mylist no longer owns the list (it was moved). 

Inside process_list, lst is a new owner of the list. When process_list returns, 
lst goes out of scope, and thus the list is freed at that point. This ensures the 
memory is freed exactly once overall. If we did want to use mylist again, 
process_list would have to return it (transfer it back), or we would pass a 
reference instead of moving (see borrowing below). These move semantics prevent 
having two owners to the same heap allocation at the same time (which would cause 
double-free) and also prevent use-after-free: once a value is moved, any attempt 
to use the old handle is a compile-time error, so you can't accidentally use an 
object after it's been freed.

Borrowing (References)
~~~~~~~~~~~~~~~~~~~~~~~

Often, you don't want to transfer ownership but merely lend access to some data 
to a function or another part of code. This is where borrowing comes in. Instead 
of passing objects by value, you can pass them by reference. In Pyrite, borrowing 
is achieved by using reference types (&T or &mut T). When you pass a reference, 
you are not giving up ownership; you are lending the object for a certain scope. 

There are two kinds of references:

  â€¢ Immutable references: &T - allow the borrower to read (but not modify) the 
    value.
  â€¢ Mutable references: &mut T - allow the borrower to modify the value, but you 
    can only have one active mutable borrow at a time.

The borrowing rules are: 
  1. You may have multiple immutable (&T) references to the same object at the 
     same time (read-only sharing is OK). 
  2. You may have at most one mutable (&mut T) reference to an object at a time, 
     and only if no immutable references exist to that object at that time 
     (exclusive write access).

These rules are enforced by the compiler and ensure there are no data races or 
conflicting accesses: you can't have two things trying to write to the same data 
simultaneously, and you can't read stale data while someone else is writing, etc. 
It's essentially Rust's borrowing rules enforced in Pyrite's context. 

For example:

    fn sum(numbers: &List[int]) -> int:
        var total = 0
        for n in numbers:  # iterate (read-only) through the list
            total += n
        return total
    
    let nums = List[int]([5,6,7,8])
    let total = sum(&nums)  # pass an immutable reference to nums
    print(nums[0])  # nums can still be used here since only an immutable borrow was taken

In this code, sum takes &List[int], meaning it borrows a read-only reference to a 
list of ints. We call sum(&nums), lending it our nums list. Inside sum, it can 
read through the list but not modify it. After sum returns, we still own nums in 
the caller, and we can continue using it. 

If sum tried to keep a reference to nums and return it or store it globally, the 
compiler would reject it because that could lead to a reference outliving its 
owner. Indeed, Pyrite's compiler internally tracks the lifetimes of references to 
ensure you don't end up with dangling pointers: a reference cannot outlive the 
variable it refers to. If you attempt something unsafe like returning a reference 
to a local variable, the compiler will give an error (e.g. "returning reference 
to local variable which will be dropped"). This way, dangling pointers are 
prevented at compile time - either you have ownership (and after scope end you 
can't use it), or you have a borrow (and the compiler ensures it doesn't outlast 
the owned value).

A mutable borrow &mut T works similarly but with the exclusivity rule. If you 
have passed &mut nums to a function, you cannot use nums in the caller during 
that time (the compiler treats nums as temporarily moved/locked for that scope). 
This prevents concurrent mutation issues. Data races (unsynchronized concurrent 
writes) are eliminated in safe Pyrite because you simply can't alias mutable 
references.

Lifetimes (under the hood)
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pyrite's compiler uses a concept of lifetimes (much like Rust) to reason about 
how long references are valid. However, the goal is to keep this implicit for 
most users. In straightforward code, the compiler can infer lifetimes without any 
annotations - e.g. a reference passed into a function is valid until that 
function returns, etc., following simple rules. 

Only in advanced scenarios (like complex self-referential structures or writing 
low-level data structures) might the programmer need to help the compiler with 
lifetime annotations or tricks. The idea is that beginners can use references in 
the obvious ways without needing to know the theoretical details; if they violate 
the rules, they'll just get a compiler error that says something like "error: 
borrowed value does not live long enough" and perhaps an explanation, which guides 
them to adjust their code. Over time, this teaches the concept of lifetimes 
gently.

In essence, the ownership and borrowing system ensures memory safety: no 
use-after-free, no double free, no null dereference (in safe code). Each resource 
is owned by exactly one entity at a time, and the compiler checks any aliasing 
through references to ensure it's safe. This allows Pyrite to be memory-safe by 
default without needing a garbage collector. 

The cost is entirely at compile time (the compiler does the analysis); at 
runtime, you pay nothing for these safety guarantees. Developers coming from 
manual memory management will find that Pyrite provides the same control (you can 
decide exactly when something is freed by controlling its scope or explicitly 
freeing in unsafe code), while eliminating the most common errors through 
compile-time enforcement.

5.2 Manual Memory Management (Unsafe)
--------------------------------------------------------------------------------

While the default ownership model covers most needs safely, Pyrite recognizes 
that systems programming sometimes requires manual control over memory 
allocation and layout, or interfacing with code that doesn't conform to Pyrite's 
safety rules. For those cases, Pyrite provides unsafe features that allow the 
programmer to step outside the compile-time guarantees. These must be used with 
care and are not intended for everyday programming - they are escape hatches for 
special scenarios. 

Crucially, unsafe operations are explicitly marked and isolated so that the 
boundaries between safe and unsafe code are clear. Safe code cannot be 
compromised by unsafe code unless you explicitly opt into it.

Unsafe Blocks
~~~~~~~~~~~~~

To perform an operation that the compiler cannot verify as safe, you must enclose 
it in an unsafe block (or be inside an unsafe fn). This signals to the compiler 
(and to readers of the code) that "here be dragons" - the normal guarantees are 
suspended and the programmer takes responsibility for upholding safety manually. 

For example:

    unsafe:
        let p = malloc(100) as *mut u8
        # allocate 100 bytes on heap, returns a raw pointer
        if p == null:
            abort("Out of memory")
        p[0] = 123  # write to the allocated memory
        free(p)     # free the memory

In this code, we explicitly called a malloc function (from, say, the C standard 
library or Pyrite's std) to allocate raw memory, which returns a *mut u8 (raw 
pointer to byte). We then check for null and use p by writing to it and finally 
freeing it. All of this must be in unsafe because raw pointer manipulation and 
manual memory free are not checked by the compiler - it's up to us to ensure 
correctness. 

Inside an unsafe block, you can do things that are normally forbidden, such as 
dereferencing raw pointers, performing pointer arithmetic, casting between 
incompatible pointer types, or calling foreign functions that the compiler cannot 
guarantee to be safe. Essentially, the compiler "trusts" you within an unsafe 
block.

However, note that even in an unsafe context, not everything is allowed: the 
basic language rules still apply (you can't magically treat an int as a float 
without a cast, for example, and you can't violate the fundamentals of the type 
system). The borrow checker also still works for references - if you use normal 
&T references inside unsafe, the compiler will enforce borrowing rules unless you 
intentionally convert them to raw pointers to circumvent the rules. The unsafe 
keyword does not turn off all checks; it only permits the specific dangerous 
operations that are otherwise disallowed (like raw pointer dereference or calling 
an unsafe function). This means even inside unsafe, you get as much help from the 
compiler as possible.

Explicit Allocation/Free
~~~~~~~~~~~~~~~~~~~~~~~~

Pyrite's standard library will likely include functions analogous to C's malloc, 
free, and realloc for raw memory management (or it will allow you to call C's 
versions easily via FFI). These are considered unsafe to use because they deal 
with raw pointers and unstructured memory. A programmer who wants to manually 
manage a memory region might use these to allocate a buffer and then manipulate 
it. 

Typically, one would encapsulate such allocations in a safe abstraction. For 
example, you might implement a custom data structure by using malloc to get a 
buffer and then constructing elements in place. Pyrite encourages isolating this 
kind of code: write the low-level memory manipulation inside a module, mark it 
unsafe, thoroughly test it, and then expose a safe API (much like how one would 
write portions of a Rust library in unsafe internally but present a safe 
interface). This way the majority of the codebase remains safe Pyrite, and only a 
tiny core is unsafe.

Unsafe Data Structures and Type Punning
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In some low-level scenarios, you may need to reinterpret bits of memory in a 
different type (type punning), or use a union to overlay different 
representations. Pyrite allows these in unsafe code. For instance, you could cast 
a *u8 to a *MyStruct after ensuring the memory is aligned and sized correctly. Or 
use a union as defined earlier: reading from a union field is only safe if you 
know that field is the one last written. If you misuse it, that's a bug the 
compiler can't catch in unsafe code. 

Pyrite's philosophy is that any trick you can do in C to optimize or interface 
with hardware, you can do in Pyrite as well - but you must opt into unsafe and 
thus you accept the responsibility. The set of operations that are considered 
undefined behavior (and thus must be kept within unsafe and done correctly) 
include things like dereferencing invalid pointers, misaligning data, creating 
data races, etc., similar to Rust's model of what's not allowed even in unsafe.

Opting Out of the Borrow Checker
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

There are cases where you might need to circumvent the borrowing rules. For 
example, you might have an API that requires two mutable pointers to the same 
data (maybe for some algorithm that treats the data in chunks). In safe Pyrite 
this is impossible, but in unsafe you could do it. 

For instance:

    fn give_two_aliases(x: &mut int):
        unsafe:
            let p1 = x as *mut int
            let p2 = x as *mut int
            # Now p1 and p2 are two mutable aliases to the same integer
            *p1 = 5
            *p2 = 6  # this is undefined behavior if used concurrently

Here we took a mutable reference x (which we got in a safe way from the function 
parameter) and cast it to raw pointers p1 and p2. We thereby created two aliases 
to the same data and used them. This is not allowed in safe code (the borrow 
checker would have prevented it), which is why we had to be in unsafe.

If this code runs in a single-threaded context sequentially, it will simply set 
the integer to 6. But if one tried to do such things concurrently, it could cause 
a data race - the compiler won't help you here because you turned off the checks. 
This example is contrived, but it illustrates that unsafe can be used to 
temporarily opt out of the strict rules. Any invariants you break in unsafe code, 
you as the programmer must ensure they don't violate overall program safety 
(perhaps through other means like synchronization).

In summary, unsafe is a tool of last resort. Pyrite provides it so that you can 
do things like manual memory allocation, low-level fiddling, interfacing with 
hardware or foreign libraries, etc., which are indispensable for systems 
programming. But all unsafe actions must be explicitly marked, and safe and 
unsafe code are kept separated as much as possible. This design mirrors Rust's 
approach to unsafe: you contain the "unsafe mess" in small, well-audited 
portions. By doing so, you get the benefit that the bulk of your program is 
verified by the compiler to be free of memory errors, and you only have to reason 
deeply about the unsafe parts.

================================================================================
6. CONTROL FLOW
================================================================================

Pyrite's control flow structures combine Python's simplicity with C's low-level 
capabilities. All the usual constructs are present (conditionals, loops, pattern 
matching, etc.) with a syntax leaning towards Python's readability. There are 
also some additions inspired by modern languages to handle errors and pattern 
matching in a safer way.

6.1 Conditionals
--------------------------------------------------------------------------------

The if statement in Pyrite is similar to Python's. For example:

    if condition:
        # block of code if condition is true
    elif other_condition:
        # block if the first condition is false, second is true
    else:
        # block if all above conditions are false

The elif and else clauses are optional (you can have just an if, or an if with 
multiple elifs, etc.). The condition in an if must be of type bool - there is no 
implicit conversion from integers or other types to boolean. For instance, if x: 
is not allowed if x is an integer; you must write if x != 0: or some explicit 
check. This avoids ambiguity and mistakes. 

As mentioned earlier, logical operators are the words and, or, not, which also 
helps readability (e.g. if x > 0 and not done:). Parentheses around the 
condition are optional (you can write if (x > 0): if you want, but it's not 
required).

Pyrite also offers a ternary conditional expression (similar to Python's): x if 
condition else y. This allows inline conditional logic. For example: max_val = a 
if a > b else b will set max_val to the greater of a or b. This is purely an 
expression and does not use a question-mark like C; it uses the Python-style 
syntax for familiarity.

6.2 Loops
--------------------------------------------------------------------------------

Pyrite provides both while loops and for loops.

While Loop
~~~~~~~~~~

The syntax is straightforward:

    while condition:
        # loop body
        # (use `break` to exit early, `continue` to skip to next iteration)

The loop evaluates the condition (which must be a boolean) at the start of each 
iteration; if true, it executes the body, then repeats. If false, it exits the 
loop. Inside the loop, you can use break to immediately exit the loop, or 
continue to skip to the next iteration (re-checking the condition). This works 
just like in C/Python.

For Loop
~~~~~~~~

Instead of C's traditional for (init; cond; step) loop, Pyrite adopts a 
high-level iteration loop similar to Python's for ... in .... The Pyrite for loop 
iterates over any iterator or range of values. 

For example:

    for item in collection:
        print(item)

This will iterate through each element of collection (which could be, say, a List 
or an Array). Under the hood, collection should be something that provides an 
iterator (likely by implementing a trait in the standard library), but the syntax 
is clean and abstracted. 

If you need a numeric loop, you could iterate over a range of numbers. Pyrite 
will likely have a range type or literal. For instance, for i in 0..10: might 
iterate i from 0 up to 9 (if using a Rust-like range syntax), or for i in 
range(0, 10): might be used (if adopting Python's range() function style). The 
exact syntax is to be determined, but the concept is that you don't manually 
write index increment logic; you express what set of values to iterate over, and 
the language handles the progression.

You can also loop directly over a fixed-size array or any iterable. If you need 
the index as well, typical solutions include an enumerate iterator in the 
library, or a C-style loop using while with an index. Pyrite chooses to 
prioritize clarity and safety (no off-by-one errors from manual indexing if you 
use the high-level loop). As with while, break can exit the loop early and 
continue goes to the next iteration.

Example:

    let nums = List[int]([10,20,30])
    for n in nums:
        print(n)

This would print each number in the list. If nums was empty, the loop would 
simply not execute at all (0 iterations). The loop construct abstracts away the 
indexing and bounds-checking (the iterator will handle that safely).

6.3 Pattern Matching
--------------------------------------------------------------------------------

Borrowing a powerful feature from Rust (and many functional languages), Pyrite 
includes a match construct for pattern matching on values, particularly on enums 
and other algebraic data types. Pattern matching allows you to branch on the 
structure of a value in a concise way, and the compiler ensures you handle all 
possible cases.

Syntax:

    match value:
        pattern1:
            # code for when value matches pattern1
        pattern2 if guard:
            # code for pattern2 with an additional boolean guard condition
        _:
            # default case if no other pattern matches

Each pattern: is like a case arm. The special pattern _ is a wildcard that 
matches anything (and is used as a default or catch-all). The if guard after a 
pattern is optional and allows an extra boolean condition to refine the match.

For example, imagine an enum result type:

    match result:
        Ok(val):
            print("Success:", val)
        Err(err_code):
            print("Error:", err_code)

Here result is matched against two patterns: Ok(val) and Err(err_code), which 
correspond to the variants of a Result enum. If result is an Ok, the val inside 
will be bound to the variable val and the success case code runs; if it's an Err, 
the err_code is bound and the error case code runs. The compiler will insist that 
this match is exhaustive - if Result had another variant, or if we omitted the 
Err case, it would be a compile error to have a non-exhaustive match (unless we 
include a wildcard _ to cover "all other cases"). This exhaustiveness checking 
helps prevent logic errors by ensuring you've considered every possible variant of 
an enum.

Pattern matching isn't limited to enums. You can match on basic types (e.g., 
different specific integers or strings), on tuples, on struct destructuring, etc. 

For instance:

    match x:
        0:
            print("x is zero")
        1 | 2:
            print("x is 1 or 2")
        _:
            print("x is something else")

Or for a tuple:

    match point:
        (0, y):
            print("Point lies on the Y axis at", y)
        (x, 0):
            print("Point lies on the X axis at", x)
        (x, y):
            print("Point at", x, ",", y)

This allows very expressive checking of complex conditions in a declarative style. 
It often leads to clearer code than a sequence of if/elif especially when dealing 
with structured data. New users can initially ignore match and use if/else, but 
as they become more comfortable, match becomes a potent tool that can make code 
more readable and robust (since the compiler checks that all cases are handled).

6.4 Function Calls and Operators
--------------------------------------------------------------------------------

Function Calls
~~~~~~~~~~~~~~

Functions are called using parentheses and a comma-separated list of arguments, 
just like in most languages. Example: result = foo(a, b). There's no quirk like 
Python's mandatory self parameter for methods - method calls are handled 
differently (see Advanced Features on methods), and regular function calls are 
straightforward. Pyrite supports positional arguments and may support keyword 
arguments for functions (to improve readability for some calls), though that's a 
design choice beyond the core syntax.

Expressions and Operators
~~~~~~~~~~~~~~~~~~~~~~~~~~

Pyrite supports the typical arithmetic and comparison operators: +, -, *, /, % 
for arithmetic; ==, !=, <, <=, >, >= for comparisons. Operator precedence is 
similar to C/Python (for example, * and / bind tighter than + and -). Parentheses 
can be used to override precedence or for clarity. 

One important design decision in Pyrite is that it does not allow user-defined 
operator overloading by default. That is, you cannot arbitrarily change what + 
does for your custom type in the language itself, to avoid the pitfalls of hidden 
control flow or surprising performance costs. For built-in numeric types, + is 
defined as usual addition. The language might also allow + for certain standard 
library types (like concatenating strings or adding two same-typed vectors) for 
convenience, but these are well-defined by the language/standard library. 

Beyond those cases, users should define ordinary functions or methods (e.g. use a 
method like .add() or a trait implementation, see advanced section) to implement 
custom operations. This way, seeing an operator in code is never misleading - a + 
b is either basic arithmetic or a documented library-defined overload (not an 
arbitrary function call). Future versions of Pyrite might introduce operator 
overloading via traits (similar to Rust's Add trait) in a controlled manner, but 
initially, to keep things simple, we assume no custom operator overloading. This 
ensures that, for example, a + b is always O(1) for fundamental types and won't 
secretly do something like allocate memory or call a complex routine without the 
programmer realizing it.

Order of Evaluation
~~~~~~~~~~~~~~~~~~~

Pyrite guarantees left-to-right evaluation order for all expressions and function 
arguments. This eliminates an entire class of undefined behavior present in C/C++ 
and makes code behavior predictable and debuggable.

For example:
    
    fn side_effect() -> int:
        print("Called!")
        return 5
    
    fn process(a: int, b: int, c: int):
        pass
    
    process(side_effect(), side_effect(), side_effect())
    # Guaranteed to print "Called!" three times in order
    # Arguments evaluated left-to-right: first, second, third

This deterministic evaluation order:
  â€¢ Makes debugging easier (predictable execution flow)
  â€¢ Prevents surprising behavior with side effects
  â€¢ Aligns with Python's predictability expectations
  â€¢ Costs nothing in performance (compilers already choose some order)
  â€¢ Removes cognitive load from developers

While it's still good practice to avoid complex expressions with multiple side 
effects, Pyrite ensures that when they occur, behavior is consistent and 
understandable.

6.5 Error Handling
--------------------------------------------------------------------------------

Rather than using exceptions for error handling, Pyrite opts for a more explicit, 
type-based approach that avoids hidden control flow and runtime overhead. There 
are no throw or try/catch statements in the traditional sense (as in Java or 
C++). Instead, Pyrite encourages the use of result types (like the Result[T, E] 
enum described earlier) and provides some syntactic sugar to make handling them 
ergonomic. This approach is influenced by Rust and Zig.

Result Type
~~~~~~~~~~~

If a function can fail (for example, a function that reads a file might fail if 
the file is not found), the function's return type is a Result (or an Optional in 
simpler cases). For example: fn read_config(path: String) -> Result[Config, 
IOError] would indicate that the function returns either a Config object on 
success or an IOError on failure.

The caller of such a function is forced by the type system to handle the 
possibility of error - either by checking which variant it is (via pattern 
matching or an if), or by propagating the error upward.

The try Operator
~~~~~~~~~~~~~~~~

To avoid boilerplate when you have many fallible calls, Pyrite could provide a 
try operator (similar to Rust's ?). In Pyrite syntax, we use a keyword (since ? 
might be a valid identifier character and for Pythonic clarity, a word fits). 

For example:

    fn load_config_or_default(path: String) -> Result[Config, IOError]:
        file = try File.open(path)  # if open fails, propagate the error up
        data = try file.read_all()  # if read fails, propagate error
        return parse_config(data)   # parse_config returns Config (assume no error for simplicity)

In this code, try File.open(path) means: call File.open(path) which returns a 
Result[File, IOError]. If it returns Ok(file), then file is assigned and 
execution continues. If it returns Err(e), then the try will return that error 
from the current function (load_config_or_default), effectively propagating the 
error to our caller. This happens without unwinding the stack or throwing an 
exception; it's a simple jump out of the function, which the compiler implements 
as needed (essentially equivalent to an if err != None: return Err(err) pattern). 

The same with try file.read_all(): if read_all fails, we exit 
load_config_or_default early with that error. If all goes well, we end up 
returning a Config (which will be wrapped as Ok(Config) implicitly, or we might 
explicitly wrap it). This try construct makes error propagation almost as concise 
as exceptions but with none of the hidden cost or control flow issues (you can 
statically see which functions may return errors by their type, and there's no 
unwinding at runtime - it's just code that checks and returns).

If a function wants to handle errors instead of propagating, it can use a match 
or if to inspect the Result. 

For instance:

    let result = File.open(path)
    match result:
        Ok(file):
            # use file
        Err(e):
            print("Could not open file:", e)
            return Err(e)  # or handle it somehow

Pyrite might also allow some sugar like a catch block or an if let-style 
construct to handle errors more succinctly, but under the hood, it's always using 
the Result type, not true exceptions.

No Exceptions
~~~~~~~~~~~~~

By not having language-level exceptions, Pyrite avoids the pitfalls of hidden 
control flow (where any function call might throw and skip the rest of your code 
unless you catch it). This means when you read Pyrite code, you don't have to 
worry that foo() will suddenly jump out to some handler unless it's explicitly 
written to do so via the Result mechanism. This determinism is important for 
reasoning about performance and correctness (no surprise performance hits from 
unwinding stack frames, etc.). In performance-critical or safety-critical 
systems, unpredictable control flow is undesirable. 

Pyrite's error handling is therefore opt-in at each call site: you either 
explicitly propagate or handle the error. There is talk of possibly a catch 
expression that can wrap a whole block and turn any error into a result, but that 
would just be a convenience that expands to checking each result.

Defer Statement
~~~~~~~~~~~~~~~

For cases where you need to guarantee code runs at scope exit (regardless of 
error or return path), Pyrite provides a defer statement (inspired by Go and Zig). 
The defer statement schedules a block of code to execute when the enclosing scope 
exits, making cleanup patterns explicit and guaranteed.

Syntax:
    
    fn process_file(path: String) -> Result[(), Error]:
        let handle = try open_resource(path)
        defer:
            close_resource(handle)  # Always runs when function exits
        
        # ... work with handle ...
        # If any errors occur and propagate, defer block still runs
        return Ok(())

Multiple defer statements execute in reverse order (last-deferred runs first):

    fn nested_cleanup():
        defer:
            print("Third")
        defer:
            print("Second")
        defer:
            print("First")
        # Prints: First, Second, Third

When to use defer vs RAII:
  â€¢ RAII: Preferred for resource management (files, locks, memory)
    Types with destructors automatically clean up
  
  â€¢ defer: Used when RAII is inconvenient:
    - C FFI resources without Pyrite wrappers
    - Multiple cleanup steps that don't fit one destructor
    - Explicit sequencing of cleanup operations
    - Temporary debugging/logging at scope exit

Example with C interop:

    fn use_c_library() -> Result[(), Error]:
        let ptr = unsafe { c_malloc(1024) }
        defer:
            unsafe { c_free(ptr) }
        
        # Use ptr safely...
        # Guaranteed cleanup even if errors occur

Benefits:
  â€¢ Beginners love "cleanup always runs" guarantee
  â€¢ Explicit and visible (no hidden destructor behavior)
  â€¢ Complements RAII for procedural-style code
  â€¢ Zero runtime cost (expanded at compile time)

Note: defer is part of Core Pyrite - it's simple enough for beginners and powerful 
enough for systems programming. While RAII handles most cleanup, defer provides 
an escape hatch for cases where ownership-based cleanup is awkward.

With Statement
~~~~~~~~~~~~~~

To provide familiar resource management patterns for Python developers, Pyrite 
includes a with statement that desugars to try + defer at compile time. This is 
pure syntactic sugar with zero runtime cost.

Syntax:

    with file = try File.open("config.txt"):
        for line in file.lines():
            process(line)
    # file.close() called automatically via defer

The with statement combines three operations:
  1. Bind the resource (let file = ...)
  2. Unwrap the Result (try)
  3. Register cleanup (defer: file.close())

Equivalent desugaring:

    let file = try File.open("config.txt")
    defer:
        file.close()
    for line in file.lines():
        process(line)

Multiple resources:

    with conn = try Database.connect(url):
        with tx = try conn.begin_transaction():
            tx.execute(query)
            tx.commit()
    # tx.rollback() if not committed, conn.close() - in reverse order

Requirements for use with `with`:
  â€¢ Type must implement the Closeable trait (or provide a close() method)
  â€¢ Expression must return a Result[T, E] where T: Closeable

Benefits:
  â€¢ Familiar to Python developers (reduces friction)
  â€¢ Zero cost (desugars to defer)
  â€¢ Explicit (you see the try, you see the with)
  â€¢ Teaches ownership (compiler shows desugared form with --explain)

When to use:
  â€¢ Opening files, network connections, database transactions
  â€¢ Acquiring locks (Mutex, RwLock)
  â€¢ Any resource with a clear "acquire â†’ use â†’ release" pattern

Teaching path:
  1. Week 1: Use with for familiar file operations
  2. Week 2: Learn that with = try + defer (show desugaring)
  3. Week 3+: Use defer directly when more control needed

The with statement is part of Core Pyrite - it's the ergonomic entry point to 
resource management that later teaches the underlying mechanisms.

In summary, Pyrite's approach to errors is to use the type system to make them 
explicit. This results in more robust code because you can't accidentally ignore 
an error (the compiler will warn or error if you don't use a Result you 
received). And you avoid the runtime cost of exceptions. This aligns with the 
overall philosophy of explicitness and zero-cost abstractions: error handling in 
Pyrite is as efficient as simple conditional checks and jumps (no stack unwinding 
machinery). It's also predictable - you know which functions might fail (it's in 
their type) and you handle it in-line with your logic.

================================================================================
7. ADVANCED FEATURES (TRAITS, GENERICS, AND MORE)
================================================================================

While Pyrite's core is procedural and data-oriented (like C), it offers 
higher-level abstraction mechanisms inspired by Rust's traits and Python's 
duck-typing, aiming to achieve flexibility without sacrificing performance. These 
features are optional - a beginner can ignore them initially - but they enable 
writing expressive and reusable code as one's proficiency grows.

7.0 Type Introspection: quarry explain-type
--------------------------------------------------------------------------------

To support Pyrite's "Intuitive Memory Model for Learners" (section 1.5), the 
tooling provides a first-class type introspection command that makes memory 
layouts and properties visible:

Command Usage
~~~~~~~~~~~~~

    quarry explain-type TypeName
    quarry explain-type TypeName --verbose

This command displays standardized "type badges" and memory characteristics in 
plain language, making low-level concepts tangible for beginners.

Example Output: Primitive Type
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    $ quarry explain-type int
    
    Type: int
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Properties: [Stack] [Copy] [ThreadSafe]
    
    Memory Layout:
      â€¢ Size: 4 bytes (32-bit) or 8 bytes (64-bit)
      â€¢ Alignment: Same as size
      â€¢ Location: Stack (inline in structs)
    
    Behavior:
      â€¢ Assignment copies the value (cheap)
      â€¢ Can be shared between threads safely
      â€¢ No cleanup needed (no destructor)
    
    Performance:
      â€¢ Copy cost: O(1) - single register copy
      â€¢ Pass by value: Efficient (fits in register)

Example Output: Collection Type
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    $ quarry explain-type List[int]
    
    Type: List[int]
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Properties: [Heap] [Move] [MayAlloc] [ThreadSafe*]
    
    Memory Layout:
      â€¢ Stack size: 24 bytes (3 words: pointer, length, capacity)
      â€¢ Heap size: Varies (capacity Ã— element size)
      â€¢ Total for empty list: 24 bytes stack + 0 bytes heap
      â€¢ Example: List with 10 ints: 24 bytes stack + 40 bytes heap
    
    Ownership:
      â€¢ Owns heap-allocated array of elements
      â€¢ Moving transfers ownership (original becomes invalid)
      â€¢ Borrowing: Use &List[int] to share read-only access
    
    Behavior:
      â€¢ Assignment moves (original cannot be used after)
      â€¢ Use .clone() to create a deep copy
      â€¢ Automatically frees heap memory when dropped
    
    Performance:
      â€¢ Drop cost: O(1) - single deallocation (elements are Copy)
      â€¢ Clone cost: O(n) - allocates and copies all elements
      â€¢ Push may reallocate: Use with_capacity() to avoid
    
    Thread Safety:
      â€¢ ThreadSafe*: Can be moved to other threads
      â€¢ Cannot be shared between threads without synchronization
      â€¢ Use Mutex<List[int]> for shared mutable access
    
    Common Patterns:
      â€¢ Pre-allocate: List[int].with_capacity(expected_size)
      â€¢ Borrow for reading: fn process(items: &List[int])
      â€¢ Take ownership: fn consume(items: List[int])

Example Output: Custom Type
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    $ quarry explain-type ImageBuffer
    
    Type: ImageBuffer
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Properties: [Stack] [Move] [NotCopy]
    
    Memory Layout:
      â€¢ Size: 1,000,016 bytes (1 MB array + 16 bytes metadata)
      â€¢ Alignment: 1 byte
      â€¢ Location: Stack or inline (WARNING: Very large)
    
    Structure:
      struct ImageBuffer:
          width: u32           # 4 bytes
          height: u32          # 4 bytes
          data: [u8; 1000000] # 1 MB array
    
    Behavior:
      â€¢ Assignment moves (too large to copy implicitly)
      â€¢ Pass by reference to avoid 1 MB copy
      â€¢ Automatically dropped (no heap allocation to free)
    
    Performance:
      â€¢ Copy cost: O(n) - 1 MB memcpy (expensive!)
      â€¢ Pass by value: Copies 1 MB onto stack (slow)
      â€¢ Pass by reference: O(1) - copies only pointer
    
    Recommendations:
      âš ï¸  This type is very large (1 MB) for stack allocation
      âœ“  Pass by reference: fn process(img: &ImageBuffer)
      âœ“  Use Box<ImageBuffer> to move to heap
      âœ“  Consider redesign: separate data buffer from metadata

Type Badge Reference
~~~~~~~~~~~~~~~~~~~~

The type introspection system uses standardized badges:

Memory Location:
  [Stack]     - Stored on the stack (or inline in parent)
  [Heap]      - Owns heap-allocated memory
  [Inline]    - Always inline (no indirection)

Ownership:
  [Copy]      - Cheap bitwise copy (can reuse after assignment)
  [Move]      - Move-only (original invalidated after assignment)
  [NotCopy]   - Explicitly not copyable

Allocation:
  [MayAlloc]  - Operations may allocate (e.g., push, insert)
  [NoAlloc]   - Never allocates memory

Thread Safety:
  [ThreadSafe]   - Can be moved or shared across threads
  [ThreadSafe*]  - Can be moved (Send) but not shared (not Sync)
  [NotThreadSafe] - Cannot cross thread boundaries

Views:
  [BorrowedView] - Refers to data owned elsewhere (e.g., &str, &[T])
  [OwnedData]    - Owns its data

Integration with Learning
~~~~~~~~~~~~~~~~~~~~~~~~~~

Type introspection serves multiple purposes:

1. **Beginner Learning:**
   - Run quarry explain-type on unfamiliar types
   - See memory layout visually ("24 bytes stack + 40 bytes heap")
   - Understand copy vs move behavior
   - Learn when to use & vs owned values

2. **Performance Debugging:**
   - Identify unexpectedly large types
   - See allocation behavior before profiling
   - Understand cache implications (size, alignment)

3. **API Documentation:**
   - Clearly communicate type costs
   - Show thread safety guarantees
   - Explain ownership semantics

4. **IDE Integration:**
   - Hover over type to see badges and summary
   - Quick-info shows size and properties
   - Inline hints for expensive types (>1KB)

Verbose Mode
~~~~~~~~~~~~

    $ quarry explain-type List[int] --verbose

Verbose mode adds:
  â€¢ Field-by-field layout with offsets
  â€¢ Padding and alignment explanation
  â€¢ Trait implementations (Debug, Clone, etc.)
  â€¢ Related types (Iterator, slices)
  â€¢ Example usage patterns with benchmarks

Why This Matters
~~~~~~~~~~~~~~~~~

This feature operationalizes section 1.5 ("Intuitive Memory Model for Learners"). 
Instead of reading documentation about stack vs heap, beginners can inspect any 
type and see:
  â€¢ WHERE it lives (stack, heap, inline)
  â€¢ HOW it behaves (copy, move, allocate)
  â€¢ WHAT it costs (size, copy cost, drop cost)

Every type becomes self-documenting. The explicit, beginner-friendly presentation 
teaches systems programming concepts through concrete examples rather than 
abstract theory.

Enhanced Layout and Aliasing Introspection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Building on the core quarry explain-type functionality, additional commands 
provide deep visibility into memory layout and aliasing guarantees for 
performance-critical code:

quarry layout - Detailed Memory Layout
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Shows exact memory layout including padding, alignment, and field offsets:

    $ quarry layout ImageBuffer
    
    Memory Layout: ImageBuffer
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Total Size: 1,000,016 bytes (1 MB + 16 bytes)
    Alignment: 4 bytes
    
    Field Layout (with padding):
    
    Offset â”‚ Size   â”‚ Align â”‚ Field
    â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       0   â”‚ 4 B    â”‚ 4 B   â”‚ width: u32
       4   â”‚ 4 B    â”‚ 4 B   â”‚ height: u32
       8   â”‚ 1 MB   â”‚ 1 B   â”‚ data: [u8; 1000000]
    
    Padding: 0 bytes (no padding needed)
    
    Recommendations:
      âš ï¸  Very large stack allocation (1 MB)
      âœ“  Consider heap allocation: Box<ImageBuffer>
      âœ“  Or separate buffer: struct ImageBuffer { width, height, data: &[u8] }

With padding example:

    $ quarry layout NetworkPacket
    
    Memory Layout: NetworkPacket
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Total Size: 24 bytes
    Alignment: 8 bytes
    
    Field Layout (with padding):
    
    Offset â”‚ Size   â”‚ Align â”‚ Field
    â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       0   â”‚ 1 B    â”‚ 1 B   â”‚ protocol: u8
       1   â”‚ (3 B)  â”‚ -     â”‚ [PADDING]
       4   â”‚ 4 B    â”‚ 4 B   â”‚ length: u32
       8   â”‚ 8 B    â”‚ 8 B   â”‚ timestamp: u64
      16   â”‚ 8 B    â”‚ 8 B   â”‚ checksum: u64
    
    Padding: 3 bytes (12.5% overhead)
    
    Optimization suggestion:
      Reorder fields to eliminate padding:
      
      struct NetworkPacket:
          timestamp: u64    # 8-byte aligned
          checksum: u64     # 8-byte aligned
          length: u32       # 4-byte aligned
          protocol: u8      # 1-byte aligned
      
      New size: 21 bytes (no padding needed)
      Savings: 3 bytes (12.5% reduction)

This teaches cache-line awareness and struct packing naturally through concrete 
examples.

Cache-Line Analysis
~~~~~~~~~~~~~~~~~~~

For performance-critical types, show cache implications:

    $ quarry layout ImageBuffer --cache-analysis
    
    Cache-Line Analysis
    ===================
    
    L1 Cache Line: 64 bytes
    
    ImageBuffer (1,000,016 bytes):
      â€¢ Spans: 15,626 cache lines
      â€¢ Sequential access: Optimal (contiguous)
      â€¢ Random access: 15,626 potential cache misses
    
    Hot Fields (frequently accessed together):
      â€¢ width + height: 8 bytes â†’ Same cache line âœ“
    
    Recommendations:
      For sequential processing: Current layout optimal
      For random access: Consider tiling (64Ã—64 blocks = 16 KB per tile)

Aliasing Analysis
~~~~~~~~~~~~~~~~~

Understand when the compiler can assume non-aliasing:

    $ quarry explain-aliasing
    
    Aliasing Guarantees in Pyrite
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    The compiler assumes non-aliasing (noalias) when:
    
    1. Exclusive mutable borrow (&mut T):
       
       fn process(data: &mut [f32]):
           # Compiler guarantees: 'data' is the ONLY mutable access
           # Enables: Aggressive vectorization, load/store reordering
       
    2. Multiple immutable borrows with disjoint provenance:
       
       let slice_a = &arr[0..100]
       let slice_b = &arr[100..200]
       process(slice_a, slice_b)  # Compiler proves disjoint
       
    3. Explicit @noalias attribute (expert-level):
       
       @noalias
       fn kernel(a: &[f32], b: &[f32], c: &mut [f32]):
           # Programmer asserts: a, b, c don't overlap
           # Compiler trusts and optimizes aggressively

For specific functions, show aliasing assumptions:

    $ quarry explain-aliasing process_vectors
    
    Aliasing Analysis: process_vectors
    ===================================
    
    fn process_vectors(a: &mut [f32], b: &mut [f32]):
        for i in 0..a.len():
            a[i] += b[i]
    
    Aliasing Assumptions:
      â€¢ 'a' and 'b' MAY alias (conservative)
      â€¢ Compiler cannot prove disjointness
      â€¢ Optimization: Limited (must assume overlap)
    
    Performance Impact:
      â€¢ Vectorization: Possible but conservative
      â€¢ Load/store reordering: Disabled (aliasing concern)
      â€¢ Estimated slowdown: 12-18% vs proven-disjoint case
    
    Solutions:
    
      1. If inputs are ALWAYS disjoint, add @noalias:
         
         @noalias
         fn process_vectors(a: &mut [f32], b: &mut [f32]):
         
         âš ï¸  Warning: Undefined behavior if inputs overlap
         âœ“  Benefit: 12-18% speedup (enables aggressive opts)
      
      2. Split into separate functions if provable:
         
         fn process_vector_pair(data: &mut [[f32; 2]]):
             # Compiler proves disjoint (separate array elements)
      
      3. Accept the cost if rarely called

This makes aliasing - an invisible, expert-level concern - concrete and actionable.

Integration with quarry cost
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    $ quarry cost --aliasing-analysis
    
    Functions Limited by Aliasing Assumptions
    ==========================================
    
    3 functions would benefit from @noalias:
    
      1. process_vectors (src/math.py:234)
         â€¢ Current: Conservative aliasing assumptions
         â€¢ Potential: 15% speedup with @noalias
         â€¢ Verified safe: Manual inspection needed
      
      2. kernel_compute (src/kernels.py:567)
         â€¢ Current: 4-wide SIMD (aliasing limits width)
         â€¢ Potential: 8-wide SIMD with @noalias = 2x speedup
         â€¢ Verified safe: Check callsites for overlap

Why This Matters
~~~~~~~~~~~~~~~~~

Enhanced layout and aliasing visibility completes the "intuitive memory model" 
story:

  â€¢ **Layout inspection teaches:**
    - Cache-line awareness (64-byte boundaries)
    - Struct padding and alignment rules
    - Memory access patterns and performance
  
  â€¢ **Aliasing inspection teaches:**
    - When compiler can optimize aggressively
    - Why &mut T enables vectorization
    - When @noalias provides benefit
  
  â€¢ **Integration with existing tools:**
    - quarry explain-type: WHAT the type is
    - quarry layout: HOW it's arranged in memory
    - quarry explain-aliasing: WHEN compiler can optimize
    - quarry cost: WHERE to apply @noalias

This trio makes performance optimization systematic:
  1. Profile with quarry perf (find hot function)
  2. Inspect with quarry layout (understand memory)
  3. Check with quarry explain-aliasing (identify optimization opportunities)
  4. Apply @noalias if provably safe
  5. Measure improvement

Implementation: Stable Release (after core quarry explain-type is stable)

7.1 Traits and Generics (Parametric Polymorphism)
--------------------------------------------------------------------------------

Traits (Interfaces)
~~~~~~~~~~~~~~~~~~~

A trait in Pyrite is a way to define a set of methods (function signatures) that a 
type can implement. Traits serve a purpose similar to interfaces in Java or 
abstract base classes in C++, and they are akin to Rust's traits. They enable 
ad-hoc polymorphism, meaning you can write functions that operate on any type 
that "implements" a certain trait, without caring about the concrete type. This 
is all done at compile time (zero runtime cost for the polymorphism, unless 
explicitly using dynamic dispatch).

For example, we might define a trait Comparable that requires a cmp method:

    trait Comparable:
        fn cmp(&self, other: &Self) -> int

This trait Comparable says any type that implements it must have a method cmp 
which compares it with another of the same type, returning an int (negative for 
less-than, 0 for equal, positive for greater-than, perhaps). This is similar to 
how many languages define a compare function, or Rust's Ord/PartialOrd traits.

To implement a trait for a type, Pyrite uses an implementation block:

    impl Comparable for int:
        fn cmp(&self, other: &int) -> int:
            return *self - *other

Here we implement Comparable for the built-in int type (assuming we allow that; 
primitive types could have trait implementations). We define cmp to subtract the 
two values, which works as a comparison (if result < 0, self < other; etc.).

For a custom struct:

    struct Person:
        name: String
        age: int
    
    impl Comparable for Person:
        fn cmp(&self, other: &Person) -> int:
            return self.age - other.age

Now Person implements Comparable by comparing ages. We can use this trait to 
write generic algorithms:

Generics
~~~~~~~~

Pyrite allows defining functions or data types with type parameters (like 
templates in C++ or generics in Rust/Java). For instance, a generic function to 
get the maximum element of a list could be written as:

    fn max_item[T: Comparable](list: &List[T]) -> Optional[T]:
        if list.is_empty():
            return None
        var max = list[0]
        for item in list[1..]:  # iterate from second element onward
            if item.cmp(&max) > 0:
                max = item
        return Some(max)

Here fn max_item[T: Comparable] means this function is generic over type T, but 
with a constraint: T must implement the Comparable trait. Inside the function, we 
can then call item.cmp(&max) because we know T has a cmp method from that trait. 

The compiler will monomorphize this function for each specific T that is used - 
in other words, if you call max_item on a List[int], it will generate an 
max_item_int version (using the int implementation of Comparable), and if you 
call it on a List[Person], it will generate a max_item_person, etc. This static 
dispatch means there's no vtable or runtime overhead for calling cmp - the call 
is resolved at compile time to the appropriate function. Thus, generics + traits 
provide flexibility (write one max_item that works for any comparable type) 
without cost relative to writing a specialized function manually.

If one truly needs runtime polymorphism (deciding at runtime which implementation 
to call, perhaps holding different types in the same collection), Pyrite can 
support trait objects or virtual dispatch as an opt-in. For example, a dyn 
Comparable type could be a trait object carrying a pointer to some object and a 
pointer to a vtable of its implementation. This incurs a runtime cost (an 
indirect function call and some extra pointer data) and would only be used if you 
explicitly use dyn. Most beginners won't need this; it's mainly for cases like 
heterogeneous collections or interfacing with OOP frameworks. By default, 
everything is static (zero-cost abstraction). This design (very much like Rust's) 
ensures you only pay for dynamic dispatch if you want it.

Traits can also be used to overload certain operators or behaviors in a 
controlled way. For example, the language might provide a standard trait like Add 
with a method fn add(&self, rhs: &Self) -> Self. If a type implements Add, the 
compiler could allow using + on that type, desugaring it to the add call. This is 
how Rust does operator overloading - it's explicit via traits, not built-in 
magic. Pyrite may or may not include a lot of these in the initial version to 
keep things simple, but it's a possible extension (e.g. an Add trait for numeric 
types, a Display trait for printing, etc.).

Duck Typing vs Traits
~~~~~~~~~~~~~~~~~~~~~~

Python uses duck-typing (if an object has the right methods, you just call them 
and it works). Pyrite's traits are like a compile-time checked version of that. 
They require you to declare the conformance, but then give you static safety and 
efficiency. For a beginner, traits might feel like an advanced topic, but they 
mirror patterns they might know in dynamic languages (like "if it quacks like a 
duck, treat it as a duck"). Pyrite just formalizes the "quack" into a trait.

7.2 Methods and Associated Functions
--------------------------------------------------------------------------------

Pyrite allows you to associate functions with types, which provides a sort of 
lightweight object-oriented feel without actual classes. These are methods 
(instance functions) and associated functions (like static functions) defined in 
impl blocks.

Methods
~~~~~~~

A method is essentially a function that has an implicit parameter for the 
instance (self). For example:

    impl Person:
        fn birthday(&mut self):
            self.age += 1

This defines a method birthday on Person that increments the person's age. The 
&mut self syntax indicates it takes a mutable reference to the instance (so it 
can modify it). We could call this method as person.birthday() on a var person = 
Person { name: "Alice", age: 29 }. After calling person.birthday(), her age 
becomes 30. Under the hood, the compiler translates that call into something like 
birthday(&mut person) - it's not doing anything magic beyond passing the instance 
as an argument - but for the user, it's convenient and clear ("do this action on 
this object").

Methods can be defined either: 
  - In a trait implementation (as seen, where we implement trait methods for a 
    type). 
  - As inherent methods on the type itself (like the birthday example, which is 
    not part of a trait, just a method for Person).

Pyrite does not support subclassing or inheritance between types (just like Rust 
and Go). You cannot say struct Employee: Person or something - instead, you'd 
compose types (an Employee could have a Person field) or use traits to achieve 
polymorphism. This avoids complexities like the diamond problem, slicing issues, 
etc., that come with class inheritance. It keeps the type system simpler and 
encourages composition (favoring "has-a" over "is-a" relationships for code 
reuse).

Associated Functions
~~~~~~~~~~~~~~~~~~~~

These are functions tied to a type but not to an instance. In many languages, 
these are called static methods or constructors. For example, we can add a 
constructor as an associated function:

    impl Person:
        fn new(name: String, age: int) -> Person:
            return Person{name: name, age: age}

Now we can create a Person by calling Person.new("Bob", 25). This is easier to 
read than having a free function. It's essentially namespacing the function under 
the type. Associated functions do not get a self parameter (since they aren't on 
an instance). They can be used for anything from constructors to utility 
functions related to that type.

The benefit of methods and associated functions is largely organizational and 
ergonomic: they let you group functionality with the data it operates on. 
Beginners might find it natural to think "an action that a Person can do, like 
have a birthday, should be part of Person's definition" - and indeed, Pyrite 
allows that, making code more intuitive. However, under the hood, they're just 
functions (no special access privileges beyond what the module's rules allow; 
Pyrite modules control visibility rather than types).

Encapsulation and Design
~~~~~~~~~~~~~~~~~~~~~~~~~

Methods allow a form of encapsulation. If you mark fields of a struct as private 
(to the module) and only expose methods, you can control how the type's state is 
manipulated. Pyrite likely will have a module-level privacy (things are either 
public or private to the module by default). Using methods, you can enforce 
invariants on the data (like never letting age be negative, etc.) by not exposing 
direct field mutation. This is similar to Python, except Python relies on 
convention (like prefix _ for private) whereas Pyrite as a compiled language will 
enforce it if specified.

Operator Methods
~~~~~~~~~~~~~~~~

As mentioned, direct operator overloading by users isn't part of the base 
language, but if included via traits, the implementations of those traits for a 
type would likely be done in an impl as well. For example, if Pyrite had an Add 
trait with method add, implementing that trait for Person could allow using + if 
semantically defined (though adding persons doesn't make sense; a better example 
is maybe a Vector3 type implementing Add to add vectors). This would all be done 
at compile time.

In summary, methods and associated functions give Pyrite a touch of OO flavor 
(methods with dot-call syntax) without the full complexity of class-based 
inheritance. It's a pragmatic middle ground: you can encapsulate behavior with 
data and invoke it in a nice way, and the language remains oriented around 
structs and functions under the hood.

7.3 Contract Programming: Design by Contract (Stable Release)
--------------------------------------------------------------------------------

Beyond memory safety (ownership) and performance guarantees (@cost_budget), 
Pyrite provides Design by Contract for logical correctness verification. 
Contracts are executable specifications that express preconditions, 
postconditions, and invariants, bridging the gap between "memory safe" and 
"logically correct."

Design Philosophy
~~~~~~~~~~~~~~~~~

Contracts in Pyrite are inspired by Eiffel and Ada/SPARK but designed for 
systems programming:
  â€¢ Compile-time verification when provable
  â€¢ Runtime checks in debug/test builds
  â€¢ Zero cost in release builds (optimized out)
  â€¢ Composable with performance contracts (@cost_budget)
  â€¢ First-class error messages with blame tracking

Basic Contract Syntax
~~~~~~~~~~~~~~~~~~~~~~

Preconditions (@requires) and postconditions (@ensures):

    @requires(n >= 0, "n must be non-negative")
    @requires(n <= 20, "factorial only defined up to 20")
    @ensures(result > 0, "factorial is always positive")
    @ensures(n <= 1 or result > n, "factorial grows")
    fn factorial(n: int) -> int:
        if n <= 1:
            return 1
        return n * factorial(n - 1)

When contracts are violated:

    error[P0901]: precondition violated
      ----> main.py:45:15
       |
    45 |     let result = factorial(-5)
       |                  ^^^^^^^^^^^^^ precondition 'n >= 0' violated
       |
       = contract: n must be non-negative
       = actual value: n = -5
       = help: Ensure input is non-negative before calling

Invariants for Types
~~~~~~~~~~~~~~~~~~~~

Struct and enum invariants ensure type correctness:

    struct BoundedBuffer:
        data: [u8; 1024]
        length: usize
        
        @invariant(self.length <= 1024, "length within bounds")
        @invariant(self.length >= 0, "length non-negative")
    
    impl BoundedBuffer:
        fn push(&mut self, byte: u8) -> Result[(), Error]:
            @requires(self.length < 1024, "buffer not full")
            @ensures(self.length == old(self.length) + 1, "length increased by 1")
            
            self.data[self.length] = byte
            self.length += 1
            Ok(())

The @invariant is checked:
  â€¢ After construction
  â€¢ After every method call (in debug builds)
  â€¢ Before destruction

Loop Invariants
~~~~~~~~~~~~~~~

For complex algorithms, express loop invariants:

    fn binary_search[T: Ord](arr: &[T], target: &T) -> Optional[usize]:
        @requires(is_sorted(arr), "array must be sorted")
        
        var low = 0
        var high = arr.len()
        
        while low < high:
            @invariant(low <= high, "search bounds valid")
            @invariant(high <= arr.len(), "high within array")
            @invariant(
                all(arr[0..low], fn(x): x < target),
                "all elements before low are less than target"
            )
            @invariant(
                all(arr[high..], fn(x): x >= target),
                "all elements after high are >= target"
            )
            
            let mid = low + (high - low) / 2
            match arr[mid].cmp(target):
                Less:
                    low = mid + 1
                Greater:
                    high = mid
                Equal:
                    return Some(mid)
        
        return None

Old-Value Syntax
~~~~~~~~~~~~~~~~

Reference previous state in postconditions:

    fn increment_counter(&mut self):
        @ensures(self.counter == old(self.counter) + 1, "counter incremented")
        
        self.counter += 1

The old() function captures value at function entry for comparison in 
postcondition.

Quantified Conditions
~~~~~~~~~~~~~~~~~~~~~

Express properties over collections:

    @ensures(forall(result, fn(x): x % 2 == 0), "all even")
    fn filter_even(input: &[int]) -> Vec[int]:
        return input.iter().filter(fn(x): x % 2 == 0).collect()
    
    @requires(exists(arr, fn(x): x > 0), "at least one positive")
    fn find_first_positive(arr: &[int]) -> Optional[int]:
        for x in arr:
            if x > 0:
                return Some(x)
        return None

Compile-Time Verification
~~~~~~~~~~~~~~~~~~~~~~~~~~

When contracts can be proven at compile time, no runtime cost:

    fn process_small_array(arr: [int; 5]) -> int:
        @requires(arr.len() == 5, "array has 5 elements")
        # Compiler proves: arr.len() is 5 (compile-time constant)
        # No runtime check generated
        ...

Symbolic execution for simple contracts:

    fn divide(a: int, b: int) -> int:
        @requires(b != 0, "divisor must be non-zero")
        return a / b
    
    let x = divide(10, 5)  # OK: b is 5, provably != 0
    let y = divide(10, 0)  # ERROR: Compile-time contract violation

Call-Graph Contract Propagation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Contracts compose through function boundaries:

    @requires(index < arr.len(), "index in bounds")
    fn get_element(arr: &[int], index: usize) -> int:
        return arr[index]
    
    fn caller():
        let arr = [1, 2, 3, 4, 5]
        let val = get_element(&arr, 10)  # ERROR: Contract violated
    
    error[P0902]: precondition violated in call
      ----> main.py:67:15
       |
    67 |     let val = get_element(&arr, 10)
       |               ^^^^^^^^^^^^^^^^^^^^^ precondition 'index < arr.len()' not satisfied
       |
       = contract: index < arr.len()
       = actual: index = 10, arr.len() = 5
       = note: Call chain:
         1. caller() at main.py:67
            â†’ calls get_element(&arr, 10)
         2. get_element() requires index < arr.len()
            â†’ 10 < 5 is false [VIOLATION]
       |
       = help: Validate index before calling:
               if index < arr.len():
                   let val = get_element(&arr, index)

Integration with Cost Contracts
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Combine logical and performance contracts:

    @requires(data.len() <= 1024, "input bounded")
    @ensures(result.len() == data.len(), "output same length")
    @cost_budget(allocs=0, cycles=10000)
    fn process_packet(data: &[u8]) -> Result[Vec[u8], Error]:
        # Both logical correctness AND performance enforced
        ...

Runtime Contract Checking
~~~~~~~~~~~~~~~~~~~~~~~~~~

In debug and test builds, contracts are checked:

    $ quarry test
    
    Running tests with contract checking enabled...
    
    test test_binary_search ... FAILED
    
    Contract violation: postcondition
      ----> src/search.py:89:5
       |
    89 | @ensures(result.is_some() implies arr[result.unwrap()] == target)
       |
       = note: Postcondition violated
       = result: Some(3)
       = arr[3]: 42
       = target: 41
       = note: Function returned index 3, but arr[3] != target
    
    This indicates a BUG in binary_search implementation.

Configuration
~~~~~~~~~~~~~

Control contract checking levels:

    # Quarry.toml
    [profile.debug]
    check-contracts = "all"          # Check all contracts
    
    [profile.test]
    check-contracts = "all"          # Check in tests
    
    [profile.release]
    check-contracts = "none"         # Optimize out (zero cost)
    
    [profile.certified]
    check-contracts = "critical"     # Check @safety-critical contracts only

Safety-Critical Contracts
~~~~~~~~~~~~~~~~~~~~~~~~~

Mark critical contracts that should be checked even in release:

    @safety_critical
    @requires(temperature >= -40 and temperature <= 85, "sensor within operating range")
    fn read_temperature_sensor() -> f32:
        # This contract checked in ALL builds (safety-critical)
        ...

Why Contracts Matter
~~~~~~~~~~~~~~~~~~~~

**Logical correctness:**
  â€¢ Ownership prevents memory bugs
  â€¢ Contracts prevent logic bugs
  â€¢ "Memory-safe but logically incorrect" is still wrong
  â€¢ Contracts close the correctness gap

**Safety certification:**
  â€¢ DO-178C: Formal methods improve certification level
  â€¢ IEC 62304: Design by Contract accepted for medical devices
  â€¢ Ada/SPARK: Contracts enable formal verification
  â€¢ Pyrite: Same benefits, more accessible syntax

**Documentation as code:**
  â€¢ Contracts are executable specifications
  â€¢ Clearer than comments (enforced, not ignored)
  â€¢ Self-documenting APIs ("requires X, ensures Y")
  â€¢ Impossible to drift from implementation (checked)

**Testing amplification:**
  â€¢ Contracts checked on every test run
  â€¢ Catch violations beyond explicit test cases
  â€¢ Fuzz testing with contracts = systematic bug finding
  â€¢ quarry fuzz + contracts = exhaustive edge case coverage

**Verification path:**
  â€¢ Beta Release: Runtime checking in debug/test
  â€¢ Stable Release: Compile-time verification for provable contracts
  â€¢ Future: Integration with formal verification tools (Z3, SMT solvers)

Use Cases
~~~~~~~~~

  â€¢ Safety-critical firmware (aerospace, medical devices)
  â€¢ Cryptographic implementations (constant-time contracts)
  â€¢ Protocol implementations (state machine invariants)
  â€¢ Parsers (input validation contracts)
  â€¢ Concurrent code (lock ordering invariants)

Teaching Path
~~~~~~~~~~~~~

  1. **Beginner:** See contracts in stdlib ("why APIs are safe")
  2. **Intermediate:** Add simple contracts (@requires non-negative)
  3. **Advanced:** Complex invariants, loop invariants
  4. **Expert:** Formal verification integration

Comparison to Other Languages
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  â€¢ **Eiffel:** Flagship contracts, but niche language
  â€¢ **Ada/SPARK:** Formal verification, complex tooling
  â€¢ **Rust:** No built-in contracts (external crates only)
  â€¢ **D:** Has contracts, but not widely used
  â€¢ **Pyrite:** Contracts integrated with ownership, cost tracking, blame chains

Pyrite is the first **mainstream systems language** with first-class contract 
support that composes with ownership and performance contracts.

Implementation: Stable Release (after core compiler is stable)
Priority: High (correctness + certification + differentiation)
Complexity: High (SMT integration for verification, runtime checking)
Impact: High (enables safety certification, prevents logic bugs)

7.4 Noalias and Restrict Semantics (Stable Release, Expert Feature)
--------------------------------------------------------------------------------

For advanced performance optimization, Pyrite provides explicit non-aliasing 
assertions that enable aggressive compiler optimizations. This is an opt-in, 
expert-level feature for cases where exclusive &mut access isn't sufficient.

Design Philosophy
~~~~~~~~~~~~~~~~~

Pyrite's ownership system already handles most aliasing through exclusive &mut 
borrows. The @noalias attribute is for the remaining cases: asserting that 
multiple immutable references don't overlap, or that pointers from FFI don't 
alias.

The @noalias Attribute
~~~~~~~~~~~~~~~~~~~~~~

Marks parameters as non-aliasing, enabling optimizations:

    @noalias
    fn process(a: &mut [f32], b: &mut [f32]):
        # Compiler assumes a and b don't overlap
        # Enables vectorization and aggressive loop optimizations
        for i in 0..a.len():
            a[i] += b[i]

Without @noalias, the compiler must assume potential aliasing and generates 
conservative code. With @noalias, the compiler can:
  â€¢ Reorder memory accesses
  â€¢ Vectorize loops more aggressively
  â€¢ Eliminate redundant loads
  â€¢ Perform more loop transformations

When to Use
~~~~~~~~~~~

@noalias is useful for:

1. **Multiple immutable references where compiler can't prove disjointness:**

   @noalias
   fn sum_two_views(a: &[f32], b: &[f32]) -> f32:
       # Assert a and b don't overlap
       var sum = 0.0
       for i in 0..a.len():
           sum += a[i] + b[i]
       return sum

2. **FFI pointers from C (no ownership information):**

   @noalias
   extern fn memcpy(dest: *mut u8, src: *const u8, n: usize)

3. **Performance-critical kernels where exclusivity is verified manually:**

   @noalias
   fn kernel_compute(in1: &[f32], in2: &[f32], out: &mut [f32]):
       # Manual verification: in1, in2, out are disjoint
       # Parameter closure (fn[...]) is inlined, zero allocation
       algorithm.vectorize[width=8](out.len(), fn[i: int]:
           out[i] = in1[i] * in2[i]
       )

Safety and Verification
~~~~~~~~~~~~~~~~~~~~~~~

@noalias is checked at compile time when possible, runtime in debug builds:

    @noalias
    fn process(a: &mut [f32], b: &mut [f32]):
        # ...
    
    let mut data = vec![1.0; 100]
    process(&mut data[0..50], &mut data[50..100])  # OK: disjoint
    
    # Debug build:
    process(&mut data[0..50], &mut data[25..75])   # PANIC: overlap detected

Release builds trust the programmer (like unsafe). The @noalias contract is 
part of the function's safety requirements.

Compiler Diagnostics
~~~~~~~~~~~~~~~~~~~~

When aliasing occurs despite @noalias:

    warning[P1300]: potential aliasing in @noalias function
      ----> src/compute.py:45:5
       |
    43 | @noalias
    44 | fn process(a: &mut [f32], b: &mut [f32]):
    45 |     process(&mut data[0..50], &mut data[25..75])
       |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ potential overlap
       |
       = note: @noalias asserts that 'a' and 'b' are disjoint
       = help: Verify manually that slices don't overlap
       = suggestion: Run with --debug to detect aliasing at runtime

Integration with Ownership
~~~~~~~~~~~~~~~~~~~~~~~~~~~

@noalias complements, doesn't replace, ownership:

  â€¢ &mut T already provides exclusive access (single mutable reference)
  â€¢ @noalias is for multiple references where compiler can't prove disjointness
  â€¢ Most code doesn't need @noalias (ownership handles common cases)

Performance Impact
~~~~~~~~~~~~~~~~~~

Typical speedups from @noalias:
  â€¢ 5-15% for compute-heavy loops (vectorization)
  â€¢ 10-25% for memory-bound operations (eliminates redundant loads)
  â€¢ Depends heavily on specific code and compiler optimizations

Use quarry cost to see when @noalias helps:

    $ quarry cost --noalias-analysis
    
    Function: process(a: &mut [f32], b: &mut [f32])
      â†’ Aliasing assumption prevents vectorization
      â†’ Suggestion: Add @noalias if inputs are guaranteed disjoint
      â†’ Estimated improvement: 12-18% (enables SIMD)

When NOT to Use
~~~~~~~~~~~~~~~

Don't use @noalias if:
  â€¢ Ownership rules already prove exclusivity (&mut handles this)
  â€¢ You're not certain inputs are disjoint (undefined behavior otherwise)
  â€¢ Performance profiling doesn't show a bottleneck
  â€¢ Code is not performance-critical

@noalias is an expert optimization, not a default tool.

Teaching Path
~~~~~~~~~~~~~

1. **Beginner-Intermediate:** Use ownership system (&, &mut)
2. **Advanced:** Understand when ownership proves disjointness
3. **Expert:** Profile, identify aliasing-limited optimizations
4. **Expert:** Apply @noalias with manual verification

Summary
~~~~~~~

@noalias fills a small but important gap: asserting non-aliasing when ownership 
can't prove it. It's explicit, checked when possible, and provides measurable 
performance gains for the right use cases.

Implementation: Stable Release (after core ownership and performance tooling are stable)

7.5 Two-Tier Closure Model: Parameter vs Runtime (Beta Release)
--------------------------------------------------------------------------------

Pyrite distinguishes between two fundamentally different kinds of closures, making 
the performance characteristics explicit and enabling zero-cost abstractions for 
algorithmic helpers. This distinction is inspired by Mojo's parameter closures 
and addresses the critical gap between "callable code" and "zero-overhead callable 
code."

Design Philosophy
~~~~~~~~~~~~~~~~~

The two-tier model makes cost explicit:
  â€¢ **Parameter closures** - Compile-time, always-inline, zero-allocation, used 
    for performance primitives (vectorize, parallelize, tile)
  â€¢ **Runtime closures** - Real values, can escape, can allocate/copy capture 
    state, used for callbacks and thread spawning

This distinction is the foundation for verifiable `--no-alloc` mode and makes 
`quarry cost` analysis precise.

Parameter Closures (Compile-Time)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Parameter closures use square bracket syntax `fn[params]` and are evaluated at 
compile time:

    import std::algorithm
    
    fn scale_array(data: &mut [f32], factor: f32):
        # Parameter closure: fn[i: int] with square brackets
        algorithm.vectorize[width=auto](data.len(), fn[i: int]:
            data[i] = data[i] * factor
        )

Key properties:
  â€¢ **Zero allocation:** Never captured on heap, inlined directly
  â€¢ **Compile-time only:** Cannot escape function scope or be stored
  â€¢ **Always inlined:** Compiler guarantees inline expansion
  â€¢ **No vtable:** Static dispatch only
  â€¢ **Captures by reference:** Environment captured at compile time
  â€¢ **Verified by --no-alloc:** Safe to use in no-allocation contexts

Parameter closures are the backbone of algorithmic helpers:

    # All use parameter closures (zero-cost)
    algorithm.vectorize[width=8](n, fn[i: int]: data[i] *= 2.0)
    algorithm.parallelize(n, fn[i: int]: process(&work[i]))
    algorithm.tile[64](rows, cols, fn[r: int, c: int]: compute(r, c))
    compile.unroll[N](fn[i: int]: result[i] = a[i] + b[i])

Cost verification:

    @noalloc
    fn process_buffer(data: &mut [f32]):
        # OK: parameter closures don't allocate
        algorithm.vectorize[width=auto](data.len(), fn[i: int]:
            data[i] = data[i].sqrt()
        )
    
    # Compiles successfully with --no-alloc

Capture semantics:

    fn scale_all(arrays: &[&mut [f32]], factor: f32):
        for arr in arrays:
            # Parameter closure captures 'factor' by reference (compile-time)
            algorithm.vectorize[width=8](arr.len(), fn[i: int]:
                arr[i] *= factor  # OK: 'factor' captured, zero cost
            )

What you CANNOT do with parameter closures:

    fn broken_example():
        let closure = fn[i: int]: i * 2  # ERROR: cannot store parameter closure
        
        let list = List[fn[int] -> int].new()
        list.push(fn[i: int]: i * 2)  # ERROR: cannot store in collection
        
        return fn[i: int]: i * 2  # ERROR: cannot return parameter closure

Parameter closures exist only at compile time for inline expansion.

Runtime Closures (First-Class Values)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Runtime closures use parenthesis syntax `fn(params)` and are first-class values:

    # Runtime closure: fn(x: int) with parentheses
    let filter_fn = fn(x: int) -> bool:
        return x > threshold
    
    # Can be stored
    let callbacks = List[fn(int) -> bool].new()
    callbacks.push(filter_fn)
    
    # Can be returned
    fn make_filter(threshold: int) -> fn(int) -> bool:
        return fn(x: int) -> bool:
            return x > threshold
    
    # Can escape scope
    Thread.spawn(fn():
        process_background()
    )

Key properties:
  â€¢ **Can allocate:** Capture environment may require heap allocation
  â€¢ **Can escape:** Store in variables, return from functions, pass to threads
  â€¢ **Dynamic dispatch possible:** Can be stored in trait objects
  â€¢ **Captured by value/move:** Ownership rules apply to captures
  â€¢ **Shows up in quarry cost:** Allocations visible and tracked
  â€¢ **Requires runtime representation:** Function pointer + environment data

Cost characteristics:

    fn create_handlers(config: &Config) -> List[fn() -> void]:
        var handlers = List[fn() -> void].new()
        
        # Runtime closure captures 'config' reference
        # Allocation: closure object on heap (environment data)
        handlers.push(fn():
            print(config.name)  # Captures 'config' by reference
        )
        
        return handlers
    
    # quarry cost shows:
    #   Allocation at line X: closure environment (24 bytes)

Capture modes:

    let x = 10
    let y = String.new("hello")
    
    # Capture by reference (no allocation if closure doesn't escape)
    let closure1 = fn() -> int:
        return x * 2  # Captures &x
    
    # Capture by move (transfers ownership)
    let closure2 = fn():
        print(y)  # Moves y into closure
    # y is now invalid in outer scope
    
    # Explicit move all
    let closure3 = move fn():
        print(x)  # Forces move even for Copy types

Thread spawning requires move semantics:

    let data = vec![1, 2, 3]
    
    # ERROR: data is borrowed, may not outlive thread
    Thread.spawn(fn():
        print(data)
    )
    
    # OK: explicit move into thread
    Thread.spawn(move fn():
        print(data)  # data moved into closure
    )
    # data invalid here

Syntax Summary
~~~~~~~~~~~~~~

Visual distinction makes cost explicit:

    # COMPILE-TIME (zero-cost, inline, no-alloc safe)
    algorithm.vectorize[width=8](n, fn[i: int]:
        data[i] *= 2.0
    )
    
    # RUNTIME (may allocate, can escape, shows in quarry cost)
    Thread.spawn(fn():
        process_background()
    )
    
    # Parameter closure for compile-time helpers
    compile.unroll[4](fn[i: int]:
        result[i] = input[i]
    )
    
    # Runtime closure for dynamic behavior
    let callbacks = vec![
        fn(): handler_one(),
        fn(): handler_two(),
    ]

Teaching Progression
~~~~~~~~~~~~~~~~~~~~

1. **Week 1-2 (Beginner):** Use algorithmic helpers without understanding closure 
   types
   
   algorithm.vectorize[width=auto](data.len(), fn[i: int]:
       data[i] = data[i] * 2.0
   )
   
   "Just write what to do per element, the compiler makes it fast"

2. **Week 3-4 (Intermediate):** Learn runtime closures for callbacks
   
   let filter = fn(x: int) -> bool:
       return x > 10
   numbers.iter().filter(filter)
   
   "Closures are values you can pass around"

3. **Week 5+ (Advanced):** Understand the distinction
   
   "fn[...] is compile-time (free), fn(...) is runtime (may allocate)"
   
   See difference in `quarry cost` output

4. **Expert:** Choose appropriate closure type for performance
   
   "Use parameter closures for hot paths, runtime closures for flexibility"

Integration with quarry cost
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The cost analyzer distinguishes closure types:

    $ quarry cost
    
    Closures: 3 parameter (zero-cost), 5 runtime (24 bytes each)
    
    Parameter closures (zero-cost, always inlined):
      âœ“ Line 234: vectorize closure - INLINED
      âœ“ Line 267: parallelize closure - INLINED  
      âœ“ Line 289: tile closure - INLINED
    
    Runtime closures (heap-allocated environments):
      Line 156: Thread::spawn closure
        â€¢ Environment: 24 bytes (captures 2 references)
        â€¢ Lifetime: Thread duration
      
      Line 178: Stored in callback list
        â€¢ Environment: 32 bytes (captures String by move)
        â€¢ Allocation: One per closure

Integration with --no-alloc Mode
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Parameter closures are safe in no-allocation contexts:

    quarry build --no-alloc
    
    @noalloc
    fn compute_kernel(data: &mut [f32]):
        # OK: parameter closure doesn't allocate
        algorithm.vectorize[width=8](data.len(), fn[i: int]:
            data[i] = data[i].sqrt()
        )
        
        # ERROR: runtime closure may allocate
        let callback = fn(): print("done")
        #              ^^^ heap allocation in no-alloc mode

This makes `--no-alloc` verification complete - no hidden allocation through 
closures.

Error Messages
~~~~~~~~~~~~~~

Clear diagnostics teach the distinction:

    error[P0801]: cannot store parameter closure as runtime value
      ----> src/compute.py:45:13
       |
    45 |     let f = fn[i: int]: i * 2
        |             ^^^^^^^^^^^^^^^^^ parameter closure (compile-time only)
       |
       = note: Parameter closures use square brackets fn[...] and exist only 
               at compile time for inline expansion
       = help: To store a closure, use runtime closure syntax:
               let f = fn(i: int) -> int: return i * 2
       = explain: Run 'pyritec --explain P0801' for closure types guide

    error[P0802]: runtime closure in no-allocation context
      ----> src/embedded.py:67:9
       |
    65 | @noalloc
    66 | fn process_data(input: &[u8]):
    67 |     let parser = fn(): parse(input)
        |                  ^^^^^^^^^^^^^^^^^^ runtime closure may allocate
       |
       = note: Runtime closures allocate environment for captured variables
       = help: Use parameter closure for no-alloc guarantee:
               algorithm.vectorize[width=auto](input.len(), fn[i: int]:
                   process_byte(input[i])
               )
       = explain: Run 'pyritec --explain P0802' for zero-cost closures

quarry explain-type Integration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Type introspection shows closure characteristics:

    $ quarry explain-type "fn[int]"
    
    Type: Parameter Closure (fn[int])
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Properties: [CompileTime] [ZeroCost] [AlwaysInline]
    
    Memory Layout:
      â€¢ Size: 0 bytes at runtime (compile-time only)
      â€¢ Captures: Compile-time environment
      â€¢ Storage: Cannot be stored (exists only during compilation)
    
    Behavior:
      â€¢ Inlined at every call site (no function pointer)
      â€¢ Zero allocation (never captured on heap)
      â€¢ Zero indirection (direct code expansion)
    
    Use Cases:
      â€¢ algorithm.vectorize - SIMD loop bodies
      â€¢ algorithm.parallelize - Work distribution
      â€¢ algorithm.tile - Cache-blocking
      â€¢ compile.unroll - Loop unrolling
    
    Restrictions:
      âœ— Cannot be stored in variables
      âœ— Cannot be returned from functions
      âœ— Cannot be passed to runtime functions
      âœ“ Can only be used with compile-time helpers

    $ quarry explain-type "fn(int) -> bool"
    
    Type: Runtime Closure (fn(int) -> bool)
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Properties: [Runtime] [MayAlloc] [FirstClass]
    
    Memory Layout:
      â€¢ Size: 16 bytes (function pointer + environment pointer)
      â€¢ Environment: Varies (depends on captured variables)
      â€¢ Total: 16 bytes + sizeof(captures)
    
    Behavior:
      â€¢ Can be stored in variables and collections
      â€¢ Can be returned from functions
      â€¢ Can escape scope and be passed to threads
      â€¢ Environment may allocate on heap (if captures non-Copy types)
    
    Cost:
      â€¢ Allocation: Varies (0 if no captures, >0 if captures heap data)
      â€¢ Call overhead: Indirect call (function pointer, ~2-3 cycles)
      â€¢ Environment access: One pointer dereference per captured variable
    
    Use Cases:
      â€¢ Callbacks and event handlers
      â€¢ Thread::spawn arguments
      â€¢ Filter/map in iterators
      â€¢ Plugin systems
      â€¢ Dynamic dispatch scenarios

Why This Distinction Matters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The two-tier model unlocks multiple benefits:

1. **Zero-cost algorithmic abstractions:**
   
   Parameter closures make helpers like vectorize truly zero-cost - no allocation, 
   no indirection, no runtime overhead. The closure body is inlined directly into 
   the generated SIMD loop.

2. **Verifiable --no-alloc mode:**
   
   The compiler can guarantee no heap allocation through algorithmic helpers 
   because parameter closures have no runtime representation.

3. **Teaching clarity:**
   
   Beginners learn "square brackets = free, parentheses = may cost" as a simple 
   visual rule. Advanced developers understand the compile-time vs runtime 
   distinction.

4. **Performance transparency:**
   
   `quarry cost` can analyze runtime closures but shows parameter closures as 
   "zero-cost inline." No ambiguity about what's expensive.

5. **Composability:**
   
   Algorithmic helpers compose without allocation:
   
       algorithm.parallelize(chunks, fn[chunk: int]:
           algorithm.vectorize[width=8](chunk_size, fn[i: int]:
               # Both closures are parameter closures (zero-cost)
               data[chunk * chunk_size + i] = compute(i)
           )
       )

Comparison to Other Languages
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  â€¢ **Mojo:** Direct inspiration - Mojo's parameter closures power `vectorize` and 
    `parallelize` with guaranteed zero cost
  â€¢ **C++ lambdas:** Can be inline or allocate, but distinction is implicit and 
    compiler-dependent (not explicit syntax)
  â€¢ **Rust closures:** All runtime closures, even when inlined (no compile-time 
    closure type)
  â€¢ **Zig comptime:** Related concept but different mechanism (comptime blocks vs 
    closure syntax)

Pyrite makes the distinction **explicit in syntax** rather than compiler-dependent, 
ensuring predictable cost regardless of optimization level.

API Design Guidelines
~~~~~~~~~~~~~~~~~~~~~

Standard library follows these patterns:

**Use parameter closures for:**
  â€¢ algorithm.vectorize - SIMD loop bodies
  â€¢ algorithm.parallelize - Work distribution (body runs per item)
  â€¢ algorithm.tile - Cache-blocking iteration
  â€¢ compile.unroll - Loop unrolling
  â€¢ compile.if_constexpr - Compile-time conditionals
  â€¢ Any compile-time code generation helper

**Use runtime closures for:**
  â€¢ Thread::spawn - Thread entry points (can capture and move data)
  â€¢ Iterator::filter, Iterator::map - Lazy iterator chains
  â€¢ Event handlers and callbacks
  â€¢ Sorted collections (comparison functions)
  â€¢ Defer blocks (scope-exit code)
  â€¢ Any closure that escapes or is stored dynamically

Example: Iterator Methods
~~~~~~~~~~~~~~~~~~~~~~~~~~

Different iterator methods use appropriate closure types:

    let numbers = vec![1, 2, 3, 4, 5]
    
    # Runtime closure (filter may need to store for lazy evaluation)
    let filtered = numbers.iter()
        .filter(fn(x: &int) -> bool: *x > 2)
    
    # But forEach uses parameter closure (always eager)
    numbers.iter().for_each(fn[x: &int]:
        print(*x)  # Inlined into loop, zero allocation
    )

This flexibility allows both lazy (runtime closures) and eager (parameter closures) 
patterns with clear performance implications.

Type System Integration
~~~~~~~~~~~~~~~~~~~~~~~

Parameter and runtime closure types are distinct:

    # Parameter closure type (only appears in signatures)
    fn vectorize[T, Body: fn[int]](
        count: int,
        body: Body  # Body is a compile-time parameter
    ):
        # Body is inlined at compile time
    
    # Runtime closure type (regular type)
    type Predicate[T] = fn(T) -> bool
    
    fn filter[T](items: &[T], pred: Predicate[T]) -> Vec[T]:
        var result = Vec[T].new()
        for item in items:
            if pred(item):  # Indirect call through function pointer
                result.push(item)
        return result

Compiler Guarantees
~~~~~~~~~~~~~~~~~~~

The compiler enforces distinctions:

  â€¢ Parameter closures **must** be inlined (compilation fails if not possible)
  â€¢ Parameter closures **cannot** allocate (compilation fails if captures would 
    require heap)
  â€¢ Runtime closures **can** allocate (tracked by quarry cost)
  â€¢ Runtime closures **can** escape (ownership rules apply to captures)

This makes the cost model explicit and verifiable - no "depends on optimization 
level" ambiguity.

Migration from Existing Spec
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Current spec shows:

    algorithm.vectorize[width=auto](data.len(), fn(i: int):
        data[i] = data[i] * factor
    )

This is ambiguous - is `fn(i: int)` zero-cost or not?

Updated syntax makes it explicit:

    algorithm.vectorize[width=auto](data.len(), fn[i: int]:
        data[i] = data[i] * factor
    )

The square brackets `fn[i: int]` signal "compile-time, zero-cost, inline."

All algorithmic helpers (vectorize, parallelize, tile, unroll) updated to use 
parameter closures consistently.

Why This Is the Highest-Value Addition
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The two-tier closure model:

1. Fills the biggest gap in current spec (closure cost ambiguity)
2. Enables `--no-alloc` verification for algorithmic helpers
3. Makes Pyrite's performance story bulletproof (zero-cost abstractions are 
   provably zero-cost)
4. Provides clear teaching model ("square brackets = free")
5. Aligns perfectly with existing compile-time parameterization (`[N: int]` 
   syntax)
6. Makes `quarry cost` analysis precise and complete

This is the foundation for Pyrite's "accessible high-performance" positioning -
ergonomic helpers (vectorize, parallelize) that are provably zero-cost, not 
"usually optimized away."

Implementation: Beta Release (after core ownership and algorithmic helpers exist)
Complexity: Moderate (new closure type, parameter tracking, inline verification)
Impact: High (unlocks verifiable zero-cost abstractions)

7.6 Compile-Time Code Execution and Metaprogramming
--------------------------------------------------------------------------------

Pyrite incorporates capabilities for executing code at compile time, inspired by 
Zig's comptime functions, Rust's const fn, Mojo's parameterization, and C++'s 
constexpr and template metaprogramming. The goal is to enable powerful 
optimizations and code generation without requiring an external macro language or 
heavy runtime reflection.

Const Functions
~~~~~~~~~~~~~~~

You can mark certain functions as const (meaning they can be evaluated at compile 
time if called with constant arguments). For example:

    const fn fib(n: int) -> int:
        if n < 2:
            return n
        else:
            return fib(n-1) + fib(n-2)
    
    const FIB_10 = fib(10)

In this code, fib is a function that calculates Fibonacci numbers, marked const. 
FIB_10 will be computed at compile time (the compiler will recursively evaluate 
fib(10) during compilation) and treated as a constant with that value. This is 
useful for precomputing lookup tables, math constants, etc., at compile time so 
that they incur no cost at runtime.

Compile-Time Parameterization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pyrite supports compile-time parameterization inspired by Mojo's parameter system, 
enabling the compiler to generate specialized versions of functions and types for 
specific constant values. This provides performance benefits similar to C++ 
templates but with clearer semantics and better error messages.

Syntax and Semantics
~~~~~~~~~~~~~~~~~~~~~

Compile-time parameters are distinguished from runtime parameters using square 
brackets:

    fn process[N: int](data: [u8; N]) -> int:
        # N is known at compile time
        # Compiler generates specialized version for each unique N
        var sum = 0
        for i in 0..N:  # Loop unrolling possible
            sum += data[i]
        return sum
    
    # Usage:
    let buffer1: [u8; 16] = ...
    let result1 = process[16](buffer1)  # Specialized for N=16
    
    let buffer2: [u8; 256] = ...
    let result2 = process[256](buffer2)  # Different specialization for N=256

The key properties of compile-time parameters:
  â€¢ Known at compile time (constant values only)
  â€¢ Used for type-level computations and optimizations
  â€¢ Each unique parameter value generates a separate function/type instance
  â€¢ Zero runtime cost (constants can be inlined, loops unrolled)

Parameter Constraints
~~~~~~~~~~~~~~~~~~~~~

Compile-time parameters can be integers, booleans, or types:

    # Integer parameter (array size, loop bounds, capacity)
    fn create_buffer[Size: int]() -> [u8; Size]:
        return [0; Size]
    
    # Boolean parameter (feature flags, optimization modes)
    fn process[DebugMode: bool](data: &[u8]):
        if DebugMode:  # Evaluated at compile time
            # Debug code completely eliminated when DebugMode=false
            print("Processing {} bytes", data.len())
        # ... actual processing ...
    
    # Type parameter with compile-time size constraint
    fn optimize[T, Size: int](data: [T; Size]) -> int:
        # Both T and Size available at compile time
        const BYTES = Size * sizeof(T)  # Computed at compile time
        if BYTES < 64:  # Compile-time decision
            # Small buffer: inline processing
            return process_inline(data)
        else:
            # Large buffer: different strategy
            return process_chunked(data)

Standard Library Integration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The standard library provides compile-time helpers for common optimizations using 
parameter closures:

    import std.compile

    # Unroll loops at compile time
    fn vector_add[N: int](a: [f32; N], b: [f32; N]) -> [f32; N]:
        var result: [f32; N]
        # Parameter closure (fn[...]) inlined and unrolled at compile time
        compile.unroll[N](fn[i: int]:
            result[i] = a[i] + b[i]
        )
        return result

@unroll - Explicit Loop Unrolling
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For performance-critical code where loop unrolling provides measurable benefit, 
Pyrite provides the @unroll attribute with explicit control and safety limits:

    @unroll(factor=4)
    fn process_array(data: &mut [f32]):
        for i in 0..data.len():
            data[i] = data[i] * 2.0 + 1.0
    
    # Compiler unrolls to process 4 elements per iteration:
    # for i in 0..data.len() step 4:
    #     data[i+0] = data[i+0] * 2.0 + 1.0
    #     data[i+1] = data[i+1] * 2.0 + 1.0
    #     data[i+2] = data[i+2] * 2.0 + 1.0
    #     data[i+3] = data[i+3] * 2.0 + 1.0
    #     # + remainder loop for data.len() % 4

Unroll factor options:
  â€¢ @unroll(factor=N) - Unroll by specific factor (2, 4, 8, 16, etc.)
  â€¢ @unroll(full) - Fully unroll (only if iteration count known at compile time)
  â€¢ @unroll(auto) - Compiler chooses optimal factor based on loop body size

Safety limits and warnings:

    @unroll(factor=128)
    fn process_small(data: &[f32; 4]):
        for i in 0..4:
            data[i] = compute(data[i])
    
    warning[P1200]: excessive unroll factor
      ----> src/compute.py:45:5
       |
    45 | @unroll(factor=128)
       | ^^^^^^^^^^^^^^^^^^^ unroll factor 128 too large for loop
       |
       = note: Loop has only 4 iterations
       = note: Unroll factor 128 would generate 512 instructions
       = help: Reduce factor or use @unroll(full) for complete unrolling
       = suggestion: @unroll(factor=4) or @unroll(full)

Compiler limits:
  â€¢ Maximum unroll factor: 64 (prevents code size explosion)
  â€¢ Maximum unrolled body size: 256 instructions per iteration
  â€¢ Full unroll: Only for loops with â‰¤ 128 iterations (compile-time known)

Integration with compile-time parameters:

    fn matrix_multiply[N: int](a: &Matrix[N], b: &Matrix[N]) -> Matrix[N]:
        var result = Matrix[N]::zero()
        
        @unroll(full)  # N known at compile time
        for i in 0..N:
            @unroll(full)
            for j in 0..N:
                for k in 0..N:
                    result[i][j] += a[i][k] * b[k][j]
        
        return result
    
    # For Matrix[4], compiler fully unrolls i and j loops
    # For Matrix[100], compiler warns and uses auto factor

Interaction with SIMD:

    @unroll(factor=4)
    @simd(width=8)
    fn process_large(data: &mut [f32]):
        # Combines loop unrolling with SIMD
        # Each unrolled iteration processes 8 elements (SIMD width)
        # Total: 32 elements per outer loop iteration

When to use @unroll:
  â€¢ Small, performance-critical loops (< 16 iterations)
  â€¢ Loops with compile-time known bounds
  â€¢ Inner loops in hot paths (profiling shows benefit)
  â€¢ Kernels that benefit from instruction-level parallelism

When NOT to use @unroll:
  â€¢ Large loops (code size explosion)
  â€¢ Loops with complex bodies (diminishing returns)
  â€¢ Loops with unpredictable branches (unrolling adds little value)
  â€¢ Without profiling data (premature optimization)

Cost transparency integration:

    quarry cost shows:
      @unroll(factor=8) at line 45:
        â€¢ Loop body: 12 instructions
        â€¢ Unrolled size: 96 instructions + 12 (remainder)
        â€¢ Code size increase: +84 bytes
        â€¢ Expected speedup: 1.5-2x (reduced branch overhead)

Teaching path:
  1. **Beginner:** Don't use @unroll, let compiler optimize
  2. **Intermediate:** Profile shows hot loop â†’ try @unroll(factor=4)
  3. **Advanced:** Tune factor based on profiling results
  4. **Expert:** Combine @unroll with @simd and compile-time params

Beta Release feature that fills the gap between "compiler auto-optimization" and 
"manual assembly." Provides explicit control while maintaining safety through 
compiler warnings and hard limits.
    
    # Choose optimal SIMD width for current platform
    fn process_parallel(data: &[f32]):
        const WIDTH = compile.simd_width[f32]()  # 4, 8, or 16 depending on CPU
        # ... use WIDTH to process in optimal chunks ...
    
    # Compile-time assertions
    fn requires_power_of_two[N: int]():
        compile.assert(is_power_of_two(N), "N must be power of 2")
    
    const fn is_power_of_two(n: int) -> bool:
        return n > 0 and (n & (n - 1)) == 0

Practical Examples
~~~~~~~~~~~~~~~~~~

Example 1: Fixed-size matrix optimizations

    struct Matrix[Rows: int, Cols: int]:
        data: [f32; Rows * Cols]
    
    impl[R: int, C: int] Matrix[R, C]:
        fn multiply[K: int](&self, other: &Matrix[C, K]) -> Matrix[R, K]:
            var result = Matrix[R, K].zero()
            
            # Compiler knows all dimensions at compile time
            # Can unroll loops, optimize memory layout
            for i in 0..R:
                for j in 0..K:
                    for k in 0..C:
                        result.data[i*K + j] += 
                            self.data[i*C + k] * other.data[k*K + j]
            
            return result
    
    # Usage:
    let a = Matrix[4, 4].identity()
    let b = Matrix[4, 3].from_array(...)
    let c = a.multiply[3](&b)  # Result type: Matrix[4, 3]

Example 2: Buffer pre-allocation with known sizes

    fn parse_packet[MaxSize: int](data: &[u8]) -> Result[Packet, Error]:
        # Stack-allocated buffer, size known at compile time
        var fields: [Field; MaxSize]
        var count = 0
        
        # Compiler can optimize bounds checks away
        for byte in data:
            if count >= MaxSize:
                return Err(Error.TooManyFields)
            fields[count] = parse_field(byte)?
            count += 1
        
        return Ok(Packet { fields: fields[0..count] })
    
    # Different protocols use different specializations:
    let ip_packet = parse_packet[20](ip_data)?   # IPv4: max 20 fields
    let tcp_packet = parse_packet[40](tcp_data)? # TCP: max 40 fields

Example 3: Compile-time string processing

    const fn compute_hash(s: &str) -> u64:
        # Runs at compile time
        var hash: u64 = 0
        for c in s.bytes():
            hash = hash * 31 + c as u64
        return hash
    
    const API_KEY_HASH = compute_hash("MySecretAPIKey")  # Computed at compile time
    
    fn check_api_key(input: &str) -> bool:
        # Only the hash is in the binary, not the original key
        return compute_hash(input) == API_KEY_HASH

Performance Benefits
~~~~~~~~~~~~~~~~~~~~

Compile-time parameterization enables:

1. **Loop unrolling:** Fixed iteration counts can be fully unrolled
2. **Dead code elimination:** Compile-time conditionals remove unused branches
3. **SIMD optimization:** Generate vector instructions for known sizes
4. **Memory layout:** Optimal struct packing based on known sizes
5. **Constant propagation:** Intermediate computations folded away
6. **Specialization:** Different algorithms for different parameter values

All of this happens at compile time with zero runtime overhead. The generated 
code is as efficient as hand-written assembly for the specific parameters.

Integration with Teaching
~~~~~~~~~~~~~~~~~~~~~~~~~

For beginners, compile-time parameterization is introduced gradually:

1. **First exposure:** Fixed-size arrays `[T; N]` (N is a compile-time parameter)
2. **Basic usage:** Functions that take sized arrays
3. **Intermediate:** Writing parameterized functions for generic sizes
4. **Advanced:** Compile-time computation, conditional compilation, SIMD

The explainer system makes this concrete:

    $ pyritec --explain compile-time-param
    
    Compile-Time Parameters
    =======================
    
    Parameters in square brackets [N: int] are evaluated at compile time.
    The compiler creates a separate version of your function for each
    unique value of N that you use.
    
    Think of it like this:
      fn process[4](data) compiles to process_4(data)
      fn process[8](data) compiles to process_8(data)
    
    Each version can be optimized for its specific N value (loop unrolling,
    specialized algorithms, etc.).
    
    Cost: Zero runtime cost. Slight increase in binary size if many different
          values are used (each specialization adds code).

Comparison to Other Languages
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  â€¢ **C++ templates:** Similar power, but clearer syntax and better errors
  â€¢ **Rust const generics:** Similar concept, extended with more parameter types
  â€¢ **Mojo parameters:** Direct inspiration, adapted to Pyrite's syntax
  â€¢ **Zig comptime:** Related but different (Pyrite uses explicit [N] syntax)

Pyrite's approach prioritizes readability (clear [] syntax) and teachability 
(explicit is better than implicit).

Comptime Execution
~~~~~~~~~~~~~~~~~~

Pyrite may introduce a keyword (like comptime or use const as above) to run 
arbitrary code at compile time. This can be more powerful than just const 
functions - it could allow conditional compilation logic in regular code. 

For instance, Zig allows things like:

    if(@import("builtin").os == .windows){
        // windows-specific code
    }else{
        // other OS code
    }

Pyrite might have a way to inspect compile-time configuration or types. For 
example, generating code based on type sizes or existence of implementations. 
This can eliminate the need for a separate preprocessor in many cases.

Generics vs Macros
~~~~~~~~~~~~~~~~~~

A lot of what one might use C++ templates or C preprocessor macros for can be 
handled by Pyrite's generics and const evaluation. However, there might still be 
scenarios where writing code that generates other code (metaprogramming) is 
useful. Pyrite could consider a hygienic macro system similar to Rust's (where 
you can write code that produces code in a controlled, compiler-integrated way). 
But this is advanced and can be tricky for beginners, so it might not be in the 
initial release. 

The philosophy is to try to achieve most metaprogramming needs via regular 
language constructs (like generics and const eval) before resorting to macros. If 
macros are introduced, they would likely be more like Rust's - i.e., part of the 
language's syntax and invoked in code, rather than a textual substitution like 
C's #define.

No Textual Preprocessor
~~~~~~~~~~~~~~~~~~~~~~~~

Unlike C, Pyrite doesn't rely on a textual preprocessor. This avoids a whole 
class of issues with macros and order of expansion and so on. Instead, features 
like conditional compilation, including other files, etc., are handled in more 
structured ways (via the module system or compile-time ifs). 

For example, Pyrite might have attributes or built-in conditionals for 
compilation target, so you could do @cfg(windows) to include some code only on 
Windows, akin to Rust's #[cfg(windows)]. The aim is to integrate these with the 
language grammar rather than as raw text substitution, making it safer and 
clearer.

Configuration Attributes (Feature Flags)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pyrite provides comprehensive conditional compilation through @cfg attributes, 
enabling platform-specific code, optional features, and build configuration 
without textual preprocessing:

Platform Conditionals:

    @cfg(target_os = "windows")
    fn get_separator() -> char:
        return '\\'
    
    @cfg(target_os = "linux")
    fn get_separator() -> char:
        return '/'
    
    @cfg(target_os = "macos")
    fn get_separator() -> char:
        return '/'
    
    # Or combined:
    @cfg(any(target_os = "linux", target_os = "macos"))
    fn get_separator() -> char:
        return '/'

Architecture Conditionals:

    @cfg(target_arch = "x86_64")
    fn fast_crypto() -> Cipher:
        # Use AES-NI hardware instructions
        return AesNiCipher::new()
    
    @cfg(target_arch = "aarch64")
    fn fast_crypto() -> Cipher:
        # Use ARM crypto extensions
        return ArmCryptoChip::new()
    
    @cfg(not(any(target_arch = "x86_64", target_arch = "aarch64")))
    fn fast_crypto() -> Cipher:
        # Software fallback
        return SoftwareCipher::new()

Feature Flags:

    # Quarry.toml
    [features]
    default = ["json", "http", "observability"]
    json = ["serde"]
    http = ["http-client", "http-server"]
    observability = ["log", "trace", "metrics"]
    gpu = ["cuda"]
    minimal = []                     # No optional features
    
    # Code using features:
    @cfg(feature = "json")
    import json
    
    @cfg(feature = "gpu")
    import std::gpu
    
    @cfg(not(feature = "observability"))]
    fn log::info(msg: &str, fields: Map[String, Value]):
        # No-op when observability disabled (zero cost)
        pass

Build Configuration:

    @cfg(debug_assertions)
    fn expensive_check():
        # Only in debug builds
        validate_invariants()
    
    @cfg(release)
    fn expensive_check():
        # No-op in release builds
        pass

Combined Conditions:

    @cfg(all(target_os = "linux", target_arch = "x86_64", feature = "gpu"))]
    fn use_cuda():
        # Linux + x86_64 + GPU feature enabled
        ...
    
    @cfg(any(target_os = "windows", target_os = "macos"))]
    fn desktop_only():
        ...
    
    @cfg(not(target_pointer_width = "64"))]
    fn handle_32bit():
        ...

Available cfg Keys:

    target_os:            "windows", "linux", "macos", "ios", "android", "freebsd", "none" (bare-metal)
    target_arch:          "x86_64", "aarch64", "arm", "riscv64", "wasm32", etc.
    target_pointer_width: "32", "64"
    target_endian:        "little", "big"
    target_env:           "gnu", "msvc", "musl"
    feature:              User-defined feature flags
    debug_assertions:     true in debug builds
    test:                 true when running tests
    
Why Structured Conditionals Matter:

  â€¢ **Type-checked:** Conditions checked at compile time, not runtime
  â€¢ **No preprocessor pitfalls:** No macro expansion order issues
  â€¢ **IDE-friendly:** Grayed-out code for disabled configurations
  â€¢ **Explicit:** See exactly what code runs on what platforms
  â€¢ **Safe:** Can't create invalid combinations

This completes Pyrite's "no textual preprocessor" philosophy while providing all 
the power of conditional compilation in a type-safe, structured way.

Reflection and Code Generation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the future, Pyrite might allow limited compile-time reflection, such as 
iterating over the fields of a struct at compile time or generating trait 
implementations automatically (like deriving in Rust). This could be done via 
compile-time functions that use a reflection API, or via macro-like facilities. 
But again, this is advanced and can be added once the base language is solid.

By moving work to compile time, Pyrite enables optimizations and flexibility 
without hitting runtime performance. A classic example is building a big lookup 
table: you could compute it with a const loop at compile time rather than at 
program startup. Another example is complex computations for constants (like 
sin/cos tables, regex compilation to automata, etc.) done once at compile time. 
All of this follows the zero-cost ethos: if you can do it at compile time, you 
pay nothing at runtime except possibly a larger binary if data tables are 
generated.

Overall, compile-time execution and metaprogramming features in Pyrite are about 
empowering the programmer to let the compiler do more work (safely) so that the 
runtime work is less. But these features are optional and will typically be used 
by more advanced users or library authors; a beginner might not encounter them on 
day one.

================================================================================
8. TOOLING: QUARRY BUILD SYSTEM
================================================================================

Pyrite's official build system and package manager is Quarry. The design 
philosophy mirrors Cargo (Rust's beloved build tool): provide one obvious 
workflow that handles everything, eliminate configuration complexity, and make 
the right thing the easy thing.

Developer surveys consistently show that great tooling is essential for language 
adoption. Quarry is designed as a first-class component of Pyrite, not an 
afterthought.

8.1 Core Quarry Workflow
--------------------------------------------------------------------------------

Single Command Philosophy
~~~~~~~~~~~~~~~~~~~~~~~~~

Quarry provides one intuitive command for each common task:

    quarry new myproject          # Create new project
    quarry build                  # Compile (debug mode)
    quarry build --release        # Compile (optimized)
    quarry run                    # Build and execute
    quarry test                   # Run all tests
    quarry bench                  # Run benchmarks
    quarry doc                    # Generate documentation
    quarry fmt                    # Format all code
    quarry lint                   # Run linter
    quarry clean                  # Remove build artifacts
    quarry publish                # Publish to package registry

No makefiles, no build scripts, no configuration hell. It just works.

Script Mode: Single-File Workflow
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For rapid prototyping, learning, and simple scripts, Pyrite supports a 
zero-configuration single-file workflow that feels like Python but compiles to 
native code:

    pyrite run hello.pyrite       # Compile and execute single file
    pyrite build hello.pyrite     # Compile to executable
    pyrite hello.pyrite           # Shorthand for 'run'

No project structure required. No Quarry.toml needed. Just write code and run it.

Example workflow:

    # Create a simple script
    $ cat > hello.pyrite
    fn main():
        print("Hello, Pyrite!")
    ^D
    
    # Run it immediately
    $ pyrite run hello.pyrite
    Hello, Pyrite!
    
    # Compile to standalone executable
    $ pyrite build hello.pyrite
    Compiling hello.pyrite â†’ hello (or hello.exe on Windows)
    
    $ ./hello
    Hello, Pyrite!

Shebang Support
~~~~~~~~~~~~~~~

On Unix-like systems, Pyrite scripts can use shebang lines for direct execution:

    #!/usr/bin/env pyrite
    
    fn main():
        print("Executable Pyrite script!")

Make the file executable and run:

    $ chmod +x script.pyrite
    $ ./script.pyrite
    Executable Pyrite script!

Script Mode Implementation
~~~~~~~~~~~~~~~~~~~~~~~~~~

Script mode is implemented as intelligent caching:

  1. **First run:** Compiles to ~/.cache/pyrite/scripts/<hash>/binary
  2. **Subsequent runs:** Reuses cached binary if source unchanged
  3. **Automatic recompilation:** Detects source changes and recompiles
  4. **Full ownership checking:** Script mode uses the same compiler with all 
     safety guarantees
  5. **Zero runtime overhead:** Cached binaries are native executables, not 
     interpreted

This means script mode has the convenience of Python's `python script.py` with 
the performance of compiled native code.

Cache management:

    pyrite cache clean            # Remove all cached scripts
    pyrite cache list             # Show cached scripts
    pyrite cache clear hello.pyrite  # Remove specific script cache

Why Script Mode Matters
~~~~~~~~~~~~~~~~~~~~~~~~

Script mode eliminates the biggest friction point for Python developers trying 
Pyrite:

  â€¢ **Instant gratification:** Write code, run code, see results
  â€¢ **No ceremony:** No project structure until you need it
  â€¢ **Natural progression:** Start with scripts, graduate to projects
  â€¢ **Learning-friendly:** Beginners can experiment immediately
  â€¢ **Still fast:** Caching means second run is instant

This addresses the "first 60 seconds" problem: a Python developer's first 
experience with Pyrite should feel familiar, not foreign.

Comparison:

    # Python
    $ python script.py           # Interpreted, ~100ms startup
    
    # Pyrite script mode
    $ pyrite run script.pyrite   # Compiled + cached, ~1ms after first run
    
    # Pyrite project mode
    $ cd project && quarry run   # Full build system

All three workflows coexist. Use the right tool for the job.

Migration Path
~~~~~~~~~~~~~~

When a script grows complex enough to need dependencies, migrate to project mode:

    $ quarry init              # Convert current directory to Quarry project
    $ mv script.pyrite src/main.pyrite
    $ quarry add dependency-name
    $ quarry build

Quarry detects single-file scripts and can auto-generate appropriate Quarry.toml.

Project Structure
~~~~~~~~~~~~~~~~~

quarry new creates a standard layout:

    myproject/
    â”œâ”€â”€ Quarry.toml        # Project manifest
    â”œâ”€â”€ src/
    â”‚   â””â”€â”€ main.pyrite    # Entry point
    â”œâ”€â”€ tests/
    â”‚   â””â”€â”€ test_main.pyrite
    â””â”€â”€ docs/
        â””â”€â”€ README.md

Quarry.toml example:

    [package]
    name = "myproject"
    version = "0.1.0"
    authors = ["Your Name <you@example.com>"]
    edition = "2025"
    
    [dependencies]
    json = "1.2"
    http-client = "3.0"
    
    [dev-dependencies]
    test-utils = "0.5"

8.2 Dependency Management
--------------------------------------------------------------------------------

Declarative Dependencies
~~~~~~~~~~~~~~~~~~~~~~~~

Dependencies are declared in Quarry.toml with semantic versioning:

    [dependencies]
    crypto = "2.1"          # Any 2.x version >= 2.1
    parser = "=1.5.3"       # Exact version
    utils = { git = "https://github.com/user/utils.git", branch = "main" }

quarry build automatically:
  â€¢ Downloads dependencies
  â€¢ Resolves version conflicts
  â€¢ Generates Quarry.lock (lockfile for reproducibility)
  â€¢ Caches builds for speed

Reproducible Builds
~~~~~~~~~~~~~~~~~~~

Quarry.lock ensures every developer/CI system builds identical binaries:

    [[package]]
    name = "crypto"
    version = "2.1.4"
    checksum = "a8f39d..."
    
    [[package]]
    name = "parser"
    version = "1.5.3"
    checksum = "b3c82f..."

Lockfile committed to version control guarantees reproducibility across 
environments and time.

8.3 Official Package Registry
--------------------------------------------------------------------------------

Pyrite packages are published to the official Quarry Registry (aspirational: quarry.dev):

    quarry publish

Publishing requirements:
  â€¢ Semantic versioning
  â€¢ Documentation for public APIs
  â€¢ Passing tests (quarry test must succeed)
  â€¢ License declaration
  â€¢ Security audit for unsafe code (optional but recommended)

Discovery and search:

    quarry search "json parser"
    quarry info json-parser

Registry provides:
  â€¢ API documentation (auto-generated)
  â€¢ Download statistics
  â€¢ Security advisories
  â€¢ Dependency graphs
  â€¢ Version compatibility matrices

8.4 Testing Framework
--------------------------------------------------------------------------------

Built-in Test Support
~~~~~~~~~~~~~~~~~~~~~

Tests are first-class in Pyrite:

    # In src/math.pyrite
    fn add(a: int, b: int) -> int:
        return a + b
    
    @test
    fn test_add():
        assert add(2, 3) == 5
        assert add(-1, 1) == 0
    
    @test
    fn test_add_overflow():
        # Tests can use Result types
        match add(MAX_INT, 1):
            Err(_):
                pass  # Expected overflow in debug mode
            Ok(_):
                fail("Should have overflowed")

Run tests:

    quarry test                    # All tests
    quarry test test_add          # Specific test
    quarry test --verbose         # Detailed output

Benchmark Support
~~~~~~~~~~~~~~~~~

Performance testing built-in:

    @bench
    fn bench_parse_json(b: &mut Bencher):
        let data = load_test_data()
        b.iter(fn():
            parse_json(data)
        )

Run benchmarks:

    quarry bench                   # Run all benchmarks
    quarry bench --save baseline   # Save baseline for comparison

8.5 Code Formatting
--------------------------------------------------------------------------------

Official Formatter
~~~~~~~~~~~~~~~~~~

quarry fmt formats all code according to official style guide:

    quarry fmt                     # Format entire project
    quarry fmt src/parser.pyrite   # Format specific file
    quarry fmt --check             # Check without modifying (for CI)

No configuration options. Zero style debates. One canonical format.

Formatting rules:
  â€¢ 4 spaces for indentation (enforced)
  â€¢ Maximum line length: 100 characters
  â€¢ Consistent spacing around operators
  â€¢ Idiomatic pattern for common constructs

Example transformation:

    # Before
    fn   foo(x:int,y:int)->int:
        return    x+y

    # After quarry fmt
    fn foo(x: int, y: int) -> int:
        return x + y

8.6 Learning Profile Mode
--------------------------------------------------------------------------------

To support Pyrite's goal of being approachable to beginners, Quarry provides a 
"Learning Profile" that packages beginner-friendly defaults into a one-command 
setup:

    quarry new --learning my_project

This creates a project configured for gentle onboarding:

  â€¢ Enables --core-only mode (rejects advanced features)
  â€¢ Sets beginner lint level (quarry lint --level=beginner)
  â€¢ Includes extra IDE hover help
  â€¢ Forbids unsafe blocks by default
  â€¢ Configures progressive learning paths in tooling
  â€¢ Adds commented examples in generated files

The Learning Profile is purely a packaging of existing features - no new semantics, 
just a curated "beginner bundle" that grows with the developer:

Configuration (Quarry.toml):

    [profile.learning]
    core-only = true
    lint-level = "beginner"
    unsafe-forbidden = true
    extra-diagnostics = true
    suggest-alternatives = true

Migration path:

    # After mastering basics, disable learning mode
    quarry config set learning false
    
    # Or migrate incrementally
    quarry config set core-only false  # Enable advanced features
    quarry config set lint-level intermediate

Benefits:
  â€¢ Zero new language complexity (just configuration)
  â€¢ One-command setup for educators and self-learners
  â€¢ Natural graduation path as skills advance
  â€¢ All code remains valid Pyrite (no dialect fragmentation)

Marketing impact: "Pyrite has a beginner mode" is a powerful message for 
Python developers exploring systems programming.

Implementation: Beta Release (after core compiler and lints are stable)

8.7 Interactive REPL (Beta Release)
--------------------------------------------------------------------------------

To deliver on Pyrite's promise of Python-like approachability, the language 
provides an interactive Read-Eval-Print Loop (REPL) with ownership visualization 
and cost transparency built in. This is essential for the "Pythonic" claim - 
Python developers expect instant experimentation.

Command Usage
~~~~~~~~~~~~~

    pyrite repl                      # Launch interactive shell
    pyrite repl --explain            # Enhanced mode with ownership visualization
    pyrite repl --script=setup.py    # Load script before interactive session

Basic REPL Workflow
~~~~~~~~~~~~~~~~~~~

    $ pyrite repl
    
    Pyrite 1.0.0 (2025-12-18)
    Type :help for help, :quit to exit
    
    >>> let x = 5
    let x: int = 5
    
    >>> let data = List[int]([1, 2, 3])
    let data: List[int] = List[int]([1, 2, 3])
    [Heap] [Move] Stack: 24B, Heap: 12B
    
    >>> data.push(4)
    error[P0594]: cannot borrow 'data' as mutable
      = note: 'data' declared with 'let' (immutable)
      = help: Use 'var data' to allow mutation
    
    >>> var mutable_data = List[int]([1, 2, 3])
    var mutable_data: List[int] = List[int]([1, 2, 3])
    
    >>> mutable_data.push(4)
    () â† pushed successfully
    
    >>> mutable_data
    List[int]([1, 2, 3, 4])

Enhanced REPL Commands
~~~~~~~~~~~~~~~~~~~~~~~

The REPL provides special commands for exploration:

    >>> :type data
    Type: List[int]
    Badges: [Heap] [Move] [MayAlloc]
    Stack: 24 bytes, Heap: 16 bytes (capacity for 4 elements)
    Owner: 'data', Not moved, Not borrowed
    
    >>> :ownership data
    Ownership State for 'data'
    ==========================
    Owner: 'data' (line 5)
    Moved: No
    Borrowed: No active borrows
    
    Next operations:
      âœ“ Can read: data.length(), data[0], etc.
      âœ“ Can mutate: data.push(), data.clear(), etc.
      âš ï¸  Passing to function will move (use &data to borrow)
    
    >>> :cost
    Session Cost Analysis
    =====================
    Allocations: 2 (40 bytes)
      â€¢ Line 5: List[int].new() - 24 bytes
      â€¢ Line 8: mutable_data.push(4) - 16 bytes (reallocation)
    
    Copies: 0
    Total memory: 40 bytes
    
    >>> :explain P0234
    [Opens detailed error explanation for P0234]
    
    >>> :clear
    [Clears session, resets state]

Ownership Visualization Mode
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

With --explain flag, REPL shows ownership changes in real-time:

    $ pyrite repl --explain
    
    >>> let data = List[int]([1, 2, 3])
    
    â”Œâ”€â”€â”€â”€â”€â”
    â”‚data â”‚ â† OWNER (owns heap allocation)
    â””â”€â”€â”€â”€â”€â”˜
    
    >>> process(data)
    
    data â”€â”€[MOVED]â”€â”€> process()
    
    >>> data.length()
    
    error: Cannot use moved value
    
    â”Œâ”€â”€â”€â”€â”€â”
    â”‚data â”‚ â† INVALID (moved on line 2)
    â””â”€â”€â”€â”€â”€â”˜
    
    Fix suggestions:
      1. process(&data)  # Borrow instead
      2. data.clone()    # Copy before moving

This real-time visualization makes ownership tangible - learning happens 
immediately, not after compilation.

Function Definitions
~~~~~~~~~~~~~~~~~~~~

Define functions interactively:

    >>> fn add(a: int, b: int) -> int:
    ...     return a + b
    ... 
    fn add(a: int, b: int) -> int
    
    >>> add(5, 3)
    8
    
    >>> fn process[N: int](arr: [int; N]) -> int:
    ...     var sum = 0
    ...     for x in arr:
    ...         sum += x
    ...     return sum
    ... 
    fn process[N: int](arr: [int; N]) -> int
    
    >>> process[3]([1, 2, 3])
    6

Multi-Line Editing
~~~~~~~~~~~~~~~~~~

REPL supports multi-line constructs with intelligent continuation:

    >>> for i in 0..5:
    ...     print(i)
    ... 
    0
    1
    2
    3
    4

Session Management
~~~~~~~~~~~~~~~~~~

    :save session.pyr               # Save session to file
    :load session.pyr               # Load previous session
    :history                        # Show command history
    :clear                          # Reset session state

Import Support
~~~~~~~~~~~~~~

Import modules during interactive session:

    >>> import math
    >>> math.sqrt(16.0)
    4.0
    
    >>> import json
    >>> json.parse('{"key": "value"}')
    Ok(Object({"key": "value"}))

Performance Mode
~~~~~~~~~~~~~~~~

For performance-critical experimentation:

    >>> :perf start                 # Begin performance tracking
    >>> expensive_computation()
    >>> :perf stop
    
    Performance Profile
    ===================
    Duration: 234ms
    Allocations: 1,247 (1.2 MB)
    Peak memory: 2.4 MB

Why REPL Is Essential
~~~~~~~~~~~~~~~~~~~~~

The absence of a REPL would be a **critical gap** for Pyrite's positioning:

**Python developers expect it:**
  â€¢ REPL is 50% of Python's "instant gratification" appeal
  â€¢ Exploration-driven learning is how beginners understand concepts
  â€¢ "Just try it" is more powerful than "read about it"
  â€¢ Without REPL, "Pythonic systems language" claim feels hollow

**Learning acceleration:**
  â€¢ Try ownership patterns instantly without compilation
  â€¢ See borrow checker feedback in real-time
  â€¢ Experiment with types, memory layout, costs interactively
  â€¢ Reduce friction from "change code â†’ compile â†’ run" to "just try"

**Competitive necessity:**
  â€¢ Rust: Has REPL (evcxr) but third-party, not built-in
  â€¢ Python: REPL is flagship feature
  â€¢ JavaScript: Node.js REPL is critical to adoption
  â€¢ Swift: Has REPL (swift repl) for iOS development
  â€¢ Mojo: Has REPL (mojo repl) with instant feedback

**Teaching impact:**
  â€¢ Instructors can demonstrate concepts live
  â€¢ Students experiment during lectures
  â€¢ Ownership becomes interactive ("try moving it, see what happens")
  â€¢ Reduces "scary compiler" perception

Implementation Approach
~~~~~~~~~~~~~~~~~~~~~~~

REPL compiles each expression/statement incrementally:

  â€¢ JIT compilation for speed (LLVM OrcJIT or similar)
  â€¢ Ownership tracking persists across statements
  â€¢ Type inference works across session
  â€¢ Memory is actual (not simulated)-real allocations, real costs

Safety in REPL:
  â€¢ Same safety guarantees as compiled code
  â€¢ Unsafe blocks still require unsafe marker
  â€¢ Ownership rules enforced (prevents use-after-free even in REPL)

Integration with Teaching:
  â€¢ quarry learn can open REPL for specific exercises
  â€¢ Compiler errors link to REPL examples: "Try this in REPL"
  â€¢ Documentation examples include REPL transcripts

Example Teaching Session:

    $ pyrite repl --explain
    
    Pyrite Interactive Shell (Learning Mode)
    ========================================
    
    Try this exercise: Create a list, move it, observe the error
    
    >>> let data = List[int]([1, 2, 3])
    
    âœ“ Created owner 'data'
    
    >>> process(data)
    
    âš ï¸  'data' moved to 'process'
    
    >>> data.length()
    
    âœ— Error: 'data' was moved on line 2
    
    What happened?
      â€¢ Line 1: 'data' owned the list
      â€¢ Line 2: Ownership transferred to 'process()'
      â€¢ Line 3: 'data' no longer valid
    
    Fix: Use &data to borrow instead
    
    >>> # Try again...

This transforms abstract concepts into interactive, visual understanding. The 
REPL is not just a nice-to-have - it's **essential for Pyrite's identity** as the 
"Pythonic systems language."

Implementation: Beta Release (high priority, high impact)
Complexity: Moderate (JIT compilation, incremental state management)
Impact: Critical (without REPL, Python developers feel the gap immediately)

8.8 Automatic Code Fixes
--------------------------------------------------------------------------------

quarry fix - Apply Compiler Suggestions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pyrite's compiler doesn't just identify problems - it suggests concrete fixes. The 
quarry fix command automatically applies safe, mechanical transformations that 
the compiler recommends.

Usage:

    quarry fix                     # Apply all safe fixes in project
    quarry fix src/main.pyrite     # Fix specific file
    quarry fix --preview           # Show fixes without applying
    quarry fix --interactive       # Prompt before each fix

Interactive Ownership Error Resolution
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When quarry fix encounters ownership/borrowing errors, it presents numbered, 
selectable fixes that teach the concepts while solving the problem:

Example compilation error:

    error[P0234]: cannot use moved value 'data'
      ----> main.py:15:10
       |
    12 |     let data = List[int]([1, 2, 3])
       |         -------- value allocated here
    13 |     process(data)
       |             -------- value moved here (ownership transferred to 'process')
    15 |     print(data.length())
       |          ^^^^ cannot use 'data' after it was moved
       |
       = help: Run 'quarry fix --interactive' to choose a solution

Running quarry fix --interactive presents:

    Found ownership error at main.py:15:10
    
    'data' was moved on line 13 and cannot be used again.
    
    Select a fix:
      1. Pass a reference to 'process' instead
         - Change: process(data) â†’ process(&data)
         - Effect: 'data' remains usable after the call
         - When to use: Function only needs to read the data
      
      2. Clone the value before moving
         - Change: process(data) â†’ process(data.clone())
         - Effect: 'data' remains usable (original retained)
         - When to use: Function needs ownership but you need the data later
         - Cost: Allocates and copies the entire list
      
      3. Restructure to return ownership
         - Change: data = process(data)
         - Effect: 'process' gives ownership back
         - When to use: Function transforms data and you need result
      
      4. Skip this fix
    
    Choice (1-4): _

This interactive mode:
  â€¢ **Teaches patterns:** Each option explains when to use it
  â€¢ **Shows trade-offs:** Performance implications visible (e.g., "allocates")
  â€¢ **Builds intuition:** Repeated fixes teach ownership patterns organically
  â€¢ **Safe by default:** Only presents compiler-verified transformations

Non-interactive mode (quarry fix) applies the safest/cheapest fix automatically 
(typically option 1: borrow instead of move).

Ownership Error Coverage:
  â€¢ Move-after-use: Offer borrow, clone, or restructure
  â€¢ Borrow conflicts: Suggest scope reduction or explicit drop
  â€¢ Lifetime issues: Add lifetime annotations or restructure
  â€¢ Double move: Clone first move or restructure logic

Types of automatic fixes:

1. **Ownership fixes:**
   
   Before:
       let data = List[int]([1, 2, 3])
       process(data)
       print(data.length())  # ERROR: value moved
   
   After quarry fix:
       let data = List[int]([1, 2, 3])
       process(data.clone())  # Added .clone()
       print(data.length())

2. **Borrowing fixes:**
   
   Before:
       fn process(list: List[int]) -> int:  # Takes ownership
           list.length()
   
   After quarry fix:
       fn process(list: &List[int]) -> int:  # Borrows instead
           list.length()

3. **Performance fixes:**
   
   Before:
       for i in 0..1000:
           let v = List[int].new()  # Allocates in loop
   
   After quarry fix:
       let v = List[int].with_capacity(10)
       for i in 0..1000:
           v.clear()

4. **Type annotations:**
   
   Before:
       let x = parse(input)  # Ambiguous type
   
   After quarry fix:
       let x: Result[Config, Error] = parse(input)

5. **Import cleanup:**
   - Remove unused imports
   - Add missing imports
   - Sort and organize imports

6. **Lifetime annotations (advanced):**
   - Add explicit lifetime annotations where inference fails
   - Only for complex generic code

Why quarry fix is transformative:

  â€¢ **Accelerates learning:** Beginners see the pattern of correct code
  â€¢ **Reduces friction:** Fix 10 borrow errors in seconds, not minutes
  â€¢ **Builds intuition:** Repeated fixes teach ownership patterns
  â€¢ **Safe:** Only applies mechanical, verified transformations
  â€¢ **IDE integration:** Real-time "Quick Fix" in editors

Safety guarantees:
  â€¢ Never changes program semantics incorrectly
  â€¢ Only applies fixes the compiler marks as "safe mechanical change"
  â€¢ Preserves code comments and formatting
  â€¢ Can be undone with version control

This is the natural evolution of Pyrite's teaching compiler: diagnose, explain, 
AND fix. It's what makes Elm, rust-analyzer, and go fmt feel magical.

8.9 Fuzzing and Sanitizers
--------------------------------------------------------------------------------

To make Pyrite a production-ready systems language and achieve widespread developer 
adoption, 
runtime verification tools are essential for catching bugs that static analysis 
misses. These tools cost zero at runtime (only used during testing) but multiply 
reliability dramatically.

quarry fuzz - Coverage-Guided Fuzzing
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Automatic test generation that explores edge cases:

    quarry fuzz                         # Fuzz all fuzzable functions
    quarry fuzz parse_packet            # Fuzz specific function
    quarry fuzz --duration=1h           # Run for 1 hour
    quarry fuzz --corpus=./inputs       # Use seed inputs

How it works:
  â€¢ Identifies functions marked @fuzz or with fuzz_ prefix
  â€¢ Generates random inputs based on parameter types
  â€¢ Tracks code coverage and prioritizes unexplored paths
  â€¢ Saves crash-inducing inputs for reproduction

Example fuzzable function:

    @fuzz
    fn parse_packet(data: &[u8]) -> Result[Packet, Error]:
        # Fuzzer generates thousands of random byte arrays
        # Finds inputs that panic, overflow, or violate assertions
        ...

Fuzzing output:

    $ quarry fuzz parse_packet --duration=10m
    
    Fuzzing: parse_packet
    =====================
    
    Progress:
      â€¢ Iterations: 1,247,892
      â€¢ Coverage: 89% of function body
      â€¢ Unique paths: 234
      â€¢ Crashes found: 2
    
    ðŸ”´ CRASH #1: Integer overflow
       Input: [0xff, 0xff, 0xff, 0xff, 0x01]
       Location: src/parser.py:234
       
       thread 'fuzzer' panicked at 'attempt to multiply with overflow'
         length = u32::from_bytes([0xff, 0xff, 0xff, 0xff])  # 4,294,967,295
         total_size = length * FIELD_SIZE  # Overflows!
       
       Fix: Use checked arithmetic:
         let total_size = length.checked_mul(FIELD_SIZE)?
       
       Saved to: fuzz/crashes/crash-1.bin
       Reproduce: quarry test --fuzz-input=fuzz/crashes/crash-1.bin
    
    ðŸ”´ CRASH #2: Slice index out of bounds
       Input: [0x00, 0x05]  # Claims 5 fields, provides 0
       Location: src/parser.py:267
       [Details...]
    
    Summary:
      âœ“ Found 2 bugs that unit tests missed
      âœ“ Corpus saved to fuzz/corpus/ (1,247 interesting inputs)
      âœ“ Add these to regression tests

Integration with CI:

    # Run fuzzing in CI for 5 minutes
    quarry fuzz --ci --duration=5m
    
    # Exit 0 if no crashes, exit 1 if crashes found

Why Fuzzing Matters:
  â€¢ Finds edge cases humans miss (off-by-one, overflow, malformed inputs)
  â€¢ Generates regression tests automatically (save crash inputs)
  â€¢ Industry standard for security-critical code
  â€¢ Minimal setup cost (just mark functions with @fuzz)

Fuzz Testing Best Practices:

    @fuzz
    @cost_budget(cycles=10000, allocs=2)  # Budgets still enforced
    fn parse_header(data: &[u8]) -> Result[Header, Error]:
        # Fuzzer explores all code paths
        # Cost budget prevents infinite loops or excessive allocation
        ...

Implementation: Beta Release (after core test framework is stable)

quarry sanitize - Runtime Error Detection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Compiler-integrated sanitizers detect undefined behavior and memory errors at 
runtime (debug/test builds only):

    quarry sanitize --asan              # AddressSanitizer
    quarry sanitize --tsan              # ThreadSanitizer  
    quarry sanitize --ubsan             # UndefinedBehaviorSanitizer
    quarry sanitize --msan              # MemorySanitizer
    quarry sanitize --all               # All sanitizers

AddressSanitizer (ASan) - Memory Error Detection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Detects memory safety violations at runtime:

    quarry sanitize --asan
    quarry test  # Tests run with ASan instrumentation

Catches:
  â€¢ Use-after-free
  â€¢ Heap buffer overflow
  â€¢ Stack buffer overflow
  â€¢ Memory leaks
  â€¢ Use of uninitialized memory

Example output:

    $ quarry sanitize --asan
    $ quarry test
    
    Running tests with AddressSanitizer...
    
    test test_parse_large ... ok
    test test_edge_cases ... FAILED
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    AddressSanitizer: heap-buffer-overflow
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    READ of size 4 at 0x60300000eff4 thread T0
        #0 process_buffer src/processor.py:234
        #1 test_edge_cases tests/test_parser.py:45
    
    0x60300000eff4 is located 4 bytes after 1024-byte region
    allocated by thread T0:
        #0 malloc
        #1 List::with_capacity src/std/collections.py:89
        #2 test_edge_cases tests/test_parser.py:42
    
    SUMMARY: AddressSanitizer: heap-buffer-overflow
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Fix: Buffer accessed beyond allocated size
      Line 234: buffer[1024] when buffer.len() == 1024
      Index 1024 is out of bounds (valid: 0..1023)

ThreadSanitizer (TSan) - Data Race Detection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Detects concurrent memory access violations:

    quarry sanitize --tsan
    quarry test

Catches:
  â€¢ Data races (unsynchronized concurrent access)
  â€¢ Lock order inversions (potential deadlocks)
  â€¢ Destroyed mutex still in use
  â€¢ Thread leaks

Example output:

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    ThreadSanitizer: data race
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Write of size 8 at 0x7b0400001234 by thread T2:
        #0 update_counter src/stats.py:156
        #1 worker_thread src/main.py:89
    
    Previous write of size 8 at 0x7b0400001234 by thread T1:
        #0 update_counter src/stats.py:156
        #1 worker_thread src/main.py:89
    
    Location: counter (global variable)
    
    SUMMARY: ThreadSanitizer: data race
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Fix: Use atomic operations or mutex:
      
      Option 1 (Atomic):
        static COUNTER: AtomicU64 = AtomicU64::new(0)
        COUNTER.fetch_add(1, Ordering::Relaxed)
      
      Option 2 (Mutex):
        static COUNTER: Mutex<u64> = Mutex::new(0)
        *COUNTER.lock() += 1

UndefinedBehaviorSanitizer (UBSan) - UB Detection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Catches undefined behavior even in safe-looking code:

    quarry sanitize --ubsan
    quarry test

Catches:
  â€¢ Integer overflow (in release mode if checks disabled)
  â€¢ Misaligned pointer access
  â€¢ Invalid enum discriminant values
  â€¢ Shift by negative or >= width
  â€¢ Division by zero

Example:

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    UndefinedBehaviorSanitizer: signed integer overflow
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    src/math.py:456:18: runtime error:
      signed integer overflow: 2147483647 + 1 cannot be represented in type 'i32'
    
    Fix: Use checked arithmetic:
      let result = a.checked_add(b)?

Integration with CI
~~~~~~~~~~~~~~~~~~~

Run all sanitizers in continuous integration:

    # .github/workflows/ci.yml
    - name: Run sanitizers
      run: |
        quarry sanitize --asan && quarry test
        quarry sanitize --tsan && quarry test
        quarry sanitize --ubsan && quarry test

Sanitizer builds are slow (2-5x overhead) but catch bugs that slip through 
static analysis and normal testing.

quarry miri - Interpreter-Based UB Detection (Future)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Long-term goal: Rust's Miri-equivalent for Pyrite. An interpreter that catches 
ALL undefined behavior, even in unsafe code:

    quarry miri

Detects:
  â€¢ All memory safety violations
  â€¢ Uninitialized memory reads
  â€¢ Invalid pointer arithmetic
  â€¢ Violating unsafe invariants
  â€¢ Even bugs sanitizers miss

Slower than sanitizers (10-100x) but exhaustive. Perfect for auditing unsafe 
code blocks.

Why Sanitizers Matter
~~~~~~~~~~~~~~~~~~~~~

This is table-stakes for "serious systems language":

  â€¢ Rust has Miri and sanitizer integration
  â€¢ C++ has ASan/TSan/UBSan (industry standard)
  â€¢ Go has race detector (built-in)
  â€¢ Zig is working toward similar tooling

Without sanitizers, Pyrite would seem "less serious" than competitors. With 
them, Pyrite becomes: "Memory-safe by design, verified by runtime checks, 
auditable with interpreters."

Real-world impact:
  â€¢ Chromium: ASan found 3,000+ bugs that code review missed
  â€¢ Rust: Miri caught soundness bugs in stdlib
  â€¢ Industry: Sanitizers are non-negotiable for security-critical code

Cost:
  â€¢ Zero runtime cost (only enabled for testing)
  â€¢ High confidence multiplier (catch what static analysis can't)
  â€¢ Required for safety certification in many domains

Implementation: Beta Release (fuzzing + ASan/TSan/UBSan via LLVM)
             Stable Release (Miri-equivalent interpreter)

8.10 Linting
--------------------------------------------------------------------------------

Multi-Level Linter
~~~~~~~~~~~~~~~~~~

quarry lint provides progressive strictness:

    quarry lint --level=beginner   # Gentle warnings, teaching mode
    quarry lint --level=standard   # Default, balanced
    quarry lint --level=pedantic   # Maximum strictness
    quarry lint --level=performance  # Focus on performance issues

Linter categories:

    Correctness:
      â€¢ Unused variables
      â€¢ Unreachable code
      â€¢ Type mismatches (if inference is ambiguous)
      â€¢ Pattern match exhaustiveness
    
    Style:
      â€¢ Naming conventions (snake_case, CamelCase)
      â€¢ Module organization
      â€¢ Public API documentation coverage
    
    Performance:
      â€¢ Heap allocations (see section 4.5)
      â€¢ Large copies
      â€¢ Unnecessary clones
      â€¢ Inefficient algorithms (e.g., O(nÂ²) where O(n) available)
    
    Safety:
      â€¢ Unsafe block auditing
      â€¢ FFI boundary checks
      â€¢ Concurrency patterns (potential deadlocks, etc.)

Customization
~~~~~~~~~~~~~

Project-level lint configuration in Quarry.toml:

    [lints]
    level = "standard"
    allow = ["large_copy"]      # Suppress specific warnings
    deny = ["heap_in_loop"]     # Elevate warnings to errors

8.10.1 Code Expansion: quarry expand
--------------------------------------------------------------------------------

To demystify compiler transformations and teach how high-level constructs desugar, 
Quarry provides quarry expand to show generated code:

Command Usage
~~~~~~~~~~~~~

    quarry expand file.pyrite              # Show all expansions
    quarry expand file.pyrite --function=foo  # Expand specific function
    quarry expand --closure-expansion      # Show parameter closure inlining

Use Cases
~~~~~~~~~

1. **Parameter closure expansion** - See how fn[...] is inlined:

   Source code:
       algorithm.vectorize[width=8](data.len(), fn[i: int]:
           data[i] = data[i] * 2.0
       )
   
   Expanded (quarry expand):
       const WIDTH = 8
       var i = 0
       
       # SIMD loop (parameter closure body inlined here)
       while i + WIDTH <= data.len():
           let vec = simd::Vec[f32, 8]::load(&data[i])
           let scaled = vec * 2.0
           scaled.store(&mut data[i])
           i += WIDTH
       
       # Remainder loop (parameter closure body inlined here)
       while i < data.len():
           data[i] = data[i] * 2.0  # Original closure body
           i += 1

2. **with statement desugaring** - See try + defer expansion:

   Source:
       with file = try File.open("config.txt"):
           process(file)
   
   Expanded:
       let file = try File.open("config.txt")
       defer:
           file.close()
       process(file)

3. **Compile-time parameter specialization:**

   Source:
       fn process[N: int](data: [u8; N]) -> int:
           # ...
       process[16](buffer)
   
   Expanded:
       # Specialized version for N=16
       fn process_16(data: [u8; 16]) -> int:
           # ... with N replaced by 16 ...

Teaching Benefits
~~~~~~~~~~~~~~~~~

quarry expand transforms abstract concepts into concrete code:

  â€¢ **Parameter closures:** "Here's exactly how it inlines"
  â€¢ **Zero-cost abstractions:** "See? No allocation, no indirection"
  â€¢ **Syntactic sugar:** "with desugars to try + defer"
  â€¢ **Compile-time params:** "Each [N] creates a specialized version"

Integration with Learning
~~~~~~~~~~~~~~~~~~~~~~~~~

Compiler errors suggest using expand:

    error[P0801]: cannot store parameter closure
      ...
      = help: Parameter closures are compile-time only (inlined)
      = explain: Run 'quarry expand --help closure-types' to see the difference
      = visual: Run 'quarry expand src/main.pyr --function=process' to see expansion

quarry learn exercises include expansion:

    Exercise: Understand Parameter Closures
    
    Run: quarry expand examples/vectorize.pyr
    
    See how the fn[i: int] closure is inlined into the SIMD loop.
    Notice: No function call, no allocation, just direct code insertion.

This makes abstract transformations concrete, accelerating learning.

Implementation: Beta Release (after parameter closures and sugar constructs are stable)

8.11 Documentation Generation
--------------------------------------------------------------------------------

Auto-Generated Docs
~~~~~~~~~~~~~~~~~~~

quarry doc generates HTML documentation from code and doc comments:

    """
    Parses a JSON string into a structured value.
    
    # Arguments
    * `input` - The JSON string to parse
    
    # Returns
    * `Ok(Value)` - Parsed JSON value
    * `Err(ParseError)` - Parse failure with error details
    
    # Examples
    ```pyrite
    let json = parse_json('{"key": "value"}')
    match json:
        Ok(val):
            print(val)
        Err(e):
            print("Parse error:", e)
    ```
    """
    fn parse_json(input: String) -> Result[Value, ParseError]:
        # Implementation

Generated docs include:
  â€¢ Public API reference
  â€¢ Example code (tested as part of doc tests)
  â€¢ Cross-links between related items
  â€¢ Search functionality
  â€¢ Source code links

8.12 Cross-Compilation
--------------------------------------------------------------------------------

First-Class Cross-Compilation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Quarry makes cross-compilation trivial:

    quarry build --target=aarch64-linux      # ARM64 Linux
    quarry build --target=x86_64-windows     # Windows x64
    quarry build --target=wasm32             # WebAssembly
    quarry build --target=arm-none-eabi      # Bare metal ARM

No separate toolchain setup required. Quarry downloads target-specific 
components automatically.

List available targets:

    quarry target list

Add target support:

    quarry target add riscv64-linux

Zero-Allocation Mode
~~~~~~~~~~~~~~~~~~~~

For embedded systems and safety-critical applications that require provable 
no-heap behavior, Quarry provides a zero-allocation build mode:

    quarry build --no-alloc
    
    # Or configure in Quarry.toml:
    [profile.embedded]
    no-alloc = true

In this mode, the compiler errors on any heap allocation:

    error[P0601]: heap allocation in no-alloc mode
      ----> src/main.pyr:42:13
       |
    42 |     let v = List[int].new()  # <---- allocates on heap
        |             ^^^^^^^^^^^^^^
        |
        = note: Build configured with --no-alloc
        = help: Use fixed-size array: [int; N]
        = help: Or pass pre-allocated buffer as parameter

What is forbidden:
  â€¢ List, Map, Set, String creation (heap containers)
  â€¢ Box[T], Rc[T], Arc[T] (heap allocation)
  â€¢ Dynamic trait objects (Box<dyn Trait>)
  â€¢ Any function marked with @may_alloc attribute

What is allowed:
  â€¢ Stack arrays: [int; 100]
  â€¢ Structs and enums (stack allocated)
  â€¢ References and slices
  â€¢ Static data and constants
  â€¢ Functions marked @noalloc

Example: Embedded firmware

    # Cargo.toml configuration
    [profile.embedded]
    no-alloc = true
    panic = "abort"
    opt-level = "z"  # Size optimization
    
    # main.pyrite
    @noalloc
    fn process_sensor_data(readings: &[u16; 32]) -> u32:
        var sum: u32 = 0
        for reading in readings:
            sum += reading as u32
        return sum / 32
    
    fn main():
        const BUFFER: [u16; 32] = [0; 32]
        loop:
            read_sensors(&mut BUFFER)
            let average = process_sensor_data(&BUFFER)
            display(average)

Benefits:
  â€¢ **Provable no-heap:** Compiler guarantees
  â€¢ **Safety certification:** Required for aerospace, medical devices
  â€¢ **Predictable memory:** No allocator = no allocation failures
  â€¢ **Minimal footprint:** No allocator code in binary

Integration with stdlib:
  â€¢ core:: namespace works in no-alloc mode
  â€¢ std:: namespace requires allocator
  â€¢ Clear documentation: "@requires alloc" on API docs

Use cases:
  â€¢ Microcontroller firmware (no MMU, limited RAM)
  â€¢ Real-time systems (deterministic memory)
  â€¢ Safety-critical software (aerospace, medical)
  â€¢ Bootloaders and kernels

This makes Pyrite credible for the most constrained embedded environments. 
Beta Release feature.

8.13 Cost Analysis and Performance Profiling
--------------------------------------------------------------------------------

Static Cost Analysis: quarry cost
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Beyond inline warnings, Pyrite provides structured cost analysis for IDE 
integration, CI/CD gates, and performance tracking.

Usage:

    quarry cost                     # Analyze entire project
    quarry cost --json              # Machine-readable output
    quarry cost --baseline=v1.json  # Compare against baseline
    quarry cost --threshold=warn    # Only show significant costs

Multi-Level Cost Reporting
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The quarry cost command supports progressive detail levels that match developer 
experience, teaching performance intuition gradually:

    quarry cost --level=beginner       # Gentle introduction
    quarry cost --level=intermediate   # Standard detail (default)
    quarry cost --level=advanced       # Comprehensive analysis

This mirrors the existing quarry lint --level pattern, creating a consistent 
mental model across tooling.

Beginner Level (--level=beginner)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Shows only high-level, actionable insights without overwhelming detail:

    Performance Analysis (Beginner)
    ================================
    
    ðŸ”¹ Your code allocates memory 12 times
       Most significant:
         â€¢ Line 234: Creates a list in a loop (runs 1000 times)
           â†’ Consider creating the list once before the loop
         â€¢ Line 156: Map grows dynamically
           â†’ Tip: Maps start small and grow as needed
    
    ðŸ”¹ Your code copies large data 3 times
         â€¢ Line 567: ImageBuffer (4 KB)
           â†’ Consider using a reference: &ImageBuffer
    
    ðŸ’¡ Learn more: run 'quarry cost --level=intermediate' for details

Beginner output characteristics:
  â€¢ Plain language (avoids jargon like "heap allocation", "amortized O(1)")
  â€¢ Only shows operations that matter (>1KB allocations, copies in loops)
  â€¢ Focuses on "what to do" not "why it happens"
  â€¢ Limits output to top 3-5 issues to avoid overwhelm

Intermediate Level (--level=intermediate, default)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Standard developer view with counts, sizes, and loop multiplication:

    Performance Analysis
    ====================
    
    Allocations: 12 sites (estimated 47 KB)
      
      ðŸ”´ Hot path (in loop):
        Line 234: List[Token].new()
          â€¢ Loop iterations: 1000
          â€¢ Per-call cost: 24 bytes (initial capacity)
          â€¢ Total cost: ~24 KB (may trigger 3-4 reallocations)
          â€¢ Suggestion: List[Token].with_capacity(100)
      
      ðŸŸ¡ Moderate:
        Line 156: Map[String, Config].insert()
          â€¢ Map capacity growth: 3 reallocations likely
          â€¢ Suggestion: with_capacity(estimated_entries)
    
    Copies: 3 sites (12 KB)
      Line 567: ImageBuffer (4096 bytes)
        â€¢ Passed by value to process_image()
        â€¢ Suggestion: Pass by reference (&ImageBuffer)
    
    ðŸ’¡ Run 'quarry cost --level=advanced' for dispatch and syscall analysis

Intermediate output characteristics:
  â€¢ Shows counts and sizes
  â€¢ Multiplies by loop iteration counts
  â€¢ Groups by severity (hot paths vs moderate)
  â€¢ Includes concrete suggestions with code examples

Advanced Level (--level=advanced)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Comprehensive analysis for performance-critical code:

    Performance Analysis (Advanced)
    ================================
    
    Allocations: 12 sites (47 KB estimated, 125 KB worst-case)
      
      ðŸ”´ Critical (hot path):
        src/parser.py:234:9 - List[Token].new()
          Function: parse_tokens() [called from 3 sites]
          Loop context: 0..1000 (bounded)
          Initial capacity: 24 bytes (6 elements)
          Growth pattern: 2x reallocation (typical usage: 15 elements)
          Expected reallocations: 2 (24â†’48â†’96 bytes)
          Total allocation cost: ~24 KB + 2 reallocations
          Suggestion: with_capacity(20) eliminates reallocations
          Allocator: DefaultHeap
      
      ðŸŸ¡ Moderate:
        src/cache.py:156:13 - Map[String, Config].insert()
          Growth: 0â†’16â†’32â†’64 entries (3 reallocations observed)
          Total cost: ~12 KB
          Frequency: Once per application startup
          Suggestion: with_capacity(50) if typical load known
    
    Copies: 3 sites (12 KB)
      src/renderer.py:567:14 - ImageBuffer
        Size: 4096 bytes
        Call chain: main() â†’ render_frame() â†’ process_image()
        Copy occurs: process_image(img: ImageBuffer)
        Suggestion: process_image(img: &ImageBuffer)
        Impact: 4KB memcpy per frame (60 FPS = 240 KB/sec)
    
    Dynamic Dispatch: 5 sites
      src/plugins.py:89:5 - Plugin::execute()
        Trait object: &dyn Plugin
        Virtual call overhead: ~2-3 cycles (indirect branch)
        Alternatives: enum dispatch, monomorphization
      
      src/events.py:234:10 - EventHandler::handle()
        Trait object: Box<dyn EventHandler>
        Pointer indirection: vtable lookup
        Context: Event loop (called frequently)
        Note: Acceptable cost for plugin architecture
    
    Syscalls: 23 sites
      File operations: 15 sites
        src/config.py:45 - File::open("/etc/app.conf")
        src/logger.py:89 - File::write() [buffered, ~10 syscalls/sec]
      
      Network operations: 8 sites
        src/api.py:156 - TcpStream::connect()
        src/api.py:234 - stream.read() [blocking I/O]
    
    Locking: 4 sites
      src/cache.py:67 - Mutex::lock()
        Hold time: <1Î¼s typical
        Contention: Low (single writer pattern)

Advanced output characteristics:
  â€¢ Full call chains and context
  â€¢ Detailed memory growth patterns
  â€¢ Allocator identification
  â€¢ Dynamic dispatch with vtable overhead estimates
  â€¢ Syscall breakdown by category
  â€¢ Lock contention analysis
  â€¢ Performance implications (e.g., "60 FPS = 240 KB/sec")

Level Selection Strategy
~~~~~~~~~~~~~~~~~~~~~~~~~

The tool automatically suggests level progression:
  â€¢ Beginner â†’ Intermediate: After user has addressed top issues
  â€¢ Intermediate â†’ Advanced: When optimizing hot paths or performance-critical code

IDE integration can show different levels in different contexts:
  â€¢ Inline hints: Beginner-level messages
  â€¢ Hover tooltips: Intermediate-level detail
  â€¢ "Show full analysis": Advanced-level deep dive

JSON Output Format:

    {
      "allocations": [
        {
          "location": "src/parser.py:234:9",
          "function": "parse_tokens",
          "type": "heap_allocation",
          "estimated_bytes": 1024,
          "frequency": "per_call",
          "suggestions": [
            "Pre-allocate with List.with_capacity(estimated_size)",
            "Reuse buffer across calls"
          ]
        }
      ],
      "copies": [
        {
          "location": "src/renderer.py:567:14",
          "bytes": 4096,
          "type": "ImageBuffer",
          "suggestion": "Pass by reference: &ImageBuffer"
        }
      ],
      "dynamic_dispatch": [
        {
          "location": "src/plugins.py:89:5",
          "trait": "Plugin",
          "method": "execute",
          "note": "Indirect call via vtable"
        }
      ],
      "syscalls": [
        {
          "location": "src/main.py:45:10",
          "operation": "file_open",
          "path": "/etc/config"
        }
      ],
      "summary": {
        "total_allocations": 47,
        "total_bytes_allocated": 125440,
        "total_copies": 12,
        "total_bytes_copied": 98304,
        "dynamic_dispatch_sites": 8,
        "syscall_sites": 23
      }
    }

IDE Integration:

IDEs can consume the JSON output to show inline cost hints:

    let tokens = List[Token].new()  ðŸ’° 1KB heap allocation
                                     âš¡ Hint: with_capacity(100)

CI/CD Integration:

Enforce performance budgets in continuous integration:

    # .github/workflows/ci.yml
    - name: Check performance budget
      run: |
        quarry cost --json > current.json
        quarry cost --baseline=baseline.json --threshold=error
        # Fails if allocations increased by >10%

Baseline tracking:

    quarry cost --save=baseline.json      # Save current as baseline
    quarry cost --compare=baseline.json   # Show differences

Example output:

    Performance Analysis
    ====================
    
    Allocations: 47 sites (125 KB estimated)
      â†‘ +3 sites since baseline
      â†‘ +12 KB since baseline
    
    Largest allocations:
      1. src/parser.py:234    8 KB  (in loop, 1000 iterations = 8 MB)
      2. src/cache.py:156     4 KB  (Map growth)
      3. src/buffer.py:89     2 KB  (String concatenation)
    
    Copies: 12 sites (96 KB total)
      â†’ src/renderer.py:567   4 KB  ImageBuffer
      â†’ src/config.py:123     8 KB  ConfigData
    
    Dynamic dispatch: 8 sites
      â†’ All in plugin system (expected)
    
    Syscalls: 23 sites
      â†’ 15 file operations
      â†’ 8 network operations

Use Cases:

1. **Performance debugging:** Find unexpected allocations
2. **CI gates:** Prevent performance regressions
3. **Optimization tracking:** Measure improvement over time
4. **IDE hints:** Show costs inline while coding
5. **Code review:** Highlight performance-sensitive changes

This completes Pyrite's cost-transparency story: from inline warnings (for 
developers) to structured reports (for tools and CI systems).

Runtime Performance Profiling
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Static analysis (quarry cost) shows what *could* be expensive. Runtime profiling 
shows what *is* expensive in actual execution. Quarry provides integrated 
profiling commands that complement static cost analysis:

quarry perf - Flamegraph Profiling
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Integrated CPU profiling with flamegraph visualization:

    quarry perf                         # Profile with default settings
    quarry perf --release               # Profile optimized build
    quarry perf --output=profile.svg    # Export flamegraph
    quarry perf --duration=30s          # Profile for 30 seconds

Under the hood:
  â€¢ Linux: Wraps perf with optimal sampling settings
  â€¢ macOS: Wraps Instruments DTrace profiler
  â€¢ Windows: Wraps ETW (Event Tracing for Windows)
  â€¢ Generates interactive flamegraph SVG automatically

Output shows:
  â€¢ CPU time spent in each function
  â€¢ Call stack context (who called what)
  â€¢ Hot paths highlighted
  â€¢ Standard flamegraph format (widely recognized)

Example workflow:

    $ quarry perf --release
    Profiling myapp (optimized build)...
    Captured 45,234 samples over 10.2 seconds
    
    Top functions by CPU time:
      34.2%  process_data
      18.5%  parse_input
      12.3%  allocate_buffers
    
    Flamegraph saved to: target/profile.svg
    Open in browser to explore call stacks

quarry alloc - Allocation Profiling
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Track heap allocations with call stack context:

    quarry alloc                        # Profile allocations
    quarry alloc --threshold=1KB        # Only show allocations > 1KB
    quarry alloc --json                 # Machine-readable output

Shows:
  â€¢ Every heap allocation with size
  â€¢ Call stack that triggered allocation
  â€¢ Allocation hot spots (frequency Ã— size)
  â€¢ Reallocation events (growth patterns)

Example output:

    Allocation Profile
    ==================
    
    Total allocations: 1,247 (2.4 MB)
    
    Hot spots (by total bytes):
      1. src/parser.py:234 - List[Token].new()
         â€¢ 1,000 allocations (24 KB each = 24 MB total)
         â€¢ Call chain: parse() â†’ tokenize() â†’ List::new()
         â€¢ Suggestion: Pre-allocate with with_capacity(1000)
      
      2. src/cache.py:156 - Map[String, Config].insert()
         â€¢ 47 allocations (growth: 16â†’32â†’64â†’128 entries)
         â€¢ Total: 12 KB
         â€¢ Suggestion: with_capacity(128) eliminates reallocations
    
    Allocation timeline:
      0-1s:    234 allocations (456 KB)
      1-2s:    189 allocations (378 KB)
      2-3s:    824 allocations (1.6 MB) â† spike here

Integration with quarry cost:
  â€¢ Static analysis predicts allocations
  â€¢ Runtime profiling confirms actual behavior
  â€¢ Cross-reference: "quarry cost predicted 12 sites, profiler found 11"

quarry pgo - Profile-Guided Optimization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

One-command PGO pipeline automates the 3-step process:

    quarry pgo                          # Build â†’ train â†’ rebuild
    quarry pgo --workload=bench         # Use benchmarks as training
    quarry pgo --workload="./train.sh"  # Custom training script

Manual PGO Workflow (Full Control)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For custom training workloads or complex scenarios, use the explicit 3-step workflow:

    # Step 1: Build instrumented binary
    $ quarry pgo generate
    Building instrumented binary for PGO training...
    âœ“ Built: target/pgo-instrument/myapp
    âœ“ Profile output: target/pgo-data/default.profraw
    
    Instrumentation:
      â€¢ Functions: 1,247 instrumented
      â€¢ Basic blocks: 12,456 tracked
      â€¢ Binary size: 3.2 MB (1.5x larger than release)
    
    Next: Run your training workload with this binary
          ./target/pgo-instrument/myapp [your args]
    
    # Step 2: Run training workload (your application-specific usage)
    $ ./target/pgo-instrument/myapp --benchmark
    # Or: ./target/pgo-instrument/myapp < typical_input.txt
    # Or: python run_training_suite.py
    # (Profile data automatically written to target/pgo-data/)
    
    # Step 3: Build optimized binary with profile data
    $ quarry pgo optimize
    Building with profile-guided optimization...
    âœ“ Analyzed profile data: target/pgo-data/default.profraw
    âœ“ Built: target/release/myapp
    
    PGO Optimizations Applied:
      â€¢ Functions inlined: 89 (based on hot paths)
      â€¢ Functions outlined: 45 (based on cold paths)
      â€¢ Branch weights: 1,234 basic blocks reordered
      â€¢ Code layout: Hot code grouped for cache locality
    
    Performance improvement:
      â€¢ Estimated: 15-30% faster on training workload
      â€¢ Binary size: 2.0 MB (cold code moved out-of-line)
    
    Next: Benchmark the optimized binary
          quarry bench --compare-to=baseline

Multiple Training Runs:

    # Collect profile data from multiple workloads
    $ quarry pgo generate
    
    $ ./target/pgo-instrument/myapp --workload=web_server
    # Profile data: target/pgo-data/run1.profraw
    
    $ ./target/pgo-instrument/myapp --workload=batch_processing
    # Profile data: target/pgo-data/run2.profraw
    
    $ ./target/pgo-instrument/myapp --workload=interactive
    # Profile data: target/pgo-data/run3.profraw
    
    # Merge all profiles and optimize
    $ quarry pgo optimize --merge-all
    Merging 3 profile datasets...
    âœ“ Combined profile represents all workloads
    Building optimized binary...

Clean up profile data:

    quarry pgo clean                       # Remove profile data
    quarry pgo reset                       # Reset and start over

Automated PGO Workflow (One Command)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For simple cases with standard workloads:

    quarry pgo                          # Build â†’ train â†’ rebuild
    quarry pgo --workload=bench         # Use benchmarks as training
    quarry pgo --workload="./train.sh"  # Custom training script

Automated workflow:
  1. quarry pgo
     â†’ Builds instrumented binary
     â†’ Runs typical workload (or prompts for one)
     â†’ Rebuilds optimized binary
     â†’ Reports performance improvement

Example:

    $ quarry pgo --workload=bench
    
    Step 1/3: Building instrumented binary...
    âœ“ Built target/pgo-instrument/myapp
    
    Step 2/3: Running training workload (benchmarks)...
    âœ“ Collected profile data from 5 benchmarks
    
    Step 3/3: Rebuilding with profile-guided optimization...
    âœ“ Built target/release/myapp
    
    Performance improvement:
      â€¢ 15% faster on parse_large_file benchmark
      â€¢ 8% faster on compute_heavy benchmark
      â€¢ Binary size: 2.1 MB â†’ 2.0 MB (branch pruning)

Why PGO matters:
  â€¢ Optimizes for actual code paths (not theoretical ones)
  â€¢ Better inlining decisions (inline hot paths, not cold ones)
  â€¢ Better branch prediction hints
  â€¢ Can yield 10-30% performance improvement

Quarry automates the tedious parts while giving control when needed.

LTO and Combined Optimization Workflows
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Link-Time Optimization (LTO) enables cross-translation-unit optimization by 
deferring code generation to link time. Quarry provides first-class LTO support 
with simple flags:

    quarry build --release --lto        # Full LTO (maximum optimization)
    quarry build --release --lto=thin   # Thin LTO (faster builds, good optimization)

LTO modes:
  â€¢ --lto or --lto=full - Full link-time optimization
    â€¢ Optimizes across all translation units
    â€¢ Slowest build times (minutes for large projects)
    â€¢ Best optimization (5-15% improvement typical)
    â€¢ Use for: Final release builds, benchmarking
  
  â€¢ --lto=thin - Thin LTO (recommended default for --release)
    â€¢ Fast incremental builds with cross-module optimization
    â€¢ 80% of full LTO benefit at 20% of compile time cost
    â€¢ Enables parallel optimization
    â€¢ Use for: Iterative optimization, CI builds
  
  â€¢ No LTO - Fastest builds, per-module optimization only
    â€¢ Use for: Debug builds, development iteration

Combined workflow for maximum performance:

    quarry build --release --lto=thin --pgo=./workload.sh
    
    # Combines:
    # 1. Release optimizations (--O3 equivalent)
    # 2. Thin LTO (cross-module inlining and optimization)
    # 3. Profile-guided optimization (optimize for actual usage)
    
    Typical results:
      â€¢ Release alone: 10x faster than debug
      â€¢ + Thin LTO: 15-25% additional improvement
      â€¢ + PGO: 10-30% additional improvement
      â€¢ Combined: 15-50% faster than plain release

One-command peak performance:

    quarry build --peak
    
    # Equivalent to: --release --lto --pgo=bench
    # Automatically:
    #   1. Builds with thin LTO + instrumentation
    #   2. Runs benchmark suite as PGO training
    #   3. Rebuilds with full LTO + PGO data
    #   4. Reports achieved optimization levels
    
    Output:
      Peak Performance Build
      ======================
      
      Step 1/3: Building instrumented binary (thin LTO)...
      âœ“ Built in 2m 34s
      
      Step 2/3: Training with benchmark workload...
      âœ“ Collected profile from 12 benchmarks (45s)
      
      Step 3/3: Final build (full LTO + PGO)...
      âœ“ Built in 8m 12s
      
      Optimizations applied:
        â€¢ Release optimizations: baseline
        â€¢ Thin LTO (training): +18% performance
        â€¢ Full LTO (final): +7% performance
        â€¢ PGO: +23% performance
        â€¢ Total improvement: +55% vs release
      
      Binary: target/peak/myapp (2.3 MB)
      Recommended for: Production deployment

Build profiles can configure defaults:

    # Quarry.toml
    [profile.release]
    opt-level = 3
    lto = "thin"        # Enable thin LTO for release builds
    pgo = false         # Manual PGO only
    
    [profile.peak]
    inherits = "release"
    lto = "full"        # Full LTO for peak builds
    pgo = true          # Always run PGO
    pgo-workload = "bench"

Comparison table:

    | Build Mode         | Time | Binary Size | Performance | Use Case               |
    |--------------------|------|-------------|-------------|------------------------|
    | Debug              | 10s  | 5 MB        | 1x          | Development            |
    | --release          | 30s  | 2 MB        | 10x         | Testing                |
    | --release --lto    | 2m   | 1.8 MB      | 12x         | Distribution           |
    | --release --pgo    | 5m   | 2 MB        | 13x         | Optimized for workload |
    | --peak             | 10m  | 1.8 MB      | 15x         | Production max perf    |

When to use each:
  â€¢ Debug: Iteration, debugging
  â€¢ Release: QA, most users
  â€¢ Release + thin LTO: Distribution default (good balance)
  â€¢ Peak: Mission-critical services, performance benchmarking

Cost transparency:

    quarry cost shows optimization decisions:
      
      Build: --release --lto --pgo
      
      Optimizations applied:
        â€¢ 1,234 functions inlined (cross-module via LTO)
        â€¢ 89 cold paths moved out-of-line (via PGO)
        â€¢ 234 branches reordered (via PGO branch weights)
        â€¢ 45 functions specialized (hot path variants)
      
      Estimated improvements:
        â€¢ LTO: +18% (reduced call overhead)
        â€¢ PGO: +23% (optimized for actual workload)

Why this matters:

LTO and PGO are proven optimizations that yield 20-50% improvements in real-world 
applications, but are often tedious to configure. Quarry makes them one-command 
operations, and --peak mode makes "absolute best performance" a single flag.

Beta Release feature (LTO support). Beta Release enhancement (--peak command).

quarry tune - Intelligent Optimization Suggestions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Correlates static analysis (quarry cost) with runtime profiling (quarry perf, 
quarry alloc) to suggest specific, high-impact optimizations:

    quarry tune                         # Analyze and suggest
    quarry tune --apply                 # Apply safe suggestions automatically
    quarry tune --interactive           # Review each suggestion

Example output:

    Performance Tuning Suggestions
    ===============================
    
    Based on static analysis and runtime profiling:
    
    ðŸ”´ HIGH IMPACT (3 suggestions)
    
    1. Pre-allocate List in hot loop
       Location: src/parser.py:234
       
       Current code:
         for line in lines:              # 1000 iterations
             let tokens = List[Token].new()
       
       Issue:
         â€¢ Static analysis: Allocates in loop (1000 times)
         â€¢ Runtime profile: 24 MB allocated, 3-4 reallocations per list
         â€¢ CPU time: 12% of total runtime
       
       Suggested fix:
         let tokens = List[Token].with_capacity(20)
         for line in lines:
             tokens.clear()
             # ... use tokens ...
       
       Expected improvement:
         â€¢ Eliminates 1000 allocations
         â€¢ Removes all reallocations
         â€¢ Estimated speedup: 15-20%
       
       [Apply] [Skip] [Explain]
    
    2. Pass ImageBuffer by reference
       Location: src/renderer.py:567
       
       Current code:
         fn process_image(img: ImageBuffer):  # Takes ownership, copies 4 KB
       
       Issue:
         â€¢ Static analysis: 4 KB copy on every call
         â€¢ Runtime profile: Called 60 times/second (240 KB/sec copied)
         â€¢ CPU time: 8% of total runtime
       
       Suggested fix:
         fn process_image(img: &ImageBuffer):  # Borrows, no copy
       
       Expected improvement:
         â€¢ Eliminates 4 KB copy per frame
         â€¢ Estimated speedup: 8-10%
       
       [Apply] [Skip] [Explain]
    
    ðŸŸ¡ MEDIUM IMPACT (5 suggestions)
    
    3. Use SmallVec for small arrays
       Location: src/ast.py:89
       [Details...]
    
    [Show all] [Apply all safe fixes] [Cancel]

How quarry tune works:
  1. Runs static cost analysis (quarry cost)
  2. Runs runtime profiling (quarry perf + quarry alloc)
  3. Correlates findings: "This allocation was predicted and IS a hot spot"
  4. Ranks by impact: CPU time Ã— memory Ã— frequency
  5. Suggests specific, actionable fixes
  6. Can apply safe transformations automatically

Why this is transformative:
  â€¢ Beginners: "Just run quarry tune" â†’ get specific guidance
  â€¢ Experts: Saves manual correlation work
  â€¢ Actionable: Not generic advice, specific line numbers + code changes
  â€¢ Measurable: Estimates improvement (based on profiling data)

This completes the performance tooling story:
  â€¢ quarry cost: Static "what could be expensive"
  â€¢ quarry perf: Runtime "what is expensive (CPU)"
  â€¢ quarry alloc: Runtime "what is expensive (memory)"
  â€¢ quarry pgo: Optimize for actual workloads
  â€¢ quarry tune: Correlate all data, suggest fixes

Performance Lockfile: Enforced "Fast Forever"
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The performance lockfile workflow transforms Pyrite's cost transparency from 
"measure performance" to "enforce performance stays constant." This is the 
missing piece that turns "Pyrite is fast" into "Pyrite stays fast forever."

Command Usage
~~~~~~~~~~~~~

    quarry perf --baseline              # Write Perf.lock
    quarry perf --check                 # Fail CI if regressed
    quarry perf --check --threshold=5%  # Custom regression tolerance
    quarry perf --diff                  # Show differences vs baseline

Performance Lockfile Creation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Generate a baseline snapshot of performance characteristics:

    $ quarry perf --baseline
    
    Profiling optimized build...
    Running benchmark suite (12 benchmarks)...
    
    Performance Baseline Generated
    ===============================
    
    Hot Functions (saved to Perf.lock):
      â€¢ process_data: 1247ms (34.2% total time)
        - SIMD width: 8 (AVX2)
        - Inlined: 23 callsites
        - Allocation sites: 3
      
      â€¢ parse_input: 674ms (18.5% total time)
        - Call count: 10,247
        - Allocation sites: 12
        - Stack usage: 4 KB
      
      â€¢ allocate_buffers: 448ms (12.3% total time)
        - Heap allocations: 47 sites
        - Total allocated: 125 KB
    
    Optimization Baseline:
      â€¢ Total allocations: 47 sites (125 KB)
      â€¢ SIMD width used: 8 (AVX2)
      â€¢ Inlining decisions: 1,234 functions
      â€¢ Code generation: optimized
    
    Baseline saved to: Perf.lock
    Commit this file to track performance over time

Perf.lock Format
~~~~~~~~~~~~~~~~

The lockfile stores concrete, measurable performance metrics:

    # Perf.lock (YAML format for readability)
    version: "1.0"
    generated: "2025-12-18T10:30:00Z"
    build: "--release --lto=thin"
    platform: "x86_64-linux"
    
    hot_functions:
      - name: "process_data"
        time_ms: 1247
        time_percent: 34.2
        simd_width: 8
        inlined_calls: 23
        alloc_sites: 3
      
      - name: "parse_input"
        time_ms: 674
        time_percent: 18.5
        call_count: 10247
        alloc_sites: 12
        stack_bytes: 4096
    
    allocations:
      total_sites: 47
      total_bytes: 125440
      hot_spots:
        - location: "src/parser.py:234"
          count: 1000
          bytes_per: 24
    
    optimizations:
      functions_inlined: 1234
      simd_vectorized: 67
      loops_unrolled: 23

CI Integration and Regression Detection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In continuous integration, enforce performance constraints:

    $ quarry perf --check
    
    Checking performance against baseline (Perf.lock)...
    
    âœ— PERFORMANCE REGRESSION DETECTED
    
    Regressions (threshold: 5%):
    
      ðŸ”´ CRITICAL: process_data regressed by 18%
         Baseline: 1247ms â†’ Current: 1471ms (+224ms)
         
         Analysis:
           â€¢ SIMD width changed: 8 (AVX2) â†’ 4 (SSE2)
           â€¢ Reason: Alignment check now fails
           â€¢ Affected code: src/process.py:156-189
         
         Run 'quarry perf --explain process_data' for details
      
      ðŸŸ¡ WARNING: parse_input regressed by 6.2%
         Baseline: 674ms â†’ Current: 716ms (+42ms)
         
         Analysis:
           â€¢ New allocation added: src/parser.py:241
           â€¢ Inlining stopped: tokenize() no longer inlined
           â€¢ Reason: Function grew beyond inlining threshold
         
         Run 'quarry perf --explain parse_input' for details
    
    Summary:
      â€¢ 2 functions regressed
      â€¢ 0 functions improved
      â€¢ Overall regression: 11.3% slower
    
    CI FAILURE: Performance regressions exceed threshold
    Exit code: 1

This output is actionable: developers know exactly which function regressed, 
by how much, and WHY (SIMD width changed, inlining stopped, allocation added).

Explaining Regressions
~~~~~~~~~~~~~~~~~~~~~~~

Deep-dive into why performance changed:

    $ quarry perf --explain process_data
    
    Performance Regression Analysis: process_data
    ==============================================
    
    Baseline: 1247ms (34.2% of runtime)
    Current:  1471ms (38.9% of runtime)
    Change:   +224ms (+18% slower) âš ï¸
    
    Root Causes:
    
    1. SIMD Width Reduced (PRIMARY CAUSE)
       
       Baseline: 8-wide SIMD (AVX2)
         â€¢ Vectorized loop at line 156
         â€¢ Processes 8 f32 per iteration
         â€¢ Generated: vfmadd231ps (AVX2 FMA instruction)
       
       Current: 4-wide SIMD (SSE2)
         â€¢ Same loop now uses SSE2
         â€¢ Processes 4 f32 per iteration
         â€¢ Generated: mulps + addps (SSE2 instructions)
       
       Why the change?
         â€¢ Buffer alignment changed from 32-byte to 16-byte
         â€¢ Change introduced in: commit abc123f
         â€¢ File: src/buffers.py:89
         â€¢ Old: #[align(32)] â†’ New: #[align(16)]
       
       Fix:
         Restore 32-byte alignment for AVX2:
           struct Buffer:
               #[align(32)]  # Required for 8-wide SIMD
               data: [f32; 1024]
    
    2. Inlining Stopped (SECONDARY CAUSE)
       
       Baseline: compute_kernel() inlined at 23 callsites
       Current:  compute_kernel() NOT inlined (function call overhead)
       
       Why?
         â€¢ Function body grew from 45 â†’ 89 instructions
         â€¢ Exceeded inlining threshold (80 instructions)
         â€¢ Change: Added debug logging in commit def456
       
       Fix:
         Remove debug logging in hot path, or use:
           @always_inline
           fn compute_kernel(...):

Assembly Diff (Optional)
~~~~~~~~~~~~~~~~~~~~~~~~

For expert debugging, show assembly differences:

    $ quarry perf --diff-asm process_data
    
    Assembly Diff: process_data (Baseline vs Current)
    ==================================================
    
    Baseline (AVX2, 8-wide SIMD):
      vmovups ymm0, [rdi + 0]      ; Load 8 f32
      vmovups ymm1, [rsi + 0]      ; Load 8 f32
      vfmadd231ps ymm0, ymm1, ymm2 ; FMA: ymm0 += ymm1 * ymm2
      vmovups [rdx + 0], ymm0      ; Store 8 f32
      add rdi, 32                  ; Advance pointer
      add rsi, 32
      add rdx, 32
    
    Current (SSE2, 4-wide SIMD):
      movups xmm0, [rdi + 0]       ; Load 4 f32
      movups xmm1, [rsi + 0]       ; Load 4 f32
      mulps xmm1, xmm2             ; Multiply
      addps xmm0, xmm1             ; Add
      movups [rdx + 0], xmm0       ; Store 4 f32
      add rdi, 16                  ; Advance pointer
      add rsi, 16
      add rdx, 16
    
    Analysis:
      â€¢ Twice as many loop iterations (8â†’4 elements per iter)
      â€¢ No FMA instruction (requires 2 ops instead of 1)
      â€¢ Estimated: 2x slower per element processed

IR Diff (Alternative)
~~~~~~~~~~~~~~~~~~~~~~

For compiler developers, show LLVM IR differences:

    $ quarry perf --diff-ir process_data
    
    Shows side-by-side LLVM IR with:
      â€¢ Vectorization decisions
      â€¢ Inlining decisions
      â€¢ Optimization passes applied
      â€¢ Differences highlighted

Why Performance Lockfile Matters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This workflow addresses the critical "performance decay" problem:

Without lockfile:
  â€¢ Performance regressions silent until users complain
  â€¢ Root cause requires manual profiling and assembly inspection
  â€¢ Team doesn't know WHICH commit caused slowdown
  â€¢ Optimization decisions lost to history

With lockfile:
  â€¢ CI fails immediately on regression
  â€¢ Exact function and root cause identified
  â€¢ Assembly/IR diff shows what changed
  â€¢ Actionable fix suggestions provided
  â€¢ Performance becomes a first-class requirement like tests

Real-World Impact:
  â€¢ Chromium: 20% of commits have measurable performance impact
  â€¢ Without gates: Performance decays 2-3% per month (death by 1000 cuts)
  â€¢ With gates: Performance stays constant or improves

This is how you turn "Pyrite is fast" (snapshot) into "Pyrite stays fast 
forever" (guarantee). It's the missing enforcement layer for cost transparency.

Implementation Strategy
~~~~~~~~~~~~~~~~~~~~~~~

Beta Release addition (after quarry cost and quarry perf are stable):

1. **Baseline generation:** Extend quarry perf to save structured snapshot
2. **CI checking:** Add --check mode that compares against Perf.lock
3. **Regression analysis:** Correlate with compiler optimization decisions
4. **Assembly diff:** Use objdump + diff for concrete evidence
5. **IR diff:** Optional LLVM IR comparison for compiler developers

Storage format:
  â€¢ Perf.lock committed to version control (like Cargo.lock)
  â€¢ Machine-readable (YAML or JSON) for tooling
  â€¢ Human-readable for code review

Threshold configuration:

    # Quarry.toml
    [perf]
    threshold = 5              # Fail CI if >5% slower
    threshold-critical = 10    # Hard failure threshold
    baseline = "Perf.lock"     # Default baseline file
    
    [perf.functions]
    "process_data" = 2         # Critical function: 2% tolerance
    "parse_input" = 10         # Less critical: 10% tolerance

This provides the highest-leverage addition to Pyrite's existing performance 
tooling.

8.14 Interactive Learning: quarry learn
--------------------------------------------------------------------------------

Pyrite includes a built-in interactive learning system inspired by Rustlings. 
The quarry learn command provides structured exercises that teach concepts 
through practice.

Command Usage
~~~~~~~~~~~~~

    quarry learn                   # Start interactive tutorial
    quarry learn ownership         # Jump to specific topic
    quarry learn --list            # Show all available topics
    quarry learn --reset           # Reset progress

Topics covered:
  â€¢ Basics: Variables, functions, control flow
  â€¢ Ownership: Moves, borrowing, lifetimes
  â€¢ Types: Structs, enums, pattern matching
  â€¢ Collections: List, Map, iteration
  â€¢ Error handling: Result types, try operator
  â€¢ Concurrency: Threads, channels, mutexes
  â€¢ Performance: SIMD, vectorization, profiling
  â€¢ Advanced: Generics, traits, metaprogramming

Interactive Exercise Format
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Each exercise presents broken code that the learner must fix:

    $ quarry learn ownership
    
    ============================================================
    Exercise: Ownership 3/12 - Borrowing Rules
    ============================================================
    
    Fix the code below to satisfy the borrow checker.
    The function should process data without taking ownership.
    
    File: exercises/ownership_03.pyrite
    
        fn main():
            let data = List[int]([1, 2, 3])
            process(data)
            print(data.length())  # Error: value was moved
        
        fn process(list: List[int]):
            print("Processing...")
    
    ============================================================
    
    Try to fix it! Then run:
      quarry learn check
    
    Hint: Use & to borrow instead of moving
    Stuck? Run: quarry learn hint

Workflow:
  1. Read exercise description
  2. Edit exercise file in your editor
  3. Run quarry learn check to verify solution
  4. Get instant feedback (pass/fail + explanation)
  5. Move to next exercise automatically

Example check output:

    $ quarry learn check
    
    âœ“ Code compiles successfully!
    âœ“ Ownership rules satisfied
    âœ“ Test cases pass
    
    Great job! You've learned:
      â€¢ How to borrow data with &T
      â€¢ When to use borrowing vs. moving
      â€¢ How to keep data usable after function calls
    
    Key concept:
      Borrowing (&) lets functions access data without taking ownership.
      The original owner retains the value and can use it after the call.
    
    Next exercise: ownership_04.pyrite (mutable borrowing)
    
    Progress: 3/12 ownership exercises complete
    Run 'quarry learn next' to continue

Hints System
~~~~~~~~~~~~

Progressive hints prevent frustration without spoiling solutions:

    $ quarry learn hint
    
    Hint 1/3:
      The problem is that 'process' takes ownership of 'list'.
      After the call, 'data' is no longer valid in 'main'.
    
    Press Enter for next hint, or Ctrl+C to try again...
    
    Hint 2/3:
      Change the function signature to borrow instead:
        fn process(list: &List[int])
    
    Press Enter for next hint...
    
    Hint 3/3 (SOLUTION):
      Change line 5 to:
        fn process(list: &List[int]):
      
      And line 3 to:
        process(&data)

Hints are spaced to encourage thinking before revealing solutions.

Progress Tracking
~~~~~~~~~~~~~~~~~

Learners can track their journey:

    $ quarry learn status
    
    Your Pyrite Learning Progress
    ==============================
    
    âœ“ Basics          [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 12/12 (100%)
    âœ“ Ownership       [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘]  8/12 (67%)
    â–‘ Types           [â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  2/10 (20%)
    â–‘ Collections     [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  0/8  (0%)
    â–‘ Error Handling  [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  0/6  (0%)
    
    Current: ownership_09.pyrite (Lifetime basics)
    
    Estimated time to complete Ownership: 1.5 hours
    
    Run 'quarry learn next' to continue

Progress saved locally (~/.cache/pyrite/learn/progress.json).

Topic Deep-Dives
~~~~~~~~~~~~~~~~

Some topics include mini-projects:

    $ quarry learn ownership --project
    
    ============================================================
    Ownership Final Project: Build a Simple Text Editor Buffer
    ============================================================
    
    Apply everything you've learned to build a text buffer that:
      â€¢ Stores lines of text efficiently
      â€¢ Allows insertion and deletion
      â€¢ Provides undo/redo functionality
      â€¢ Manages memory safely without leaks
    
    Starter code: exercises/projects/text_buffer/
    
    Requirements:
      âœ“ Pass all 15 test cases
      âœ“ No memory leaks (quarry test --leak-check)
      âœ“ No unsafe code blocks
      âœ“ Efficient (< 100ms for 10,000 operations)
    
    This project synthesizes:
      â€¢ Ownership and borrowing
      â€¢ Mutable references
      â€¢ Collections (Vec, String)
      â€¢ Error handling
    
    Good luck! Run 'quarry test' to check your implementation.

Projects provide synthesis opportunities and build confidence.

Integration with Error Messages
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The learning system connects to the compiler:

When you encounter error P0234 in real code:

    error[P0234]: cannot use moved value 'data'
      ...
      = learn: Run 'quarry learn ownership' for interactive practice
      = explain: Run 'pyritec --explain P0234' for detailed explanation

This creates a learning loop:
  1. Hit error in real code
  2. Compiler suggests relevant exercises
  3. Practice concept in isolation
  4. Return to real code with understanding

Why quarry learn Matters
~~~~~~~~~~~~~~~~~~~~~~~~~

Interactive learning addresses the "Rust is hard" perception:

  â€¢ **Structured path:** Clear progression from basics to advanced
  â€¢ **Hands-on practice:** Learn by fixing code, not reading theory
  â€¢ **Immediate feedback:** Know instantly if you got it right
  â€¢ **Progressive hints:** Never stuck, never spoiled
  â€¢ **Synthesis projects:** Apply concepts in realistic scenarios

Research shows interactive practice > passive reading for systems concepts. 
Rustlings (Rust's equivalent) is consistently praised as the best way to learn 
ownership.

Pyrite adopts this proven approach as a first-class feature, making the learning 
curve feel manageable instead of insurmountable.

Stable Release Feature
~~~~~~~~~~~~~~~

quarry learn is implemented in Stable Release (Developer Experience) after the core 
compiler and error messages are stable. It depends on:
  â€¢ Excellent error messages (to teach through failures)
  â€¢ quarry test framework (to verify solutions)
  â€¢ pyritec --explain system (for deep dives)

Once available, it becomes the recommended path for all newcomers: "Install 
Pyrite, run quarry learn, build real projects with confidence."

8.15 Integration and CI/CD
--------------------------------------------------------------------------------

CI-Friendly Commands
~~~~~~~~~~~~~~~~~~~~

Quarry provides non-interactive, CI-optimized commands:

    quarry build --locked          # Fail if Quarry.lock is outdated
    quarry test --no-fail-fast     # Run all tests, report all failures
    quarry fmt --check             # Verify formatting without changes
    quarry audit                   # Check for security vulnerabilities

Example GitHub Actions workflow:

    name: CI
    on: [push, pull_request]
    jobs:
      test:
        runs-on: ubuntu-latest
        steps:
          - uses: actions/checkout@v2
          # Aspirational: does not exist yet
          - uses: pyrite-lang/setup-quarry@v1
          - run: quarry fmt --check
          - run: quarry lint
          - run: quarry test
          - run: quarry build --release

8.16 Edition System and Stability
--------------------------------------------------------------------------------

Pyrite uses an Edition system to enable language evolution without breaking 
existing code. This is a critical trust signal for developers betting careers 
and projects on the language.

What are Editions?
~~~~~~~~~~~~~~~~~~

Editions are opt-in milestones that introduce backward-compatible changes to 
syntax, lints, and language defaults. Code written in one edition continues to 
work with newer compilers indefinitely.

Key principles:

  â€¢ **Backward compatibility:** Pyrite 2028 compiler compiles 2025 code
  â€¢ **Forward migration:** Automated tools upgrade code between editions
  â€¢ **Semantic versioning:** Editions released every 3 years (2025, 2028, 2031...)
  â€¢ **ABI stability:** Editions never break binary compatibility
  â€¢ **Interoperability:** Libraries from different editions work together

Declaring Edition
~~~~~~~~~~~~~~~~~

Projects declare their edition in Quarry.toml:

    [package]
    name = "myproject"
    version = "0.1.0"
    edition = "2025"          # Explicitly declares edition

Scripts without Quarry.toml use the latest stable edition at compilation time.

What Can Change Between Editions?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Editions can introduce:

1. **New syntax sugar:**
   - Edition 2025: `&T` for references
   - Edition 2028: Might add `?T` sugar for `Optional[T]`
   - Old code still uses `Optional[T]`, new code can choose

2. **New keywords:**
   - Edition 2028 might reserve `async` as keyword
   - 2025 code using `async` as identifier still compiles in 2025 mode

3. **Lint defaults:**
   - Edition 2028 might make certain warnings errors by default
   - 2025 code still uses 2025 lint levels

4. **Standard library additions:**
   - New modules available in all editions
   - Backward compatible only

What CANNOT Change:
  âœ— Core semantics (ownership rules remain stable)
  âœ— Type system fundamentals
  âœ— ABI/calling conventions
  âœ— Binary format
  âœ— Unsafe behavior definitions

Migration Between Editions
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Quarry provides automatic migration tooling:

    # Check if code is ready for new edition
    quarry edition check --target=2028
    
    # Preview migration changes
    quarry edition migrate --edition=2028 --dry-run
    
    # Apply migration
    quarry edition migrate --edition=2028
    
    # Fix any remaining issues interactively
    quarry fix --edition=2028

Migration is typically straightforward:
  â€¢ Rename identifiers that became keywords
  â€¢ Apply automatic syntax transformations
  â€¢ Update lint suppressions if needed

Example migration output:

    Migrating from Edition 2025 to Edition 2028
    ============================================
    
    Found 3 changes needed:
    
    1. src/main.pyrite:45
       Identifier 'async' is now a keyword
       â†’ Rename to 'async_operation'
    
    2. src/parser.pyrite:123
       Deprecated syntax: match x: case 1: ...
       â†’ New syntax: match x { 1 => ... }
    
    3. Lints now error by default:
       - unused_variables (was warning)
       - large_copy (was allow)
    
    Apply changes? [Y/n]

Mixed-Edition Projects
~~~~~~~~~~~~~~~~~~~~~~

Dependencies can use different editions:

    [package]
    edition = "2028"
    
    [dependencies]
    parser = "1.0"      # Uses edition 2025 internally
    json = "2.0"        # Uses edition 2028

The compiler bridges editions automatically. Binary interfaces remain 
compatible.

Edition Schedule
~~~~~~~~~~~~~~~~

Planned edition cadence:

  â€¢ **2025:** Launch edition (baseline)
  â€¢ **2028:** First evolution edition
  â€¢ **2031:** Second evolution edition
  â€¢ Continue every 3 years...

Each edition has:
  â€¢ 6-month beta period for community testing
  â€¢ Automated migration tools ready at launch
  â€¢ LTS support for at least 2 prior editions (6 years minimum)

Support Policy
~~~~~~~~~~~~~~

  â€¢ **Current edition:** Full support, all new features
  â€¢ **Previous edition:** Security fixes, critical bugs
  â€¢ **Two editions back:** Security fixes only
  â€¢ **Older editions:** Best-effort compatibility (compiler still accepts)

Example timeline for edition 2028:

    2028-01: Edition 2028 stable launch
    2031-01: Edition 2031 launch (2028 becomes "previous")
    2034-01: Edition 2034 launch (2028 becomes "two back")
    2037-01: Edition 2037 launch (2028 moves to best-effort)

Even after active support ends, the compiler continues to accept old editions.

Why Editions Matter
~~~~~~~~~~~~~~~~~~~

The edition system addresses the "stability vs evolution" dilemma:

  â€¢ **For established projects:** Guaranteed stability, no forced churn
  â€¢ **For new projects:** Access to latest improvements
  â€¢ **For the ecosystem:** Gradual, coordinated evolution
  â€¢ **For developers:** Confidence to invest in Pyrite

This is inspired by Rust's edition system, which successfully enabled language 
evolution (async/await, new keywords) without breaking millions of lines of 
existing code.

Trust Signal
~~~~~~~~~~~~

The edition system is a promise: "Your Pyrite code will compile in 10 years."

This commitment differentiates Pyrite from languages that break compatibility 
frequently (Python 2â†’3) or stagnate to avoid breaking changes. Pyrite evolves 
without breaking.

8.17 Supply-Chain Security and Trust
--------------------------------------------------------------------------------

For production systems and security-critical applications, supply-chain security 
is non-negotiable. Pyrite makes dependency trust and verification first-class 
features, not afterthoughts. This addresses the growing industry concern about 
software supply-chain attacks and dependency vulnerabilities.

Design Philosophy
~~~~~~~~~~~~~~~~~

Pyrite's approach to supply-chain security:
  â€¢ **Explicit verification:** Developers actively audit dependencies
  â€¢ **Reproducible builds:** Lockfiles and checksums prevent tampering
  â€¢ **Package signing:** Cryptographic verification of package authors
  â€¢ **Vulnerability scanning:** Automated detection of known CVEs
  â€¢ **Review manifests:** Organizations track trusted versions
  â€¢ **Minimal dependencies:** Batteries-included stdlib reduces attack surface

This makes Pyrite suitable for industries with strict security requirements 
(aerospace, medical devices, financial services, government).

quarry audit - Vulnerability Scanning
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Automated scanning for known security vulnerabilities in dependencies:

    quarry audit                           # Scan all dependencies
    quarry audit --json                    # Machine-readable output
    quarry audit --fix                     # Update to patched versions

Example output:

    $ quarry audit
    
    Security Audit Report
    =====================
    
    Scanning 47 dependencies...
    
    ðŸ”´ CRITICAL (2 vulnerabilities)
    
    1. CVE-2025-1234: Buffer overflow in json-parser 1.2.3
       Package: json-parser
       Installed: 1.2.3
       Fixed in: 1.2.4, 1.3.0
       Severity: CRITICAL (CVSS 9.8)
       
       Vulnerability:
         â€¢ Type: Heap buffer overflow
         â€¢ Component: JSON string parser
         â€¢ Exploitable: Remote code execution
         â€¢ Affected: parse_string() function
       
       Impact on your code:
         â€¢ Used in: src/api.py:156 (parse_json_request)
         â€¢ Exposure: User-supplied JSON input
         â€¢ Risk: High (external input, network-facing)
       
       Fix:
         Update to patched version:
           quarry update json-parser --version=1.2.4
         
         Or remove dependency if not critical:
           quarry remove json-parser
    
    ðŸŸ¡ WARNING (3 vulnerabilities)
    
    2. CVE-2024-5678: Denial of service in http-client 2.1.0
       [Details...]
    
    Summary:
      â€¢ 5 vulnerabilities found (2 critical, 3 warning)
      â€¢ 2 packages need updates
      â€¢ Run 'quarry audit --fix' to automatically update
    
    Last audit: 3 days ago
    Run 'quarry audit' regularly or enable in CI

CI Integration:

    # Fail CI if critical vulnerabilities found
    quarry audit --fail-on=critical
    
    # Fail on any vulnerability
    quarry audit --fail-on=any
    
    # Check and update automatically
    quarry audit --fix --ci

Audit database:
  â€¢ Quarry maintains central vulnerability database (aspirational: quarry.dev/advisories)
  â€¢ Updated continuously from multiple sources (CVE, NVD, security researchers)
  â€¢ Community-reported vulnerabilities via quarry report-vuln

Benefits:
  â€¢ Continuous monitoring of known vulnerabilities
  â€¢ Automated detection in CI (no manual tracking)
  â€¢ Immediate notification of critical issues
  â€¢ One-command remediation (quarry audit --fix)

quarry vet - Dependency Review and Trust
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For organizations with strict security requirements, quarry vet provides 
dependency review and approval workflows inspired by cargo-vet:

    quarry vet                             # Show unvetted dependencies
    quarry vet review json-parser          # Review specific package
    quarry vet certify json-parser 1.2.4   # Mark version as trusted
    quarry vet import-audits --from=org    # Import organization audits

How it works:

1. **Track trusted versions:** Maintain manifest of reviewed dependencies
2. **Block unvetted packages:** Fail build if dependency not reviewed
3. **Share trust:** Organizations publish review manifests
4. **Continuous verification:** CI enforces vet requirements

Example workflow for security-critical organization:

    $ quarry vet
    
    Dependency Review Status
    ========================
    
    Unvetted dependencies (4):
    
    ðŸ”´ json-parser 1.2.4
       â€¢ Added: 2 days ago
       â€¢ Used by: src/api.py
       â€¢ Risk: Parses external input (network-facing)
       â€¢ Lines of unsafe code: 234 (12% of crate)
       â€¢ Previous version vetted: 1.2.3
       
       Action required:
         1. Review changes: quarry vet diff json-parser 1.2.3 1.2.4
         2. Audit unsafe blocks: quarry vet show-unsafe json-parser
         3. Certify if safe: quarry vet certify json-parser 1.2.4
    
    ðŸŸ¡ http-client 3.0.1
       â€¢ Added: 1 week ago
       â€¢ Used by: src/fetch.py
       â€¢ Risk: Network operations
       â€¢ Community reviews: 47 organizations vetted this version
       
       Quick certify (trusted by community):
         quarry vet certify http-client 3.0.1 --trust-community

Review process:

    $ quarry vet review json-parser 1.2.4
    
    Reviewing: json-parser 1.2.4
    ============================
    
    Package information:
      â€¢ Authors: security-team@example.com (verified)
      â€¢ Downloads: 1.2M last month
      â€¢ Stars: 3,400
      â€¢ Open issues: 12 (0 security-related)
    
    Changes since 1.2.3 (last vetted):
      â€¢ 15 commits
      â€¢ +234 lines, -89 lines
      â€¢ 2 unsafe blocks modified
    
    [Show diff] [Show unsafe code] [Check CVEs]
    
    Audit questions:
      1. Does it handle untrusted input safely? [Y/n]
      2. Are unsafe blocks justified and correct? [Y/n]
      3. Are dependencies vetted? [Y/n]
      4. Is error handling robust? [Y/n]
    
    Certify this version? [y/N]

Certification levels:

    quarry vet certify pkg 1.0 --level=full
      # Full audit: All code reviewed, all unsafe blocks verified
    
    quarry vet certify pkg 1.0 --level=safe-to-deploy
      # Safe for production: No known vulnerabilities, trusted author
    
    quarry vet certify pkg 1.0 --level=safe-to-run
      # Safe for development: No immediate security concerns

Import organization audits:

    # Use your organization's pre-vetted packages
    quarry vet import-audits --from=https://security.mycompany.com/pyrite-audits
    
    # Trust Mozilla's security team audits
    quarry vet import-audits --from=mozilla
    
    # Trust community consensus (50+ organizations)
    quarry vet import-audits --from=community --min-reviewers=50

Configuration (Quarry.toml):

    [vet]
    required = true                        # Fail build if unvetted deps
    import-audits = ["mozilla", "myorg"]   # Trust these sources
    allow-community = true                 # Trust community consensus
    min-community-reviews = 50             # Minimum reviewers for auto-trust

Benefits:
  â€¢ **Reduces supply-chain risk:** Every dependency explicitly reviewed
  â€¢ **Scales across organization:** Share audits, don't duplicate work
  â€¢ **Continuous verification:** CI enforces vet requirements
  â€¢ **Community trust:** Leverage collective security review efforts
  â€¢ **Audit trail:** Know who reviewed what and when

Use cases:
  â€¢ Aerospace: DO-178C requires dependency verification
  â€¢ Medical: IEC 62304 mandates security review
  â€¢ Finance: SOC 2 compliance requires supply-chain controls
  â€¢ Government: Executive Order 14028 mandates SBOM and verification

quarry sign - Package Signing
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Cryptographic signing of packages ensures authenticity and prevents tampering:

    # As package author
    quarry sign                            # Sign package with your key
    quarry publish --signed                # Publish with signature
    
    # As package consumer
    quarry install pkg --verify-sig        # Require valid signature
    quarry config set verify-signatures always  # Enforce for all installs

Key management:

    quarry keygen                          # Generate signing keypair
    quarry key publish                     # Publish public key to registry
    quarry key import author@example.com   # Import author's public key

Package signature verification:

    $ quarry verify json-parser
    
    Signature Verification: json-parser 1.2.4
    =========================================
    
    âœ“ Package signed by: security-team@example.com
    âœ“ Signature valid (RSA-4096)
    âœ“ Public key verified (published 2023-04-15)
    âœ“ Checksum matches: a8f39d...
    âœ“ No tampering detected
    
    Author reputation:
      â€¢ 47 packages published
      â€¢ 12M total downloads
      â€¢ 0 reported vulnerabilities
      â€¢ Verified email domain

Quarry.toml configuration:

    [security]
    require-signatures = true              # Only install signed packages
    trusted-authors = [
        "security-team@example.com",
        "mozilla-team@mozilla.org"
    ]
    verify-checksums = true                # Always verify SHA-256
    reject-unsigned = true                 # Fail on unsigned packages

Registry enforcement:
  â€¢ Quarry Registry (aspirational: quarry.dev) encourages signing (badged packages)
  â€¢ Security-sensitive packages require signing for featured status
  â€¢ Signature verification integrated into quarry install by default

Reproducible Builds and SBOM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Software Bill of Materials (SBOM) generation for supply-chain transparency:

    quarry sbom                            # Generate SBOM for project
    quarry sbom --format=spdx              # SPDX format
    quarry sbom --format=cyclonedx         # CycloneDX format

Example SBOM output:

    {
      "bomFormat": "CycloneDX",
      "version": "1.4",
      "components": [
        {
          "name": "json-parser",
          "version": "1.2.4",
          "purl": "pkg:quarry/json-parser@1.2.4",
          "hashes": [{"alg": "SHA-256", "content": "a8f39d..."}],
          "licenses": ["MIT"],
          "supplier": {"name": "security-team@example.com"},
          "signature": "----------BEGIN SIGNATURE----------..."
        },
        // ... all dependencies ...
      ],
      "dependencies": [
        {"ref": "json-parser", "dependsOn": ["string-utils"]}
      ]
    }

Reproducible build verification:

    quarry build --reproducible            # Enable reproducible build mode
    quarry verify-build --sbom=project.sbom  # Verify build matches SBOM

Benefits:
  â€¢ Compliance: Meet regulatory requirements (executive orders, standards)
  â€¢ Transparency: Complete visibility into dependency tree
  â€¢ Verification: Confirm what's in the binary matches declared dependencies
  â€¢ Incident response: Quickly identify affected systems when CVE announced

Use cases:
  â€¢ Government contracts (SBOM required)
  â€¢ Healthcare (FDA guidance on software dependencies)
  â€¢ Finance (audit requirements)
  â€¢ Enterprise (security policy compliance)

Integration with CI/CD
~~~~~~~~~~~~~~~~~~~~~~

Complete supply-chain security pipeline:

    # .github/workflows/security.yml
    name: Security CI
    
    jobs:
      supply-chain:
        steps:
          - name: Audit dependencies
            run: quarry audit --fail-on=critical
          
          - name: Verify all dependencies are vetted
            run: quarry vet --enforce
          
          - name: Verify signatures
            run: quarry verify --all
          
          - name: Generate SBOM
            run: quarry sbom --format=spdx --output=sbom.json
          
          - name: Upload SBOM for compliance
            uses: actions/upload-artifact@v2
            with:
              name: software-bill-of-materials
              path: sbom.json

Dashboard integration:

    quarry security-dashboard
    
    Shows:
      â€¢ Dependency vulnerability count over time
      â€¢ Unvetted dependencies requiring review
      â€¢ Signature verification status
      â€¢ SBOM compliance status
      â€¢ Audit trail (who reviewed what)

Why Supply-Chain Security Matters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Supply-chain attacks are increasing (SolarWinds, Log4Shell, XZ Utils backdoor). 
Languages that make security an afterthought lose trust. Pyrite makes it 
first-class:

**Industry trends:**
  â€¢ Executive Order 14028 (2021): U.S. government mandates SBOM for software
  â€¢ EU Cyber Resilience Act: Requires vulnerability disclosure and updates
  â€¢ Open source supply-chain attacks: Up 650% from 2021-2024

**Pyrite's competitive advantage:**
  â€¢ Built-in tooling (not third-party bolted on)
  â€¢ Zero-friction security (quarry audit is one command)
  â€¢ Scalable verification (import organization audits)
  â€¢ Compliance-ready (SBOM generation standard)

**Real-world impact:**
  â€¢ Rust: cargo-vet adoption growing in security-critical orgs
  â€¢ npm: Signature verification added after multiple attacks
  â€¢ Go: SBOM support added in response to executive order
  â€¢ Pyrite: Ships with all of this on day one

For embedded-first strategy, supply-chain security is **table stakes**:
  â€¢ Aerospace: Supply-chain verification required for DO-178C
  â€¢ Medical: FDA guidance mandates software composition analysis
  â€¢ Automotive: ISO 26262 requires dependency security analysis
  â€¢ Industrial: IEC 62443 mandates secure development practices

Without supply-chain security: "Just another language"
With supply-chain security: "Enterprise-ready from day one"

This is a **love multiplier** - it costs implementation time, not language 
complexity, but makes Pyrite feel "serious" and "production-ready" to 
organizations evaluating systems languages for critical infrastructure.

Implementation Timeline
~~~~~~~~~~~~~~~~~~~~~~~

Stable Release:
  â€¢ quarry audit - Vulnerability database and scanning
  â€¢ quarry sign/verify - Package signing and verification
  â€¢ Basic SBOM generation
  â€¢ quarry vet - Full review workflow with organization audits
  â€¢ Advanced SBOM features (dependency graphs, license compliance)
  â€¢ Integration with enterprise security tools

Cost: Implementation time only (no language complexity)
Impact: Trust and adoption multiplier for security-critical domains
ROI: High (required for aerospace/medical/government adoption)

8.18 Binary Size Profiling (Beta Release)
--------------------------------------------------------------------------------

For embedded systems where flash memory is constrained (32KB-512KB typical), 
binary size transparency is as critical as performance profiling. The quarry bloat 
command provides detailed analysis of what's consuming binary space, enabling 
systematic size optimization.

Command Usage
~~~~~~~~~~~~~

    quarry bloat                     # Analyze binary size
    quarry bloat --sections          # Per-section breakdown
    quarry bloat --functions         # Per-function breakdown
    quarry bloat --compare=v1.0      # Track size over time
    quarry bloat --crates            # Per-dependency breakdown

Example Output
~~~~~~~~~~~~~~

    $ quarry bloat
    
    Binary Size Analysis
    ====================
    
    Target: ARM Cortex-M4 (256 KB flash budget)
    Binary: target/thumbv7em-none-eabi/release/firmware.elf
    
    Total: 47,234 bytes (46.1 KB)
    Available: 262,144 bytes (256 KB)
    Used: 18% of flash budget âœ“
    
    Largest Contributors:
    
    Section          Size      Percent
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    .text (code)     38,456    81.4%
    .rodata (data)   6,234     13.2%
    .data            1,544     3.3%
    .bss             1,000     2.1%
    
    Top 10 Functions by Size:
    
    Function                          Size      Percent   Crate
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1. std::fmt::format_args          4,234     9.0%      std
    2. core::panic::panic_fmt         3,456     7.3%      core
    3. sensor::read_and_process       2,890     6.1%      app
    4. json::parse                    2,456     5.2%      json-parser
    5. List[u8]::grow                 1,234     2.6%      std
    6. uart::write_formatted          1,123     2.4%      hal
    7. String::fmt                    987       2.1%      std
    8. f32::to_string                 856       1.8%      core
    9. network::handle_packet         734       1.6%      app
    10. crypto::verify_signature      689       1.5%      crypto-lib
    
    Optimization Suggestions:
    
    ðŸ”´ HIGH IMPACT (save 14 KB):
    
      1. Replace std::fmt with core::fmt_minimal
         Current: 4,234 bytes (full formatting)
         Minimal: 856 bytes (basic formatting only)
         Savings: 3,378 bytes (7.1%)
         
         Change: Use --features=minimal-fmt
         Trade-off: Less flexible format strings
      
      2. Use --panic=abort instead of unwind
         Current: Panic handler + unwind = 3,456 bytes
         Abort: 234 bytes (just abort)
         Savings: 3,222 bytes (6.8%)
         
         Change: panic = "abort" in Quarry.toml
         Trade-off: No panic message on device (use debugger)
    
    ðŸŸ¡ MEDIUM IMPACT (save 4 KB):
    
      3. Remove unused generic instantiations
         List[u8], List[u16], List[u32] all present
         Code uses only List[u8]
         Savings: 2,468 bytes (5.2%)
         
         Run: quarry build --gc-sections (removes unused)

Comparison Mode
~~~~~~~~~~~~~~~

Track binary size over development:

    $ quarry bloat --compare=baseline.json
    
    Binary Size Comparison
    ======================
    
    Baseline: 42,156 bytes (v1.0.0)
    Current:  47,234 bytes (main branch)
    Change:   +5,078 bytes (+12.0%) âš ï¸
    
    New code added:
      â€¢ sensor::read_and_process: +2,890 bytes
      â€¢ json::parse dependency: +2,456 bytes
      â€¢ Formatting changes: +856 bytes
    
    Removed code:
      â€¢ Old sensor code: -1,124 bytes
    
    Regressions (unintended size increases):
      â€¢ List[u8]::grow: 1,234 â†’ 1,456 bytes (+222)
        Reason: New bounds checking logic added
        Consider: --no-bounds-check for release

Section Analysis
~~~~~~~~~~~~~~~~

Detailed per-section breakdown:

    $ quarry bloat --sections
    
    .text (Code) - 38,456 bytes
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    By module:
      app code:        12,345 bytes (32.1%)
      std library:     15,234 bytes (39.6%)
      dependencies:    8,456 bytes (22.0%)
      startup code:    2,421 bytes (6.3%)
    
    .rodata (Read-Only Data) - 6,234 bytes
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    String literals:  4,567 bytes (73.3%)
      â€¢ Error messages: 2,345 bytes
      â€¢ Log messages: 1,234 bytes
      â€¢ Config strings: 988 bytes
    
    Constants:        1,234 bytes (19.8%)
    Lookup tables:    433 bytes (6.9%)
    
    Suggestions:
      â€¢ Use --strip-strings to remove debug strings
      â€¢ Move config to external flash if available

Dependency Analysis
~~~~~~~~~~~~~~~~~~~

Show which dependencies contribute to binary size:

    $ quarry bloat --crates
    
    Size by Dependency
    ==================
    
    Crate               Size      Percent   Unused
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    std                 15,234    32.2%     2,345
    json-parser         8,456     17.9%     0
    crypto-lib          3,456     7.3%      567
    hal (hardware)      2,890     6.1%      0
    app (your code)     12,345    26.1%     0
    Other               4,853     10.3%     1,234
    
    Unused code: 4,146 bytes (8.8%)
      â†’ Run with --gc-sections to remove

CI Integration
~~~~~~~~~~~~~~

Enforce size budgets in continuous integration:

    # Quarry.toml
    [build]
    max-binary-size = "64KB"        # Fail if exceeded
    warn-binary-size = "60KB"       # Warn approaching limit
    
    # CI runs:
    $ quarry bloat --check
    
    âœ“ Binary size: 46.1 KB (under 64 KB limit)
    âš ï¸  Warning: 76% of limit (approaching 60 KB warning threshold)

Size Optimization Modes
~~~~~~~~~~~~~~~~~~~~~~~

Embedded-specific optimization flags:

    quarry build --optimize=size     # Aggressive size optimization
    quarry build --strip-all         # Remove all debug info, symbols
    quarry build --minimal-panic     # Smallest panic handler
    quarry build --no-strings        # Remove all string literals

Example optimization progression:

    # Default release
    $ quarry build --release
    Binary: 124 KB
    
    # Size-optimized
    $ quarry build --release --optimize=size
    Binary: 87 KB (30% reduction)
    
    # Minimal embedded
    $ quarry build --release --optimize=size --strip-all --minimal-panic
    Binary: 52 KB (58% reduction)

Integration with Cost Analysis
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Binary size correlates with quarry cost static analysis:

    $ quarry bloat --explain json::parse
    
    Function: json::parse
    =====================
    Size: 2,456 bytes
    
    Breakdown:
      â€¢ String parsing: 1,234 bytes (50%)
      â€¢ Number parsing: 567 bytes (23%)
      â€¢ Error handling: 432 bytes (18%)
      â€¢ Whitespace handling: 223 bytes (9%)
    
    Why this is large:
      â€¢ Generic over multiple integer types (4 instantiations)
      â€¢ Comprehensive error messages (345 bytes strings)
      â€¢ Bounds checking for all array accesses
    
    Alternatives:
      â€¢ json::parse_minimal - 856 bytes (no error messages)
      â€¢ json::parse_streaming - 1,123 bytes (lower memory, larger code)

Why Binary Size Profiling Matters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This is **essential for embedded-first strategy credibility:**

**Embedded constraints:**
  â€¢ STM32F103: 64 KB flash (common microcontroller)
  â€¢ Arduino Uno: 32 KB flash
  â€¢ ESP8266: 4 MB flash (large, but code + data + OTA = tight)
  â€¢ Every byte counts when targeting resource-constrained devices

**Competitive requirement:**
  â€¢ C/C++: Manual tracking, no tooling
  â€¢ Rust: cargo bloat (excellent, must match this)
  â€¢ Zig: Excellent size transparency
  â€¢ Pyrite: Must match or exceed Rust's bloat tool

**Trust factor:**
  â€¢ Embedded developers evaluate binary size immediately
  â€¢ "How big is 'Hello World'?" is first question
  â€¢ Visible bloat = "not serious about embedded"
  â€¢ Transparent profiling = "understands our constraints"

**Real-world validation:**
  â€¢ Rust's cargo bloat widely praised in embedded community
  â€¢ Binary size is top 3 concern for embedded (after correctness, performance)
  â€¢ Transparent tooling = confidence to adopt

Without quarry bloat, Pyrite's embedded-first positioning is incomplete. With it, 
Pyrite demonstrates: "We understand embedded constraints at the tooling level, 
not just language level."

Implementation: Beta Release (after core compilation is stable)
Priority: Critical for embedded-first strategy
Complexity: Low (parse ELF/PE/Mach-O symbols, sum sizes)
Impact: High (required for embedded credibility)

8.19 Deterministic and Reproducible Builds (Beta Release)
--------------------------------------------------------------------------------

To complete Pyrite's supply-chain security story and enable verifiable builds, 
Quarry provides deterministic compilation where identical source and 
configuration produce bit-for-bit identical binaries across all machines and 
environments.

Command Usage
~~~~~~~~~~~~~

    quarry build --deterministic     # Enable deterministic mode
    quarry build --reproducible      # Alias for --deterministic
    quarry verify-build              # Verify binary matches manifest
    quarry build-hash                # Generate content-addressable hash

Deterministic Mode
~~~~~~~~~~~~~~~~~~

When enabled, the compiler ensures reproducibility:

    $ quarry build --deterministic
    
    Building in deterministic mode...
    âœ“ Source hash: a8f39d4e...
    âœ“ Quarry.lock hash: b3c82f1a...
    âœ“ Compiler version: 1.0.0
    âœ“ Target: x86_64-linux
    
    Deterministic constraints enforced:
      â€¢ Fixed timestamps (SOURCE_DATE_EPOCH=1640000000)
      â€¢ Sorted symbol tables
      â€¢ Deterministic random seeds for layout randomization
      â€¢ Stable iteration order for codegen
    
    Binary: target/release/myapp
    Build hash: 7d5e9c8b3a4f2d1e... (SHA-256)
    
    Verification:
      quarry verify-build --hash=7d5e9c8b3a4f2d1e...

Configuration
~~~~~~~~~~~~~

Enable determinism by default:

    # Quarry.toml
    [build]
    deterministic = true             # All builds reproducible
    source-date-epoch = 1640000000   # Fixed timestamp
    
    [profile.release]
    deterministic = true             # Release builds always reproducible

Verification Workflow
~~~~~~~~~~~~~~~~~~~~~

Verify that a binary matches declared sources:

    $ quarry verify-build myapp --manifest=BuildManifest.toml
    
    Verifying: myapp
    ================
    
    Checking:
      âœ“ Source files match manifest (SHA-256 hashes)
      âœ“ Dependencies match Quarry.lock
      âœ“ Compiler version: 1.0.0 (expected: 1.0.0)
      âœ“ Build flags: --release --deterministic
      âœ“ Target: x86_64-linux
    
    Rebuilding to verify...
    âœ“ Rebuild complete
    âœ“ Binary hash matches: 7d5e9c8b3a4f2d1e...
    
    VERIFIED: Binary is reproducible from declared sources

Build Manifest Format
~~~~~~~~~~~~~~~~~~~~~

Generated for every deterministic build:

    # BuildManifest.toml (committed alongside binary)
    [build]
    hash = "7d5e9c8b3a4f2d1e..."
    timestamp = "2025-12-18T10:30:00Z"
    compiler = "1.0.0"
    target = "x86_64-linux"
    flags = ["--release", "--deterministic"]
    
    [sources]
    "src/main.pyrite" = { hash = "a8f39d4e...", size = 1234 }
    "src/lib.pyrite" = { hash = "b3c82f1a...", size = 5678 }
    
    [dependencies]
    "json-parser" = { version = "1.2.4", hash = "c4d93e2b..." }
    "http-client" = { version = "3.0.1", hash = "d5e04f3c..." }

Integration with Supply-Chain Security
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Reproducible builds complete the supply-chain story:

    # Verify everything about a binary
    $ quarry verify-supply-chain myapp
    
    Supply-Chain Verification
    ==========================
    
    âœ“ Binary reproducible from sources
    âœ“ All dependencies signed and verified
    âœ“ No known CVEs in dependency tree
    âœ“ All dependencies vetted by organization
    âœ“ SBOM generated and matches binary
    
    Trust level: âœ“âœ“âœ“âœ“âœ“ (5/5) - Fully verified

CI Enforcement
~~~~~~~~~~~~~~

Ensure builds remain reproducible:

    # .github/workflows/ci.yml
    - name: Verify reproducible build
      run: |
        quarry build --deterministic
        quarry verify-build --strict
        # Fails if build is non-deterministic

Content-Addressable Builds
~~~~~~~~~~~~~~~~~~~~~~~~~~~

For maximum security, store binaries by content hash:

    $ quarry build-hash
    
    Content Hash: 7d5e9c8b3a4f2d1e0c9f8a7b6d5e4f3a2b1c0d9e8f7
    
    Storage path:
      ~/.cache/quarry/builds/7d/5e/9c/7d5e9c8b.../myapp
    
    Reproduce:
      quarry build-from-hash 7d5e9c8b...

Why Deterministic Builds Matter
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This is **essential for supply-chain security** (incomplete without it):

**Security requirements:**
  â€¢ Can't verify "this binary came from this source" without reproducibility
  â€¢ Supply-chain attacks undetectable without build verification
  â€¢ Debian, Arch Linux, F-Droid all require reproducible builds
  â€¢ Government/military: Build verification mandatory

**Competitive landscape:**
  â€¢ Rust: Reproducible by default (proven)
  â€¢ Go: Reproducible with flags
  â€¢ Zig: Strong emphasis on reproducibility
  â€¢ Pyrite: **Must match this** for credibility

**Real-world impact:**
  â€¢ SolarWinds attack: Non-reproducible builds hid backdoor
  â€¢ XZ Utils backdoor: Reproducible builds would have caught it earlier
  â€¢ Industry trend: Reproducibility becoming standard requirement

**Trust multiplier:**
  â€¢ "You can verify the binary you run" â†’ confidence
  â€¢ "Build hash is proof" â†’ auditable
  â€¢ "No hidden modifications" â†’ trustworthy

**Integration with existing features:**
  â€¢ quarry sign signs the BuildManifest + binary hash
  â€¢ quarry vet verifies dependencies' build hashes
  â€¢ quarry audit checks CVEs based on verified source versions
  â€¢ quarry sbom includes build hash for traceability

Without deterministic builds, supply-chain security (Section 8.17) is incomplete. 
With it, Pyrite offers the complete package: audit + vet + sign + verify + 
reproducible. This is **table-stakes for aerospace, medical, and government 
contracts.**

Implementation: Beta Release (high priority for supply-chain security)
Complexity: Moderate (requires compiler determinism work)
Impact: Critical (required for security certification and trust)

8.20 Energy Profiling (Stable Release)
--------------------------------------------------------------------------------

To address sustainability concerns and optimize for battery-powered devices, 
Pyrite provides built-in energy profiling that makes power consumption visible 
and optimizable. This is a **unique differentiator** - no other systems language 
has first-class energy awareness.

Command Usage
~~~~~~~~~~~~~

    quarry energy                    # Profile energy consumption
    quarry energy --duration=60s     # Profile for 60 seconds
    quarry energy --compare=baseline # Detect energy regressions
    quarry energy --json             # Machine-readable output

Example Output
~~~~~~~~~~~~~~

    $ quarry energy
    
    Energy Profile (30s run, Intel i9-12900K)
    ==========================================
    
    Total energy: 45.2 joules
    Average power: 1.51 watts
    Peak power: 8.3 watts
    
    Energy Hot Spots (by component):
    
    Component            Energy    Power    Percent
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    CPU cores            28.4 J    0.95 W   62.8%
    DRAM                 9.2 J     0.31 W   20.4%
    CPU package          5.6 J     0.19 W   12.4%
    GPU (idle)           2.0 J     0.07 W   4.4%
    
    Top Energy-Consuming Functions:
    
    Function                Energy    CPU Time   Efficiency
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1. matrix_multiply      18.2 J    5.2s       3.5 J/s
       â€¢ AVX-512 active (high power draw)
       â€¢ 12 cores utilized
       â€¢ Suggestion: Lower SIMD width for battery mode
    
    2. network_poll         12.4 J    15.8s      0.78 J/s
       â€¢ Polling every 10ms (prevents CPU sleep)
       â€¢ 1,580 wake-ups
       â€¢ Suggestion: Adaptive polling (100ms when idle)
       â€¢ Energy savings: 60%
    
    3. json_parse           8.6 J     3.1s       2.8 J/s
       â€¢ Memory-intensive (high DRAM power)
       â€¢ 4,567 allocations
       â€¢ Suggestion: Pre-allocate buffers

Battery Mode Optimization
~~~~~~~~~~~~~~~~~~~~~~~~~

Build with energy-aware optimizations:

    quarry build --optimize=battery
    
    # Or in Quarry.toml:
    [profile.battery]
    opt-level = 2                    # Balance speed and power
    simd-width = 4                   # Use SSE2, not AVX-512
    adaptive-polling = true          # Longer sleep periods
    cpu-frequency = "powersave"      # Hint to OS scheduler

Energy Budget Enforcement
~~~~~~~~~~~~~~~~~~~~~~~~~~

For battery-powered devices, enforce energy constraints:

    @energy_budget(joules=0.5, duration_s=1.0)
    fn process_sensor_reading(data: &[u8]) -> Result[Reading, Error]:
        # Compiler warns if energy budget exceeded
        # Based on hardware model + instruction costs
        ...

Example enforcement:

    warning[P1501]: energy budget may be exceeded
      ----> src/sensor.py:45:5
       |
    43 | @energy_budget(joules=0.5, duration_s=1.0)
    44 | fn process_sensor_reading(data: &[u8]) -> Result[Reading, Error]:
    45 |     let result = expensive_fft(data)
       |                  ^^^^^^^^^^^^^^^^^^^ estimated 0.8 J (exceeds 0.5 J)
       |
       = note: FFT on 1024 samples estimated at 0.8 joules
       = help: Consider:
               1. Reduce sample size: fft(&data[0..512])
               2. Use lower-power algorithm: fast_approximation()
               3. Increase budget if justified

Platform Support
~~~~~~~~~~~~~~~~

Energy profiling requires platform-specific APIs:

  â€¢ **Linux:** RAPL (Running Average Power Limit) via perf
  â€¢ **macOS:** powermetrics (requires sudo)
  â€¢ **Windows:** ETW (Event Tracing for Windows) power events
  â€¢ **Embedded:** Hardware-specific power monitors (STM32 power profiler, etc.)
  â€¢ **Android/iOS:** Platform battery APIs

Fallback for unsupported platforms:
  â€¢ Estimate based on instruction costs + hardware models
  â€¢ Warn: "Energy profiling unavailable, showing estimates"

Battery-Life Estimation
~~~~~~~~~~~~~~~~~~~~~~~

For mobile/embedded applications:

    $ quarry energy --battery=2500mAh --voltage=3.7V
    
    Battery Life Estimate
    =====================
    
    Battery capacity: 2500 mAh Ã— 3.7 V = 9.25 Wh = 33,300 J
    Average power: 1.51 W
    
    Estimated battery life:
      â€¢ Continuous operation: 6.1 hours
      â€¢ With sleep mode (90% idle): 48 hours
    
    Breakdown:
      â€¢ Active processing: 1.51 W Ã— 10% = 0.151 W
      â€¢ Sleep mode: 0.05 W Ã— 90% = 0.045 W
      â€¢ Average: 0.196 W
      â€¢ Battery life: 33,300 J / 0.196 W / 3600 = 47.2 hours

Why Energy Profiling Is a Differentiator
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**No systems language has built-in energy profiling:**
  â€¢ C/C++: External tools only (Intel VTune, etc.)
  â€¢ Rust: No energy tooling
  â€¢ Zig: No energy tooling
  â€¢ Go: No energy tooling
  â€¢ Mojo: No energy tooling
  â€¢ **Pyrite: First-class feature**

**Growing importance:**
  â€¢ **Sustainability:** Green software movement (reduce data center energy)
  â€¢ **Mobile:** Battery life is primary UX constraint
  â€¢ **Embedded IoT:** Coin-cell batteries (years of operation required)
  â€¢ **Cloud costs:** Energy = money (AWS/Azure charge for power)
  â€¢ **Regulatory:** EU energy efficiency requirements for devices

**Marketing message:**
  â€¢ "The energy-aware systems language"
  â€¢ "Optimize for battery life, not just speed"
  â€¢ "Sustainability by design"

**Practical value:**
  â€¢ IoT devices: Optimize for coin-cell battery (10+ years target)
  â€¢ Mobile apps: "Our app uses 40% less battery" (competitive advantage)
  â€¢ Data centers: Reduce cooling costs (energy efficiency)
  â€¢ Laptops: Longer battery life = better UX

**Integration with existing tools:**
  â€¢ quarry cost shows allocations â†’ correlate with DRAM power
  â€¢ quarry perf shows CPU usage â†’ correlate with CPU power
  â€¢ quarry energy synthesizes: "This allocation costs 0.2 mJ"

This positions Pyrite as forward-thinking: not just fast and safe, but 
**responsible**. Sustainability-conscious developers (growing demographic) will 
appreciate that Pyrite considers the environmental impact of software.

Implementation: Stable Release (requires platform-specific power APIs)
Priority: Medium (unique differentiator, growing importance)
Complexity: Moderate (platform-specific integrations)
Impact: High (unique positioning, sustainability appeal)

8.21 Dead Code Analysis and Elimination (Beta Release)
--------------------------------------------------------------------------------

To optimize binary size and maintainability, Quarry provides comprehensive dead 
code detection and removal tooling.

Command Usage
~~~~~~~~~~~~~

    quarry deadcode                  # Find unused code
    quarry deadcode --remove         # Remove dead code automatically
    quarry build --gc-sections       # Link-time dead code elimination

Example Output
~~~~~~~~~~~~~~

    $ quarry deadcode
    
    Dead Code Analysis
    ==================
    
    Found 23 unused items (3,456 bytes in binary)
    
    Unused functions (18):
      â€¢ src/utils.py:45 - old_algorithm() [234 bytes]
        Last used: Never
        Suggestion: Remove or mark @deprecated
      
      â€¢ src/parser.py:123 - parse_legacy_format() [567 bytes]
        Last used: Removed in v1.2.0
        Suggestion: Remove (format no longer supported)
    
    Unused types (3):
      â€¢ src/types.py:89 - struct LegacyConfig [145 bytes]
        Never instantiated
        Suggestion: Remove or document why kept
    
    Unused imports (2):
      â€¢ src/main.py:5 - import old_crypto
        Module imported but never used
    
    Generic instantiations never called (5):
      â€¢ List[u16] instantiated but never used [1,234 bytes]
      â€¢ Map[i8, String] instantiated but never used [876 bytes]
    
    Total savings if removed: 3,456 bytes (7.3% of binary)
    
    Run 'quarry deadcode --remove' to apply

Automatic Removal
~~~~~~~~~~~~~~~~~

    $ quarry deadcode --remove --dry-run
    
    Would remove:
      â€¢ 18 unused functions
      â€¢ 3 unused types
      â€¢ 2 unused imports
      â€¢ 5 generic instantiations
    
    Apply? [y/N]

Link-Time Optimization
~~~~~~~~~~~~~~~~~~~~~~

    quarry build --gc-sections
    
    # Linker removes unreferenced sections
    # Effective for:
    #   â€¢ Generic instantiations never called
    #   â€¢ Library functions never used
    #   â€¢ Debug code in release builds

Integration with CI
~~~~~~~~~~~~~~~~~~~

    # Fail CI if dead code exceeds threshold
    quarry deadcode --threshold=1KB --fail
    
    error: Dead code exceeds 1KB threshold
      Current: 3.456 KB unused
      Threshold: 1 KB
    
    CI FAILURE: Remove dead code before merging

Why Dead Code Analysis Matters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Binary size optimization:**
  â€¢ Embedded: Every byte of flash matters
  â€¢ Distribution: Smaller binaries download faster
  â€¢ Security: Less code = smaller attack surface

**Code quality:**
  â€¢ Dead code is technical debt
  â€¢ Confuses new contributors
  â€¢ May hide bugs (code not tested because never called)

**Maintenance:**
  â€¢ Clear signal: "This code isn't used, safe to remove"
  â€¢ Prevents accumulation of cruft
  â€¢ Keeps codebase lean

Implementation: Beta Release (after core compilation is stable)
Priority: Medium-High (valuable for embedded + maintainability)
Complexity: Low (static analysis + symbol table inspection)
Impact: Medium-High (binary size + code quality)

8.22 Dependency License Compliance (Stable Release)
--------------------------------------------------------------------------------

For organizations with legal requirements, Quarry provides license compatibility 
checking and reporting to ensure dependency licenses are compatible with project 
requirements.

Command Usage
~~~~~~~~~~~~~

    quarry license-check             # Verify license compatibility
    quarry license-report            # Generate license report
    quarry sbom --licenses           # Include in SBOM

License Configuration
~~~~~~~~~~~~~~~~~~~~~

    # Quarry.toml
    [package]
    license = "MIT"
    
    [licenses]
    allowed = ["MIT", "Apache-2.0", "BSD-3-Clause"]
    denied = ["GPL-3.0", "AGPL-3.0"]  # Copyleft incompatible with MIT
    warn = ["LGPL-2.1"]                # Requires review

Example Output
~~~~~~~~~~~~~~

    $ quarry license-check
    
    License Compatibility Report
    =============================
    
    Your project: MIT License
    
    Dependencies: 47 packages
    
    âœ“ Compatible: 44 packages
      â€¢ 32 packages: MIT
      â€¢ 8 packages: Apache-2.0
      â€¢ 4 packages: BSD-3-Clause
    
    âš ï¸  Requires Review: 2 packages
      â€¢ json-parser 1.2.4: LGPL-2.1
        Note: LGPL requires dynamic linking or source distribution
        Your usage: Static linking
        Risk: License violation
        
        Options:
          1. Switch to MIT-licensed alternative: json-fast 2.0
          2. Use dynamic linking (--crate-type=dylib)
          3. Provide source distribution (LGPL compliance)
      
      â€¢ crypto-lib 3.0: ISC
        Note: ISC is MIT-compatible but not in allowed list
        Action: Add "ISC" to allowed licenses if acceptable
    
    âœ— INCOMPATIBLE: 1 package
      â€¢ legacy-parser 0.8: GPL-3.0
        Conflict: GPL-3.0 is copyleft, incompatible with MIT
        Your project cannot use this dependency
        
        Fix:
          1. Remove dependency: quarry remove legacy-parser
          2. Find alternative with compatible license
          3. Relicense your project (if all contributors agree)
    
    CI FAILURE: Incompatible licenses detected

CI Enforcement
~~~~~~~~~~~~~~

    # .github/workflows/ci.yml
    - name: Check license compatibility
      run: quarry license-check --fail-on=incompatible

License Report Generation
~~~~~~~~~~~~~~~~~~~~~~~~~

    $ quarry license-report --format=markdown
    
    # Generated: LICENSES.md
    
    # Third-Party Licenses
    
    This project includes the following dependencies:
    
    ## MIT License (32 packages)
    - json-parser 1.2.4
    - http-client 3.0.1
    [... full license text ...]
    
    ## Apache-2.0 (8 packages)
    - crypto-lib 3.0
    [... full license text ...]

Integration with SBOM
~~~~~~~~~~~~~~~~~~~~~

    $ quarry sbom --licenses --format=spdx
    
    {
      "components": [
        {
          "name": "json-parser",
          "version": "1.2.4",
          "licenses": ["LGPL-2.1"],
          "license-text": "...",
          "license-url": "https://opensource.org/licenses/LGPL-2.1"
        }
      ]
    }

Why License Compliance Matters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Legal requirements:**
  â€¢ Enterprises: Legal departments require license audits
  â€¢ Open source: GPL contamination prevents commercial use
  â€¢ Distribution: App stores require license disclosure

**Competitive feature:**
  â€¢ Rust: cargo-license (third-party, not built-in)
  â€¢ Go: go-licenses (third-party)
  â€¢ Pyrite: First-class built-in feature

**Trust signal:**
  â€¢ Shows Pyrite understands enterprise needs
  â€¢ Reduces legal risk for adopters
  â€¢ Makes Pyrite "enterprise-ready"

Implementation: Stable Release (extends SBOM work)
Priority: Medium (enterprise adoption enabler)
Complexity: Low (parse license metadata, check compatibility)
Impact: Medium (removes adoption barrier for regulated industries)

8.23 Hot Reloading for Rapid Iteration (Stable Release)
--------------------------------------------------------------------------------

For long-running processes during development, Quarry provides hot reloading that 
updates code without restarting the application, dramatically accelerating the 
iteration cycle for certain workflows.

Command Usage
~~~~~~~~~~~~~

    quarry dev                       # Watch mode with hot reload
    quarry dev --preserve-state      # Reload code, keep data structures
    quarry dev --functions-only      # Only reload function bodies

How It Works
~~~~~~~~~~~~

    $ quarry dev
    
    Starting development server...
    Watching: src/**/*.pyrite
    
    âœ“ Initial build complete
    âœ“ Application running (PID 12345)
    âœ“ Hot reload enabled
    
    Press Ctrl+C to stop, Ctrl+R to force reload
    
    [10:30:15] File changed: src/renderer.pyrite
    [10:30:15] Recompiling renderer module...
    [10:30:16] âœ“ Hot reloaded in 847ms
    [10:30:16] Application state preserved

Example Use Cases
~~~~~~~~~~~~~~~~~

**Game development:**

    # Game running at 60 FPS
    # Developer changes enemy AI logic
    # Hot reload updates AI without restarting game
    # Player position, score, etc. all preserved

**Web development:**

    # Server running, handling requests
    # Developer fixes bug in route handler
    # Hot reload updates handler function
    # Active connections preserved, no downtime

**Data processing:**

    # Processing large dataset (30 minutes expected)
    # Developer spots bug in processing function
    # Hot reload fixes bug mid-run
    # Already-processed data not re-computed

Restrictions and Safety
~~~~~~~~~~~~~~~~~~~~~~~

Hot reloading only works for certain changes:

    âœ“ Function bodies (logic changes)
    âœ“ Method implementations
    âœ“ Constants and static data
    âœ“ Module-level functions
    
    âœ— Type definitions (struct fields, enum variants)
    âœ— Function signatures (parameters, return types)
    âœ— Unsafe blocks (requires full recompilation)
    âœ— Dependency changes (requires restart)

When incompatible change detected:

    [10:35:42] File changed: src/types.pyrite
    [10:35:42] âœ— Cannot hot reload (struct fields changed)
    [10:35:42] Restart required: quarry dev --restart

Safety Guarantees
~~~~~~~~~~~~~~~~~

Hot reloading maintains safety:
  â€¢ Ownership rules still enforced (can't hot-reload into invalid state)
  â€¢ Type changes rejected (would break memory layout assumptions)
  â€¢ Unsafe changes rejected (require audit)
  â€¢ Only safe, compatible changes allowed

Implementation Approach
~~~~~~~~~~~~~~~~~~~~~~~

    1. Monitor source files for changes
    2. Incremental recompilation of changed module
    3. Dynamic library loading (dlopen) for new code
    4. Atomic function pointer swap (single instruction)
    5. Old code GC'd when no longer referenced

State Preservation
~~~~~~~~~~~~~~~~~~

Developer controls what state persists:

    @hot_reload(preserve_state = true)
    static mut CACHE: Map[String, Data] = Map::new()
    
    fn process(key: &str) -> Data:
        # CACHE survives hot reload
        # Function body can be updated

Configuration
~~~~~~~~~~~~~

    # Quarry.toml
    [dev]
    hot-reload = true
    preserve-state = ["CACHE", "CONNECTIONS"]
    watch-paths = ["src/**/*.pyrite"]
    ignore-paths = ["src/generated/**"]

Why Hot Reloading Matters
~~~~~~~~~~~~~~~~~~~~~~~~~~

**Developer joy:**
  â€¢ Game dev: Tweak gameplay without losing session
  â€¢ Web dev: See changes instantly (no restart)
  â€¢ Data science: Iterate on algorithms mid-computation
  â€¢ Learning: Faster experimentation ("what if I change this?")

**Competitive parity:**
  â€¢ Rust: rust-analyzer supports limited hot reload
  â€¢ Erlang/Elixir: Hot code swapping is flagship feature
  â€¢ JavaScript: Hot Module Replacement (HMR) standard
  â€¢ Python: importlib.reload() for modules

**Productivity multiplier:**
  â€¢ Restart time: 5-30 seconds typical
  â€¢ Hot reload: <1 second
  â€¢ 100 iteration cycles: 8-50 minutes saved per session

**Limitations:**
  â€¢ Debug builds only (not for production)
  â€¢ Requires explicit design (not all code hot-reloadable)
  â€¢ Best effort (some changes still require restart)

Implementation: Stable Release (after incremental compilation is stable)
Priority: Medium (developer experience enhancement)
Complexity: High (dynamic loading, state management)
Impact: Medium-High (productivity boost for certain workflows)

8.24 Incremental Compilation (Beta Release)
--------------------------------------------------------------------------------

Fast rebuilds are essential for developer productivity. Quarry implements 
incremental compilation to cache unchanged modules and recompile only what's 
necessary.

Command Usage
~~~~~~~~~~~~~

    quarry build --incremental       # Enable incremental compilation (default)
    quarry build --no-incremental    # Force full rebuild
    quarry clean --incremental       # Clear incremental cache

How It Works
~~~~~~~~~~~~

    $ quarry build --incremental
    
    Checking incremental cache...
    âœ“ Cached: 234 modules (unchanged)
    âœ“ Recompiling: 3 modules (modified)
    âœ“ Relinking: target/debug/myapp
    
    Finished in 1.8s (full build: 28s, 15.5x faster)

Incremental Strategy
~~~~~~~~~~~~~~~~~~~~

Compiler tracks:
  â€¢ Source file hashes (detect changes)
  â€¢ Dependency graph (what depends on what)
  â€¢ Module interface fingerprints (detect API changes)
  â€¢ Cached compilation artifacts per module

Rebuild decision tree:
  â€¢ File unchanged â†’ use cached artifact
  â€¢ File changed, interface unchanged â†’ recompile, no downstream rebuilds
  â€¢ File changed, interface changed â†’ recompile + all dependents

Example:

    src/utils.pyrite:
      - Changed: Implementation only (private function)
      - Interface: Unchanged (public API same)
      - Action: Recompile utils.pyrite only
      - Dependents: No rebuild needed
    
    src/types.pyrite:
      - Changed: Added struct field
      - Interface: Changed (public API modified)
      - Action: Recompile types.pyrite + all dependents
      - Dependents: main.pyrite, parser.pyrite, renderer.pyrite (all rebuild)

Performance Characteristics
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Typical rebuild times:

    | Project Size      | Full Build | Incremental | Speedup |
    |-------------------|------------|-------------|---------|
    | Small (10K LOC)   | 3s         | 0.5s        | 6x      |
    | Medium (100K)     | 28s        | 1.8s        | 15x     |
    | Large (1M)        | 320s       | 12s         | 27x     |

Cache Management
~~~~~~~~~~~~~~~~

    quarry cache info                # Show cache statistics
    quarry cache clean               # Remove stale cache entries
    quarry cache purge               # Delete entire cache

Example cache info:

    Incremental Cache Statistics
    =============================
    
    Location: ~/.cache/quarry/incremental
    Size: 1.2 GB (234 projects cached)
    
    Current project:
      â€¢ Cached modules: 234
      â€¢ Cache size: 45 MB
      â€¢ Last full build: 2025-12-18 10:30:00
      â€¢ Incremental builds: 147 since last full build
      â€¢ Average incremental time: 1.2s

Why Incremental Compilation Is Essential
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Developer experience:**
  â€¢ Fast iteration = more experimentation
  â€¢ Slow rebuilds = frustration, context switching
  â€¢ 1-2s rebuilds feel instant, 30s rebuilds feel slow

**Competitive requirement:**
  â€¢ Rust: Incremental compilation standard (expected)
  â€¢ Go: Fast compilation by default
  â€¢ C++: ccache, distcc for incremental builds
  â€¢ **Without this, Pyrite feels slow even if compiler is fast**

**Learning impact:**
  â€¢ Beginners iterate more when feedback is instant
  â€¢ "Change â†’ test" loop should be <5s
  â€¢ Slow rebuilds discourage experimentation

**Real-world data:**
  â€¢ Google: 30% productivity increase with faster builds
  â€¢ Rust: Incremental compilation adoption correlated with satisfaction
  â€¢ Developer survey: "Fast builds" in top 5 language features

Implementation: Beta Release (essential for developer experience)
Priority: Critical (expected feature, high impact on satisfaction)
Complexity: Moderate (requires module dependency tracking)
Impact: High (productivity multiplier for all developers)

8.25 Community Transparency Dashboard (Stable Release)
--------------------------------------------------------------------------------

To make Pyrite's goal of widespread developer adoption measurable rather than purely 
aspirational, the ecosystem provides a public metrics dashboard at (aspirational: quarry.dev/metrics) that 
displays real-time, verifiable data about language performance, safety, 
learning, and adoption.

Dashboard Contents
~~~~~~~~~~~~~~~~~~

**Performance Metrics (User-Submitted Benchmarks):**

    Pyrite vs Competitors (1,247 benchmarks)
    ========================================
    
    Average performance relative to C:
      â€¢ Pyrite: 98.3% of C speed
      â€¢ Rust: 97.1% of C speed
      â€¢ Zig: 99.1% of C speed
      â€¢ Go: 82.4% of C speed
    
    Compilation speed:
      â€¢ Pyrite: 1.2s average (100K LOC project)
      â€¢ Rust: 8.4s average
      â€¢ C++: 12.3s average
      â€¢ Go: 0.8s average
    
    Binary size (Hello World):
      â€¢ Pyrite: 312 KB
      â€¢ Rust: 387 KB
      â€¢ C (dynamic): 8 KB
      â€¢ C (static): 2.1 MB
      â€¢ Go: 2.0 MB

**Safety Metrics (CVE Tracking):**

    Memory Safety CVEs (2020-2025)
    ===============================
    
    Language    Total CVEs    Memory-Related    Percentage
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    C           1,247         892               71.5%
    C++         1,089         654               60.1%
    Rust        23            0                 0%
    Go          89            4                 4.5%
    Pyrite      0             0                 0%
    
    Data races:
      â€¢ C/C++: Not preventable by language
      â€¢ Rust: 0 (prevented by type system)
      â€¢ Go: 12 (race detector catches at runtime)
      â€¢ Pyrite: 0 (prevented by ownership system)

**Learning Metrics (quarry learn Data):**

    Learning Curve Analysis
    =======================
    
    Exercise completion rates:
      â€¢ Ownership exercises: 82% complete all 12
      â€¢ SIMD exercises: 67% complete
      â€¢ Concurrency: 74% complete
    
    Comparison with Rust:
      â€¢ Rust ownership exercises: 64% complete
      â€¢ Pyrite advantage: 28% higher completion
    
    Time to productivity:
      â€¢ Pyrite: 2.3 weeks average (first PR)
      â€¢ Rust: 4.1 weeks average
      â€¢ C++: 6.8 weeks average
    
    quarry fix usage:
      â€¢ 89% of beginners use quarry fix --interactive
      â€¢ Average fixes per developer: 147
      â€¢ Adoption: "Compiler taught me ownership" - 94%

**Ecosystem Health:**

    Quarry Registry Metrics
    =======================
    
    Total packages: 15,247
    Growth: +23% month-over-month
    
    Package categories:
      â€¢ Embedded/HAL: 3,456 packages
      â€¢ Web frameworks: 2,345 packages
      â€¢ CLI tools: 1,890 packages
      â€¢ Crypto/security: 1,234 packages
    
    Active maintainers: 2,340 (â†‘ 18% MoM)
    Security audits: 1,456 packages vetted
    
    Dependency health:
      â€¢ 94% of packages updated in last 6 months
      â€¢ Average dependencies: 4.2 (vs Rust: 8.7)
      â€¢ Batteries-included stdlib reduces dependency hell

**Compile-Time Safety:**

    Bugs Caught at Compile Time
    ============================
    
    (Data from user error reports)
    
    Error category               Frequency   C/C++ equivalent
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Use-after-move              34.2%       Use-after-free (segfault)
    Borrow conflicts            23.5%       Data races (UB)
    Type mismatches             18.7%       Type punning (UB)
    Unhandled errors            12.4%       Unchecked returns (bugs)
    Lifetime violations         11.2%       Dangling pointers (segfault)
    
    Estimated bugs prevented: 3,247 per 100K LOC
    (Compared to typical C/C++ project with same code structure)

**Adoption Metrics:**

    Production Deployments
    ======================
    
    Companies using Pyrite: 1,247
    Industries:
      â€¢ Embedded/IoT: 34%
      â€¢ Web services: 28%
      â€¢ Gaming: 15%
      â€¢ Finance: 12%
      â€¢ Other: 11%
    
    Lines of code in production: 47M
    Developer satisfaction: 8.7/10
    Would recommend: 89%

Public API
~~~~~~~~~~

Dashboard data accessible via API:

    # Query metrics programmatically
    # Aspirational: does not exist yet
    curl https://quarry.dev/api/metrics/performance
    
    # Aspirational: does not exist yet
    <script src="https://quarry.dev/widgets/metrics.js"></script>

Community Contribution
~~~~~~~~~~~~~~~~~~~~~~

Users can submit benchmark data:

    quarry bench --upload            # Upload benchmark to metrics
    quarry bench --compare=community # Compare with community average

Privacy and opt-in:
  â€¢ Anonymous by default (no personal data)
  â€¢ Opt-in for benchmark submission
  â€¢ Aggregated statistics only (no individual user data)

Why Transparency Dashboard Is Transformative
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Makes developer adoption measurable:**

  â€¢ Not subjective claim, but verifiable data
  â€¢ "98% of C speed" with proof, not promises
  â€¢ "82% complete ownership exercises" shows teachability
  â€¢ Real-world evidence beats marketing claims

**Competitive positioning:**
  â€¢ Direct comparison with C/Rust/Zig/Mojo
  â€¢ Show strengths objectively (compile speed, safety, learning curve)
  â€¢ Acknowledge trade-offs honestly (ecosystem size, maturity)

**Trust multiplier:**
  â€¢ "See the data yourself" builds confidence
  â€¢ Open data = transparent community
  â€¢ Track progress over time (metrics improve as language matures)

**Gamification:**
  â€¢ Package maintainers compete for "most-used" ranking
  â€¢ Contributors see impact ("my PR improved compile time 12%")
  â€¢ Community engagement through visible metrics

**Evidence-based advocacy:**
  â€¢ "Pyrite caught 3,247 bugs per 100K LOC" â†’ share with CTO
  â€¢ "15x faster compilation than Rust" â†’ share with team
  â€¢ "28% higher learning success rate" â†’ share with educators

**Example impact:**
  â€¢ Rust doesn't have public dashboard â†’ metrics scattered
  â€¢ Go has limited metrics â†’ not comprehensive
  â€¢ Pyrite dashboard â†’ one place for all evidence
  â€¢ Result: "Just look at (aspirational: quarry.dev/metrics)" becomes standard response

This transforms subjective claims into objective evidence. The path to "most 
admired" becomes visible: watch metrics improve over time, celebrate milestones 
publicly, demonstrate progress to skeptics.

Implementation: Stable Release (after core language is stable)
Priority: High (trust multiplier, advocacy enabler)
Complexity: Moderate (web dashboard, data aggregation, privacy)
Impact: High (makes success measurable, enables evidence-based marketing)

8.26 Why Quarry Matters
--------------------------------------------------------------------------------

Developer Experience = Language Adoption
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Stack Overflow surveys correlate Rust's growth with Cargo's excellence. Quarry 
delivers similar experience:

  â€¢ Zero configuration for common cases
  â€¢ One obvious way to do things
  â€¢ Fast, reliable builds with caching
  â€¢ Reproducible across environments
  â€¢ Integrated testing, docs, formatting
  â€¢ Frictionless dependency management
  â€¢ Cross-compilation out of the box
  â€¢ Script mode for rapid prototyping
  â€¢ Edition system for long-term stability

Quarry transforms Pyrite from "interesting language" to "practical tool I want 
to use daily." Great tooling is the difference between languages that are 
admired in theory versus loved in practice.

================================================================================
9. STANDARD LIBRARY AND ECOSYSTEM
================================================================================

To be a true "do-anything" language, Pyrite is bundled with a comprehensive 
standard library that makes it productive out of the box. The standard library 
is designed with the same philosophy as the language: performance, safety, and 
simplicity by default, with no hidden costs.

9.1 Standard Library Design Philosophy
--------------------------------------------------------------------------------

Pyrite's standard library is "batteries included" - shipping with everything 
needed to build real applications without pulling in dozens of dependencies. The 
stdlib enables developers to evaluate Pyrite by building complete projects (web 
servers, CLI tools, games) using only built-in functionality.

The philosophy: provide excellent baseline implementations for common tasks, with 
zero magic and predictable performance.

Pit of Success: API Design Principles
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The standard library is designed to make the performant choice the obvious choice:

0. **APIs take views by default; ownership-taking is always spelled:**
   
   This is the **dominant rule** that prevents the "why did this move?" confusion:
   
   Standard (90% of APIs):
       fn parse(content: &str) -> Result[Data, Error]
       fn process(items: &[Item]) -> Summary
       fn validate(config: &Config) -> bool
   
   Ownership-taking (rare, always explicit):
       fn consume(data: List[int]) -> ProcessedData
       fn take_ownership(resource: File) -> Handle
       # These require @consumes annotation in docs
   
   Rule enforcement:
     â€¢ stdlib APIs default to borrowed views (&T, &str, &[T])
     â€¢ Taking ownership requires doc comment explaining why
     â€¢ quarry lint warns: "function takes ownership without @consumes"
     â€¢ Compiler suggests: "Consider &T instead of T"
   
   Why this matters:
     â€¢ Beginners learn "pass by reference" as default
     â€¢ Ownership transfer becomes explicit and rare
     â€¢ "Cannot use moved value" errors become uncommon
     â€¢ Code reads like high-level Python but compiles to zero-cost
   
   Exception cases (ownership-taking justified):
     â€¢ Builder pattern: consume builder to produce final type
     â€¢ Thread spawn: move data to thread (Send requirement)
     â€¢ Containers: insert/push take ownership to store
   
   This single convention prevents the #1 beginner frustration with ownership.

0.5. **Safe accessors by default; unchecked when provable:**
   
   Collection APIs provide safe accessors that bounds-check by default, with 
   optimizer elision when safety is provable at compile time:
   
   Safe accessor (Core mode default):
       let value = arr.get(i)  # Returns Optional[T], never panics
       match value:
           Some(v): process(v)
           None: handle_out_of_bounds()
   
   Direct indexing (optimizes to unchecked when safe):
       let value = arr[i]      # Bounds-checked at runtime
                               # Optimizer removes check if i provably valid
   
   Explicit unchecked (unsafe only):
       unsafe:
           let value = arr.get_unchecked(i)  # Caller guarantees validity
   
   Optimizer elision examples:
   
       # Compiler proves i is always valid, elides bounds check:
       for i in 0..arr.len():
           let value = arr[i]  # No runtime check (loop bounds prove safety)
       
       # Compiler cannot prove safety, keeps check:
       let i = user_input.parse()
       let value = arr[i]      # Runtime bounds check (prevents crash)
   
   Teaching progression:
     1. **Week 1 (Core mode):** Use .get() exclusively, learn Optional handling
     2. **Week 2:** Learn that arr[i] is checked too, but returns Result/panics
     3. **Week 3:** Understand optimizer removes checks when provable
     4. **Week 4+:** Use arr[i] knowing it's safe with compiler help
   
   API design rules:
     â€¢ Collection.get(index) â†’ Optional[T] (safe, explicit None case)
     â€¢ Collection[index] â†’ T (checked, panics on out-of-bounds in debug)
     â€¢ Collection.get_unchecked(index) â†’ T (unsafe, no checks)
   
   Why this matters:
     â€¢ Beginners learn safe patterns (Optional handling)
     â€¢ Intermediate developers use direct indexing with safety
     â€¢ Optimizer makes safe code as fast as unsafe code (when provable)
     â€¢ Explicit unsafe only needed when truly required
   
   Compiler optimization examples:
   
       # Pattern 1: Iterator indices (always safe)
       for (i, item) in arr.iter().enumerate():
           let neighbor = arr[i + 1]  # Bounds check KEPT (may overflow)
       
       # Pattern 2: Proven bounds
       if i < arr.len():
           let value = arr[i]         # Bounds check ELIDED (proven safe)
       
       # Pattern 3: Fixed-size arrays
       let buffer: [u8; 256] = ...
       for i in 0..256:
           buffer[i] = 0              # Bounds check ELIDED (compile-time size)
   
   quarry cost shows which checks remain:
   
       Performance Analysis
       ====================
       
       Bounds checks (3 sites, 2 elided):
         âœ“ Line 45: arr[i] - ELIDED (loop bounds prove safety)
         âœ“ Line 67: buffer[idx] - ELIDED (compile-time size check)
         âš  Line 89: data[user_idx] - CHECKED (unprovable, keeps safety)
   
   This approach gives beginners safety by default while teaching that "fast" and 
   "safe" aren't opposed - the compiler makes safe code fast when it can prove 
   correctness.

1. **Fast path is the easy path:**
   
   Good:
       let builder = StringBuilder.with_capacity(estimated_size)
       builder.append("Hello")
       builder.append(" world")
       let result = builder.to_string()  # Single allocation
   
   Discouraged:
       let result = "Hello" + " world"   # Multiple allocations
       # String + operator marked @deprecated in stdlib
       # Compiler suggests: "Use StringBuilder for efficiency"

2. **Expensive operations look expensive:**
   
   Cheap:
       list.len()           # O(1), field access
       map.get(key)         # O(1) expected
   
   Expensive:
       list.clone()         # Explicit allocation + copy
       data.to_owned()      # Heap allocation (vs borrowing)
       vec.sort()           # O(n log n), obvious operation

3. **Default to borrowed views:**
   
   stdlib APIs prefer:
       fn parse(content: &str) -> Result[Data, Error]
       fn process(items: &[Item]) -> Summary
   
   Over:
       fn parse(content: String) -> Result[Data, Error]  # Takes ownership
       fn process(items: Vec[Item]) -> Summary           # Unnecessary move

4. **Pre-allocation is obvious:**
   
   Collections provide capacity hints:
       List::with_capacity(n)
       Map::with_capacity(n)
       String::with_capacity(n)
   
   Compiler warns when growing in loops:
       warning: list may reallocate in loop
         suggestion: pre-allocate with with_capacity()

5. **Builders for complex construction:**
   
   Instead of:
       let url = "https://" + host + ":" + port + "/" + path
   
   Provide:
       let url = UrlBuilder::new()
           .scheme("https")
           .host(host)
           .port(port)
           .path(path)
           .build()

6. **Iterators avoid allocations:**
   
   Lazy evaluation by default:
       numbers.iter()
           .filter(|x| x % 2 == 0)
           .map(|x| x * 2)
           .sum()
       # Zero intermediate collections

7. **Escape hatches clearly labeled:**
   
   When you need the expensive operation:
       data.clone()              # Explicit: makes a copy
       text.to_owned()           # Explicit: heap allocation
       list.into_vec()           # Explicit: transfer to owned
   
   Never hidden behind operator overloading or implicit conversions.

Implementation Examples
~~~~~~~~~~~~~~~~~~~~~~~

String Building:

    # BAD (stdlib actively discourages this)
    var result = ""
    for name in names:
        result = result + name + ", "  # N allocations!
    
    # GOOD (stdlib makes this easy)
    let mut builder = StringBuilder::with_capacity(names.len() * 10)
    for name in names:
        builder.append(name)
        builder.append(", ")
    let result = builder.to_string()  # 1 allocation

The stdlib might not even provide String::operator+ to prevent the antipattern.

Collection Growth:

    # Suboptimal (but works)
    let mut list = List::new()
    for i in 0..1000:
        list.push(i)  # May reallocate ~10 times
    
    # Optimal (stdlib makes this obvious)
    let mut list = List::with_capacity(1000)
    for i in 0..1000:
        list.push(i)  # Never reallocates
    
    # Compiler helps:
    warning[P1050]: list may reallocate in loop
      â†’ suggestion: List::with_capacity(1000)

View-First APIs:

    # stdlib function signatures prefer views
    fn join(separator: &str, items: &[&str]) -> String
    fn contains(haystack: &[u8], needle: &[u8]) -> bool
    fn parse_json(text: &str) -> Result[Value, ParseError>
    
    # Callers can pass owned or borrowed data
    let result = join(", ", &vec_of_strings)      # vec deref to slice
    let result = join(", ", &["a", "b", "c"])     # array to slice
    let result = join(", ", items_as_slice)       # already a slice

Teaching Through APIs
~~~~~~~~~~~~~~~~~~~~~

The stdlib teaches performance intuition through its design:

  â€¢ If it's cheap, it's easy (len(), get())
  â€¢ If it's expensive, it's explicit (clone(), to_owned())
  â€¢ If it allocates, it's obvious (new(), with_capacity())
  â€¢ If it might reallocate, compiler warns

This is superior to documentation that says "be careful with +". The API design 
makes mistakes hard and good practices easy.

Key aspects of the standard library design:

9.2 Core Collections
--------------------------------------------------------------------------------

List[T] - Dynamic Array
~~~~~~~~~~~~~~~~~~~~~~~

Heap-allocated, growable array (analogous to std::vector or Vec):

    let mut numbers = List[int].new()
    numbers.push(1)
    numbers.push(2)
    numbers.push(3)
    
    # Pre-allocate for performance
    let mut buffer = List[u8].with_capacity(1024)
    
    # Iteration
    for n in numbers:
        print(n)
    
    # Slicing
    let slice = numbers[1..3]  # elements 1-2

API guarantees:
  â€¢ Amortized O(1) push (may reallocate)
  â€¢ O(1) indexed access with bounds checking
  â€¢ O(n) insert/remove in middle
  â€¢ Capacity and length separated (explicit reallocation control)

Map[K, V] - Hash Table
~~~~~~~~~~~~~~~~~~~~~~

Key-value store with O(1) average lookup:

    let mut scores = Map[String, int].new()
    scores.insert("Alice", 100)
    scores.insert("Bob", 85)
    
    match scores.get("Alice"):
        Some(score):
            print("Alice scored:", score)
        None:
            print("Not found")
    
    # Iteration
    for (name, score) in scores:
        print(name, "-->", score)

Set[T] - Hash Set
~~~~~~~~~~~~~~~~~

Unique elements with O(1) membership testing:

    let mut seen = Set[String].new()
    seen.insert("foo")
    
    if seen.contains("foo"):
        print("Already processed")

Other Collections
~~~~~~~~~~~~~~~~~

  â€¢ LinkedList[T] - Doubly-linked list
  â€¢ BinaryHeap[T] - Priority queue
  â€¢ VecDeque[T] - Double-ended queue
  â€¢ BTreeMap[K, V] - Sorted map
  â€¢ BTreeSet[T] - Sorted set

Inline Storage Collections (Optimization Helpers)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For performance-critical code that frequently uses small collections, Pyrite 
provides inline-storage variants that avoid heap allocation when sizes are small:

SmallVec[T, N] - Inline Vector
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Stack-allocated up to N elements, spills to heap if exceeded:

    import std::collections
    
    # Common case: lists with 0-8 items
    let mut tags = SmallVec[String, 8]::new()
    
    tags.push("rust")      # On stack
    tags.push("systems")   # On stack
    tags.push("safe")      # On stack
    # ... up to 8 items stay on stack
    
    # If you exceed capacity, automatically spills to heap
    for tag in many_tags:  # If > 8 items
        tags.push(tag)     # Transparent heap allocation

Memory layout:
  â€¢ Stack size: sizeof(T) * N + pointer + length + capacity
  â€¢ Example: SmallVec[int, 8] = 64 bytes (8 ints) + 24 bytes (metadata) = 88 bytes
  â€¢ If spilled: 24 bytes stack + heap allocation

When to use:
  â€¢ Profiling shows: "Most lists have < 10 items, but a few are large"
  â€¢ Function-local collections with typical small size
  â€¢ Avoiding allocation in hot paths for common cases

Example from quarry tune:

    warning: List[Token] has median size 6, max size 247
      â†’ Suggestion: Use SmallVec[Token, 8] for 90% zero-alloc case
      â†’ Remaining 10% still work correctly (heap allocation)

Performance characteristics:
  â€¢ N or fewer items: Zero heap allocations, stack-only
  â€¢ More than N items: One heap allocation (same as List)
  â€¢ Still has amortized O(1) push
  â€¢ Trade-off: Larger stack footprint

SmallString[N] - Inline String
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Small String Optimization (SSO) for short strings:

    import std::collections
    
    # Most strings are short (file names, identifiers, etc.)
    let mut name = SmallString[32]::new()
    
    name.append("config")      # On stack
    name.append(".toml")       # On stack
    # Total: 11 bytes, fits in 32-byte inline buffer

Memory layout:
  â€¢ Stack size: N bytes + length byte + flags
  â€¢ Example: SmallString[32] = 32 bytes + 2 bytes = 34 bytes total
  â€¢ If exceeded: Falls back to heap-allocated String

Typical sizes:
  â€¢ SmallString[16] - Short identifiers, tags
  â€¢ SmallString[32] - File names, config keys (common default)
  â€¢ SmallString[64] - Short paths, URLs
  â€¢ SmallString[256] - Full paths, longer text

When to use:
  â€¢ Profiling shows: "90% of strings are < 30 characters"
  â€¢ Temporary string building in hot paths
  â€¢ String keys in tight loops

Example:

    fn format_log_line(level: &str, message: &str) -> SmallString[128]:
        let mut line = SmallString[128]::new()
        line.append("[")
        line.append(level)
        line.append("] ")
        line.append(message)
        return line
    
    # For typical logs: zero allocations
    # For long messages: falls back to heap gracefully

Performance win:
  â€¢ Avoids millions of small allocations for short strings
  â€¢ Cache-friendly (data inline with metadata)
  â€¢ Still correct for any length (automatic spillover)

InlineMap[K, V, N] - Small Hash Map
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Inline hash map for small key-value sets:

    import std::collections
    
    # Configuration often has 2-5 entries
    let mut flags = InlineMap[String, bool, 4]::new()
    
    flags.insert("debug", true)
    flags.insert("verbose", false)
    # ... up to 4 entries stay on stack

Memory layout:
  â€¢ Stack array of N entries (key, value, hash)
  â€¢ Linear search for small N (faster than hashing for N < 8)
  â€¢ If exceeded N entries: Converts to heap-allocated Map

When to use:
  â€¢ Small lookup tables (2-8 entries)
  â€¢ Function-local caches
  â€¢ Configuration dictionaries

Performance characteristics:
  â€¢ N or fewer entries: Zero allocations, O(N) lookup (fast for small N)
  â€¢ More than N entries: Converts to O(1) hash map on heap
  â€¢ Trade-off: Linear search (but cache-friendly, predictable)

Typical usage:

    fn parse_color(name: &str) -> Optional[Color]:
        # Static color map, 8 common colors
        const COLORS = InlineMap[&str, Color, 8]::from([
            ("red", Color.RED),
            ("green", Color.GREEN),
            ("blue", Color.BLUE),
            // ... 5 more
        ])
        
        return COLORS.get(name)  # Zero allocations, fast lookup

Integration with quarry tune
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The quarry tune command suggests inline-storage types automatically:

    $ quarry tune
    
    Performance Tuning Suggestions
    ===============================
    
    3. Use SmallVec for small arrays
       Location: src/ast.py:89
       
       Current code:
         fn parse_attributes() -> List[Attribute]:
             let mut attrs = List[Attribute].new()
             // typically returns 0-3 attributes
       
       Profile data:
         â€¢ Called 10,000 times
         â€¢ Size distribution: min=0, median=2, p95=5, max=47
         â€¢ Current cost: 10,000 heap allocations
       
       Suggested fix:
         fn parse_attributes() -> SmallVec[Attribute, 6]:
             let mut attrs = SmallVec[Attribute, 6]::new()
       
       Expected improvement:
         â€¢ Eliminates 9,500 allocations (95% case)
         â€¢ Remaining 500 calls still allocate (max > 6)
         â€¢ Estimated speedup: 12-15%
       
       [Apply] [Skip] [Explain]

This makes performance optimization mechanical: profiler identifies patterns, 
suggests specific containers, estimates improvement.

Teaching Path
~~~~~~~~~~~~~

Inline-storage types are introduced as optimization, not fundamentals:

1. **Week 1-2:** Use standard List, String, Map
   - Learn ownership and borrowing with familiar containers
   - Build working programs

2. **Week 3-4:** Profile code with quarry cost and quarry alloc
   - Identify allocation hot spots
   - See "1000 allocations in loop"

3. **Week 5+:** Optimize with inline-storage types
   - quarry tune suggests SmallVec[T, 8]
   - Apply suggestion, measure improvement
   - Understand trade-offs (stack space vs allocations)

4. **Expert:** Choose optimal sizes based on profiling
   - "Median=3, p95=7 â†’ use N=8"
   - Balance stack footprint vs allocation avoidance

These containers are "pit of success" for performance: common patterns get easy, 
fast solutions without requiring deep expertise.

Why This Matters
~~~~~~~~~~~~~~~~~

Inline-storage containers are proven winners in production systems:

  â€¢ Rust: smallvec crate (8M downloads/month)
  â€¢ C++: llvm::SmallVector (ubiquitous in LLVM/Clang)
  â€¢ Real world: Most collections are small (< 10 items)

By including these in stdlib, Pyrite makes the fast path easy:
  â€¢ Beginners learn standard containers first
  â€¢ Intermediate developers optimize hot paths
  â€¢ quarry tune makes suggestions mechanical
  â€¢ Performance win without complexity explosion

This is the "pit of success" in action: the ergonomic choice becomes the 
performant choice for the 90% case.

9.3 String Handling
--------------------------------------------------------------------------------

String Type
~~~~~~~~~~~

Immutable UTF-8 string:

    let greeting = "Hello, world!"
    let length = greeting.len()          # Byte length
    let char_count = greeting.chars().count()  # Character count
    
    # Slicing (byte indices)
    let hello = greeting[0..5]
    
    # Searching
    if greeting.contains("world"):
        print("Found it")
    
    # Splitting
    for word in greeting.split(", "):
        print(word)

StringBuilder
~~~~~~~~~~~~~

Efficient mutable string building:

    let mut builder = StringBuilder.new()
    builder.append("Hello")
    builder.append(", ")
    builder.append("world")
    let result = builder.to_string()  # Single allocation

String Formatting
~~~~~~~~~~~~~~~~~

Type-safe formatting without allocations in hot paths:

    let name = "Alice"
    let age = 30
    let msg = format("Hello, {}! You are {} years old.", name, age)
    
    # Or for performance-critical code:
    let mut buf = [u8; 256]
    let written = format_to_slice(&mut buf, "x = {}", x)

9.4 File and I/O Operations
--------------------------------------------------------------------------------

File Operations
~~~~~~~~~~~~~~~

Safe file handling with Result types:

    # Reading entire file
    match File.read_to_string("config.txt"):
        Ok(contents):
            print(contents)
        Err(e):
            print("Error reading file:", e)
    
    # Writing file
    match File.write("output.txt", data):
        Ok(_):
            print("Written successfully")
        Err(e):
            print("Write error:", e)
    
    # Buffered reading
    match File.open("large.dat"):
        Ok(file):
            let reader = BufferedReader.new(file)
            for line in reader.lines():
                process(line)
        Err(e):
            print("Cannot open:", e)

Path Manipulation
~~~~~~~~~~~~~~~~~

Cross-platform path handling:

    let path = Path.new("/usr/local/bin")
    let joined = path.join("myapp")
    
    if path.exists():
        print("Directory exists")
    
    if path.is_dir():
        for entry in path.read_dir():
            print(entry.name())

9.5 Serialization (JSON, TOML)
--------------------------------------------------------------------------------

JSON Support
~~~~~~~~~~~~

Built-in JSON parsing and generation:

    # Parsing
    let json_str = '{"name": "Alice", "age": 30}'
    match json.parse(json_str):
        Ok(value):
            let name = value["name"].as_string()
            let age = value["age"].as_int()
        Err(e):
            print("Parse error:", e)
    
    # Generating
    let data = json.object()
    data.set("name", "Bob")
    data.set("age", 25)
    let output = data.to_string()

TOML Support
~~~~~~~~~~~~

Configuration file format:

    match toml.parse_file("config.toml"):
        Ok(config):
            let database = config["database"]["host"].as_string()
        Err(e):
            print("Config error:", e)

Derive Serialization
~~~~~~~~~~~~~~~~~~~~

Automatic serialization for structs:

    @derive(Serialize, Deserialize)
    struct Config:
        host: String
        port: int
        debug: bool
    
    let config = Config { host: "localhost", port: 8080, debug: true }
    let json = json.to_string(config)

9.6 Networking
--------------------------------------------------------------------------------

TCP Client/Server
~~~~~~~~~~~~~~~~~

    # TCP Server
    let listener = TcpListener.bind("127.0.0.1:8080")?
    for stream in listener.incoming():
        match stream:
            Ok(conn):
                handle_client(conn)
            Err(e):
                print("Connection error:", e)
    
    # TCP Client
    let mut stream = TcpStream.connect("example.com:80")?
    stream.write(b"GET / HTTP/1.0\r\n\r\n")?
    let response = stream.read_to_end()?

HTTP Client
~~~~~~~~~~~

Built-in HTTP client for common use cases:

    # Simple GET request
    let response = http.get("https://api.example.com/data")?
    print("Status:", response.status)
    print("Body:", response.text())
    
    # POST with JSON
    let data = json.object()
    data.set("name", "Alice")
    
    let response = http.post("https://api.example.com/users")
        .json(data)
        .send()?

HTTP Server (Basic)
~~~~~~~~~~~~~~~~~~~

Lightweight HTTP server for simple applications:

    fn handle_request(req: &Request) -> Response:
        if req.path == "/":
            return Response.ok("Hello, world!")
        return Response.not_found()
    
    let server = HttpServer.new("127.0.0.1:3000", handle_request)
    server.run()?

9.7 Time and Dates
--------------------------------------------------------------------------------

Duration and Instant
~~~~~~~~~~~~~~~~~~~~

    let start = Instant.now()
    # ... do work ...
    let elapsed = start.elapsed()
    print("Took {} ms", elapsed.as_millis())
    
    # Sleep
    sleep(Duration.from_secs(2))

DateTime
~~~~~~~~

    let now = DateTime.now()
    print(now.format("%Y-%m-%d %H:%M:%S"))
    
    let christmas = DateTime.parse("2025-12-25", "%Y-%m-%d")?
    let days_until = christmas.duration_since(now).as_days()

9.8 Command-Line Argument Parsing
--------------------------------------------------------------------------------

Args Parser
~~~~~~~~~~~

Built-in CLI argument parsing:

    let args = Args.parse()
    
    if args.has_flag("--verbose"):
        enable_verbose()
    
    let output = args.get_value("--output").unwrap_or("stdout")
    let inputs = args.get_positionals()

Or with structured parsing:

    @derive(Args)
    struct CliArgs:
        @arg(short='v', long="verbose")
        verbose: bool
        
        @arg(short='o', long="output", default="stdout")
        output: String
        
        @arg(positional)
        files: List[String]
    
    let args = CliArgs.parse()?
    if args.verbose:
        print("Verbose mode enabled")

9.9 Regular Expressions
--------------------------------------------------------------------------------

Regex Support
~~~~~~~~~~~~~

    let re = Regex.new(r"\d{3}--\d{4}")?  # Phone pattern
    
    if re.is_match("555-1234"):
        print("Valid phone number")
    
    # Capture groups
    match re.captures("Call 555-1234 now"):
        Some(caps):
            print("Phone:", caps[0])
        None:
            print("No match")

9.10 Mathematics
--------------------------------------------------------------------------------

Common Math Functions
~~~~~~~~~~~~~~~~~~~~~

    import math
    
    let angle = math.pi / 4.0
    let sine = math.sin(angle)
    let cosine = math.cos(angle)
    let power = math.pow(2.0, 10.0)
    let root = math.sqrt(16.0)

Random Numbers
~~~~~~~~~~~~~~

    import random
    
    let r = random.thread_rng()
    let dice = r.gen_range(1..=6)
    let coin = r.gen_bool(0.5)

9.11 Numerical Computing: Tensor Type (Stable Release)
--------------------------------------------------------------------------------

For numerical computing, scientific computing, and machine learning applications, 
Pyrite provides a first-class Tensor type that combines compile-time shape 
checking with explicit memory layout control.

Design Philosophy
~~~~~~~~~~~~~~~~~

Pyrite's Tensor is NOT a full ML framework (not Numpy/PyTorch). It's a 
memory layout and indexing abstraction that composes with existing performance 
primitives (SIMD, tiling, parallelization).

Core Tensor Type
~~~~~~~~~~~~~~~~

    import std::numerics
    
    # Compile-time shape and layout
    struct Tensor[T, Shape: (int...), Layout: TensorLayout]:
        data: [T; Shape.total_size()]  # Fixed-size, stack or inline
        
        fn shape() -> (int...):
            return Shape
        
        fn get(&self, indices: (int...)) -> &T:
            # Layout-aware indexing
        
        fn get_mut(&mut self, indices: (int...)) -> &mut T:
            # Mutable access

Tensor layouts:

    enum TensorLayout:
        RowMajor     # C-style: rightmost index varies fastest
        ColMajor     # Fortran-style: leftmost index varies fastest
        Strided      # Arbitrary strides (for views)

Example usage:

    # 2D matrix (1024Ã—768)
    let mut image = Tensor[f32, (1024, 768), RowMajor]::zeros()
    
    # Compile-time shape checking
    image.get((100, 200))  # OK
    # image.get((100, 200, 50))  # ERROR: wrong rank
    
    # Layout-aware iteration
    for i in 0..1024:
        for j in 0..768:
            image[(i, j)] = compute(i, j)

Tensor Views (Slicing)
~~~~~~~~~~~~~~~~~~~~~~

Zero-cost slicing with explicit aliasing through borrowing:

    fn process_row(row: &TensorView[f32, (768,)]):
        # Operate on borrowed view, no copy
    
    let image = Tensor[f32, (1024, 768), RowMajor]::zeros()
    process_row(image.row(10))  # Borrow row 10

TensorView types:

    struct TensorView[T, Shape: (int...), Layout: TensorLayout]:
        data: &[T]           # Borrowed slice
        strides: (int...)    # For non-contiguous views
        
        fn is_contiguous() -> bool:
            # Check if view is contiguous in memory

Integration with Performance Primitives
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Tensors compose naturally with Pyrite's existing features using parameter 
closures (zero-cost):

    # SIMD operations on tensors
    fn scale_tensor[Rows: int, Cols: int](
        data: &mut Tensor[f32, (Rows, Cols), RowMajor],
        factor: f32
    ):
        # Parameter closure (fn[...]) is inlined into SIMD loop
        algorithm.vectorize[width=auto](Rows * Cols, fn[i: int]:
            data.data[i] *= factor
        )
    
    # Cache-aware matrix multiply
    fn matmul[M: int, N: int, K: int](
        a: &Tensor[f32, (M, K), RowMajor],
        b: &Tensor[f32, (K, N), ColMajor]
    ) -> Tensor[f32, (M, N), RowMajor]:
        var result = Tensor[M, N]::zeros()
        
        # Parameter closure (fn[...]) inlined into tiling loop (zero-cost)
        algorithm.tile[block_size=64](M, N, fn[i_block: int, j_block: int]:
            # Process 64Ã—64 tiles (cache-friendly)
            for i in i_block..min(i_block + 64, M):
                for j in j_block..min(j_block + 64, N):
                    result[(i, j)] = dot_product(a.row(i), b.col(j))
        )
        
        return result

Dynamic-Size Tensors
~~~~~~~~~~~~~~~~~~~~

For cases where sizes aren't known at compile time:

    struct DynTensor[T, Rank: int, Layout: TensorLayout]:
        data: List[T]        # Heap-allocated
        shape: [int; Rank]   # Runtime shape
        strides: [int; Rank] # Runtime strides

Type Safety
~~~~~~~~~~~

Compile-time checking prevents common numeric code bugs:

    let a = Tensor[f32, (10, 20), RowMajor]::zeros()
    let b = Tensor[f32, (20, 30), RowMajor]::zeros()
    let c = matmul(&a, &b)  # OK: (10,20) Ã— (20,30) â†’ (10,30)
    
    let d = Tensor[f32, (15, 25), RowMajor]::zeros()
    # let e = matmul(&a, &d)  # ERROR: incompatible shapes

Why This Matters
~~~~~~~~~~~~~~~~

Pyrite's Tensor fills the gap between "write loops" and "use heavyweight ML 
framework":

  â€¢ Compile-time shape checking prevents dimension mismatch bugs
  â€¢ Explicit layout control (row-major vs col-major) for optimal cache usage
  â€¢ Zero-cost slicing through borrowing semantics
  â€¢ Composes with existing performance tools (SIMD, tiling, parallelization)
  â€¢ No runtime overhead (fixed-size tensors are stack-allocated)

This makes Pyrite credible for numerical computing without becoming "yet another 
ML framework." Provide the foundation; let libraries build on top.

Implementation: Stable Release (after SIMD and algorithmic helpers are stable)

9.12 SIMD and Vectorization (Stable Release)
--------------------------------------------------------------------------------

For performance-critical numerical code, Pyrite provides explicit SIMD 
(Single Instruction, Multiple Data) support through the std::simd module. 
This is opt-in and explicit - Pyrite does not auto-vectorize, ensuring 
predictable performance.

Design Philosophy
~~~~~~~~~~~~~~~~~

SIMD in Pyrite follows the language's core principles:

  â€¢ **Explicit, not implicit:** You write SIMD code explicitly
  â€¢ **Portable, not platform-specific:** APIs work across architectures
  â€¢ **Zero-cost:** Compiles to native vector instructions
  â€¢ **Type-safe:** Compiler prevents SIMD misuse
  â€¢ **Teachable:** Clear syntax and error messages

SIMD Types
~~~~~~~~~~

The std::simd module provides generic vector types:

    import std::simd
    
    # Portable vector types
    let v1 = simd::Vec[f32, 4]::new([1.0, 2.0, 3.0, 4.0])  # 4-wide f32 vector
    let v2 = simd::Vec[f32, 4]::new([5.0, 6.0, 7.0, 8.0])
    
    # Element-wise operations
    let sum = v1 + v2           # [6.0, 8.0, 10.0, 12.0]
    let product = v1 * v2       # [5.0, 12.0, 21.0, 32.0]

Platform-Specific Width Detection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use compile-time introspection to choose optimal SIMD width:

    import std::simd
    
    fn process_array(data: &[f32]):
        # Choose width based on CPU capabilities
        const WIDTH = simd::preferred_width[f32]()  # 4, 8, or 16
        
        # Process in SIMD chunks
        let chunks = data.chunks(WIDTH)
        for chunk in chunks:
            let vec = simd::Vec[f32, WIDTH]::load(chunk)
            # ... SIMD operations ...
            vec.store(chunk)

Common SIMD Operations
~~~~~~~~~~~~~~~~~~~~~~

    import std::simd
    
    fn dot_product[N: int](a: [f32; N], b: [f32; N]) -> f32:
        # Compile-time SIMD width selection
        const WIDTH = simd::preferred_width[f32]()
        
        var sum = 0.0
        var i = 0
        
        # Process SIMD chunks
        while i + WIDTH <= N:
            let va = simd::Vec[f32, WIDTH]::load(&a[i])
            let vb = simd::Vec[f32, WIDTH]::load(&b[i])
            let prod = va * vb
            sum += prod.horizontal_sum()
            i += WIDTH
        
        # Process remainder scalar
        while i < N:
            sum += a[i] * b[i]
            i += 1
        
        return sum

Integration with Compile-Time Parameterization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

SIMD combines naturally with Pyrite's compile-time parameters:

    fn matrix_multiply[Rows: int, Cols: int, Width: int](
        a: &Matrix[Rows, Cols],
        b: &Matrix[Cols, Width]
    ) -> Matrix[Rows, Width]:
        # All dimensions known at compile time
        # Compiler can unroll loops and use SIMD
        
        const SIMD_WIDTH = simd::preferred_width[f32]()
        var result = Matrix[Rows, Width]::zero()
        
        for i in 0..Rows:
            for j in 0..Width:
                # SIMD inner product if Cols >= SIMD_WIDTH
                if Cols >= SIMD_WIDTH:
                    result[i, j] = simd_dot_product(&a[i], &b.col(j))
                else:
                    result[i, j] = scalar_dot_product(&a[i], &b.col(j))
        
        return result

Why Explicit SIMD?
~~~~~~~~~~~~~~~~~~

Pyrite does NOT auto-vectorize user code because:

1. **Predictability:** Developers know exactly which code uses SIMD
2. **Cost transparency:** SIMD is a deliberate performance choice
3. **Portability:** Explicit SIMD works across compilers and platforms
4. **Debugging:** SIMD bugs are isolated to SIMD-marked code
5. **Teaching:** Developers learn when and why to use SIMD

Auto-vectorization is "magic" that sometimes works and sometimes doesn't, 
depending on compiler whims. Explicit SIMD is a contract: you write vector 
code, you get vector instructions.

SIMD in Standard Library Implementation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

While user code requires explicit SIMD, the standard library MAY use SIMD 
internally for implementation efficiency:

  â€¢ Numeric operations (integer/float arithmetic) can use SIMD primitives
  â€¢ String operations (search, compare) may use vector instructions
  â€¢ Collection operations (sorting, scanning) can leverage SIMD

Key principle: **User-facing API remains explicit, internal implementation can 
optimize**

Example:

    # User code: explicit about SIMD
    let result = algorithm.vectorize[width=8](data.len(), fn[i: int]:
        data[i] *= 2.0
    )
    
    # Stdlib internals: may use SIMD for implementation
    impl String:
        fn contains(&self, pattern: &str) -> bool:
            # Internal: may use SIMD for byte scanning
            # External: API doesn't promise SIMD (implementation detail)

Documentation clarifies:

    """
    String::contains - Searches for substring
    
    Implementation: Uses SIMD byte scanning on supported platforms
    Fallback: Scalar search on platforms without SIMD
    
    Performance: O(n*m) worst case, O(n) typical with SIMD
    
    This is an implementation detail. The API contract is:
      â€¢ Returns bool indicating presence
      â€¢ No allocation
      â€¢ No side effects
    
    Run 'quarry explain-type String' to see platform-specific implementation
    """

This approach:
  â€¢ Keeps user code explicit (no surprise SIMD in user functions)
  â€¢ Allows stdlib to be fast by default (internal SIMD for efficiency)
  â€¢ Maintains cost transparency (quarry explain-type shows implementation choices)
  â€¢ Teaches that "explicit for control, implicit for convenience" applies at the 
    right boundary (stdlib internals vs user code)

The distinction: **User code = explicit SIMD, Stdlib internals = may use SIMD**

This aligns with Mojo's "SIMD is fundamental" philosophy while maintaining 
Pyrite's "no surprises" surface API. Users get fast stdlib implementations 
without needing to understand SIMD until they're ready to optimize their own 
hot paths.

Attributes for SIMD Code
~~~~~~~~~~~~~~~~~~~~~~~~

Mark functions that should use SIMD:

    @simd(width=4)
    fn process_floats(data: &mut [f32]):
        # Compiler ensures SIMD instructions are generated
        # Errors if SIMD width not achievable

Compare with non-SIMD:

    fn process_floats_scalar(data: &mut [f32]):
        # Regular scalar code
        # No SIMD expectations

Platform Support
~~~~~~~~~~~~~~~~

SIMD support is tiered:

  â€¢ **Tier 1:** x86_64 (SSE2/AVX2/AVX-512), ARM64 (NEON)
  â€¢ **Tier 2:** ARM32 (NEON), RISC-V (vector extension)
  â€¢ **Fallback:** Scalar emulation on unsupported platforms

The std::simd API is the same across all platforms - only the generated 
instructions differ.

CPU Multi-Versioning (Stable Release)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For shipping single binaries that run optimally across diverse hardware, Pyrite 
provides CPU multi-versioning with automatic runtime dispatch for SIMD and other 
CPU features:

SIMD Multi-Versioning
~~~~~~~~~~~~~~~~~~~~~

    @multi_version(
        baseline="sse2",           # Minimum requirement (runs everywhere)
        targets=["avx2", "avx512"] # Optimized variants
    )
    fn process_pixels(data: &mut [f32]):
        # Compiler generates 3 versions:
        # 1. process_pixels_sse2   - baseline (4-wide SIMD)
        # 2. process_pixels_avx2   - AVX2 (8-wide SIMD)
        # 3. process_pixels_avx512 - AVX-512 (16-wide SIMD)
        
        # Write code once using portable SIMD
        # Parameter closure (fn[...]) inlined per version
        const WIDTH = simd::preferred_width[f32]()
        algorithm.vectorize[width=WIDTH](data.len(), fn[i: int]:
            data[i] = data[i].sqrt() * 2.0
        )

General CPU Feature Multi-Versioning
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Beyond SIMD, @multi_version supports targeting arbitrary CPU feature sets for 
maximum performance across diverse hardware:

    @multi_version(
        baseline="x86-64",           # Core x86-64 ISA
        targets=[
            "x86-64-v2",             # +SSE4.2, +POPCNT
            "x86-64-v3",             # +AVX2, +BMI2, +FMA
            "x86-64-v4"              # +AVX-512
        ]
    )
    fn hash_data(data: &[u8]) -> u64:
        # Compiler generates 4 versions using different instruction sets
        # v2: Uses POPCNT, CRC32 intrinsics
        # v3: Uses BMI2 (PDEP/PEXT), FMA for polynomial hashing
        # v4: Uses AVX-512 for vectorized hashing
        
        var hash: u64 = 0
        for byte in data:
            hash = hash.wrapping_mul(31).wrapping_add(byte as u64)
        return hash

Feature-specific optimizations:

    @multi_version(
        baseline="generic",
        targets=[
            "+popcnt",               # Population count instruction
            "+bmi2",                 # Bit manipulation instructions 2
            "+aes"                   # AES-NI hardware acceleration
        ]
    )
    fn count_bits(data: &[u64]) -> usize:
        var count = 0
        for word in data:
            count += word.count_ones()  # Uses POPCNT on +popcnt target
        return count

Cross-architecture support:

    @multi_version(
        baseline="generic",
        targets=[
            "x86-64-v3",             # Intel/AMD modern
            "aarch64+neon",          # ARM64 with NEON
            "aarch64+sve",           # ARM64 with SVE
            "riscv64+v"              # RISC-V with vector extension
        ]
    )
    fn accelerated_compute(data: &mut [f32]):
        # Single code, multiple architectures
        # Compiler selects target based on runtime CPU detection

At program startup, runtime dispatcher detects CPU features and selects best 
version. Subsequent calls use the fast path directly (cached function pointer).

How it works:

1. **Compile time:** Compiler generates multiple versions
   - Each version compiled with different target-feature flags
   - Baseline version always generated (guaranteed to run)
   - Optimized versions use advanced instructions

2. **Startup time:** Runtime CPU feature detection (one-time cost)
   - Check CPUID on x86_64
   - Check hwcaps on ARM
   - Build dispatch table (function pointer per multi-versioned function)

3. **Runtime:** First call goes through dispatcher
   - Dispatcher selects best available version
   - Updates function pointer to direct call
   - Subsequent calls are direct (zero overhead)

Example generated code (conceptual):

    # Generated by compiler
    fn process_pixels_sse2(data: &mut [f32]):
        # Use SSE2 instructions (4-wide)
    
    fn process_pixels_avx2(data: &mut [f32]):
        # Use AVX2 instructions (8-wide)
    
    fn process_pixels_avx512(data: &mut [f32]):
        # Use AVX-512 instructions (16-wide)
    
    # Runtime dispatcher (called once at startup)
    static mut PROCESS_PIXELS_PTR: fn(&mut [f32]) = process_pixels_dispatch
    
    fn process_pixels_dispatch(data: &mut [f32]):
        # CPU feature detection
        if cpu_has_avx512():
            PROCESS_PIXELS_PTR = process_pixels_avx512
        elif cpu_has_avx2():
            PROCESS_PIXELS_PTR = process_pixels_avx2
        else:
            PROCESS_PIXELS_PTR = process_pixels_sse2
        
        # Tail call to selected version
        PROCESS_PIXELS_PTR(data)
    
    # Public API (always calls through pointer)
    fn process_pixels(data: &mut [f32]):
        PROCESS_PIXELS_PTR(data)

Cross-architecture support:

    @multi_version(
        baseline="generic",           # Scalar fallback
        targets=["neon", "sve"]       # ARM variants
    )
    fn compute(data: &mut [f32]):
        # Works on ARM64 with NEON/SVE detection

Benefits:
  â€¢ **Ship once, run fast everywhere:** One binary for all CPUs
  â€¢ **No user configuration:** Automatic at runtime
  â€¢ **Predictable performance:** Always uses best available instructions
  â€¢ **Gradual adoption:** Old CPUs run baseline, new CPUs get speedup
  â€¢ **Zero cost if not used:** No overhead for non-multi-versioned code

Performance impact:
  â€¢ Baseline (SSE2): 4-wide SIMD
  â€¢ AVX2: 8-wide = 2x faster
  â€¢ AVX-512: 16-wide = 4x faster
  â€¢ Typical real-world: 2-3x speedup on modern CPUs vs baseline

Use cases:
  â€¢ Image/video processing (run fast on desktop and laptop)
  â€¢ Scientific computing (support diverse HPC hardware)
  â€¢ Game engines (optimal on high-end and low-end machines)
  â€¢ Cryptography (constant-time algorithms with best instructions)

Cost transparency:
  â€¢ quarry cost shows: "3 versions generated (+10% binary size)"
  â€¢ Compiler warns if variants are identical: "AVX2 and AVX-512 generate same code"
  â€¢ quarry explain shows dispatch cost: "One-time <5Î¼s, amortized zero"

Teaching path:
  1. **Intermediate:** Write portable SIMD with preferred_width
  2. **Advanced:** Add @multi_version for production deployment
  3. **Expert:** Profile across targets, tune per-variant code

Integration with compilation:

    quarry build --release --multi-version
    # Generates all target variants
    # Binary contains: _sse2, _avx2, _avx512 versions + dispatcher

Limitations:
  â€¢ Only for performance-critical functions (not everything)
  â€¢ Binary size increases (N versions Ã— function size)
  â€¢ Startup cost for CPU detection (amortized over runtime)

This is a **Stable Release flagship feature** that differentiates Pyrite in deployment 
scenarios. No other systems language makes "fast everywhere" this easy.

Learning Path
~~~~~~~~~~~~~

SIMD is introduced late in the learning curve:

1. **Beginner:** Write scalar code, learn ownership
   
   for i in 0..data.len():
       data[i] *= 2.0

2. **Intermediate:** Use algorithmic helpers (parameter closures)
   
   algorithm.vectorize[width=auto](data.len(), fn[i: int]:
       data[i] *= 2.0
   )
   
   "Write scalar logic in fn[...], get SIMD automatically"

3. **Advanced:** Learn explicit SIMD for critical loops
   
   const WIDTH = simd::preferred_width[f32]()
   while i + WIDTH <= len:
       let vec = simd::Vec[f32, WIDTH]::load(&data[i])
       ...

4. **Expert:** Write SIMD libraries with compile-time specialization
   
   fn kernel[Width: int](data: &mut [f32]):
       # Specialized versions per Width
       ...

The parameter closure model (fn[...]) provides the bridge between scalar code 
(beginner-friendly) and SIMD code (expert-level). Beginners write scalar logic 
in parameter closures, get SIMD performance automatically through vectorize.

SIMD is opt-in performance for those who need it, not a requirement for 
everyday code. Parameter closures make the "opt-in" ergonomic enough for 
intermediate developers.

Algorithmic Helpers: Ergonomic Entry Points
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

While explicit SIMD (std::simd) provides full control, most developers benefit 
from higher-level helpers that compile to optimal SIMD code. The std::algorithm 
module provides ergonomic entry points inspired by Mojo's algorithmic primitives:

vectorize - Automatic SIMD Loop Generation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The vectorize helper automatically generates SIMD loops from scalar operations 
using parameter closures (compile-time, zero-cost):

    import std::algorithm
    
    fn scale_array(data: &mut [f32], factor: f32):
        # High-level: describe what to do per element
        # Parameter closure (fn[...]) is inlined, zero allocation
        algorithm.vectorize[width=auto](data.len(), fn[i: int]:
            data[i] = data[i] * factor
        )
    
    # Compiler generates:
    # 1. SIMD loop for aligned chunks (Vec[f32, WIDTH])
    # 2. Scalar loop for remainder elements
    # 3. Optimal width selection based on platform
    # 4. Closure body inlined directly (no function call)

Parameters:
  â€¢ width=auto: Compiler chooses optimal SIMD width (4, 8, or 16)
  â€¢ width=N: Force specific width (e.g., width=8 for AVX)
  â€¢ unroll=N: Loop unrolling factor (default: auto)

Example with manual width:

    # Force 8-wide SIMD (AVX2)
    # Parameter closure: fn[i: int] with square brackets
    algorithm.vectorize[width=8](pixels.len(), fn[i: int]:
        pixels[i] = clamp(pixels[i], 0.0, 1.0)
    )

Generated code (conceptual):

    const WIDTH = 8
    var i = 0
    
    # SIMD loop
    while i + WIDTH <= pixels.len():
        let vec = simd::Vec[f32, 8]::load(&pixels[i])
        let clamped = vec.clamp(0.0, 1.0)
        clamped.store(&mut pixels[i])
        i += WIDTH
    
    # Scalar remainder
    while i < pixels.len():
        pixels[i] = clamp(pixels[i], 0.0, 1.0)
        i += 1

Benefits:
  â€¢ Write scalar logic, get SIMD performance
  â€¢ Handles alignment and remainder automatically
  â€¢ Compiler can optimize based on array length (compile-time if known)
  â€¢ Still explicit (vectorize call is visible, not hidden)

parallelize - Structured Parallel Execution
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The parallelize helper provides safe, structured parallelism using parameter 
closures (compile-time, zero-allocation):

    import std::algorithm
    
    fn process_image_rows(image: &mut Image):
        # Parallel execution with automatic work distribution
        # Parameter closure (fn[...]) is replicated per thread, zero allocation
        algorithm.parallelize(image.height(), fn[row: int]:
            process_row(&mut image[row])
        )

Parameters:
  â€¢ workers=auto: Use all available CPU cores (default)
  â€¢ workers=N: Use exactly N threads
  â€¢ chunk_size=auto: Optimal work distribution (default)
  â€¢ chunk_size=N: Process N items per thread

Example with custom parameters:

    # Use 4 threads, process 100 items per chunk
    # Parameter closure is inlined into each thread's work loop
    algorithm.parallelize[workers=4, chunk_size=100](
        data.len(),
        fn[i: int]:
            data[i] = expensive_computation(data[i])
    )

Safety guarantees:
  â€¢ Parameter closure body must be Send (checked at compile time)
  â€¢ No data races (ownership rules enforced on captures)
  â€¢ Structured concurrency (waits for all threads before returning)
  â€¢ Exceptions in worker threads are caught and propagated
  â€¢ Zero allocation for closure itself (parameter closure is inlined)

Generated code (conceptual):

    let num_workers = 4
    let chunk_size = 100
    let channel = Channel[Result[(), Error]].new()
    
    for worker_id in 0..num_workers:
        Thread.spawn(fn():
            let start = worker_id * chunk_size
            let end = min(start + chunk_size, data.len())
            for i in start..end:
                channel.send(try work_fn(i))
        )
    
    # Wait for all workers, propagate errors
    for _ in 0..num_workers:
        channel.receive()?

Combined: Parallel + SIMD
~~~~~~~~~~~~~~~~~~~~~~~~~~

Algorithmic helpers compose naturally using parameter closures (zero allocation):

    fn process_large_dataset(data: &mut [f32]):
        # Parallel execution across cores
        # Both closures are parameter closures (compile-time, inlined)
        algorithm.parallelize(data.len() / 1000, fn[chunk_id: int]:
            let start = chunk_id * 1000
            let end = start + 1000
            let chunk = &mut data[start..end]
            
            # SIMD within each parallel chunk
            algorithm.vectorize[width=auto](chunk.len(), fn[i: int]:
                chunk[i] = chunk[i].sqrt() * 2.0
            )
        )

This gives both:
  â€¢ Parallelism across CPU cores (thread-level)
  â€¢ Vectorization within each core (SIMD-level)
  â€¢ **Zero allocation:** Both closures are parameter closures (inlined)
  â€¢ **Verifiable:** Works with `quarry build --no-alloc`

Result: Maximum hardware utilization with clear, maintainable code and proven 
zero-cost abstraction.

tile - Cache-Aware Blocking
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The tile helper enables cache-friendly access patterns by processing data in 
blocks that fit in CPU cache (inspired by Mojo's tiling primitives), using 
parameter closures for zero-cost abstraction:

    import std::algorithm
    
    fn matrix_multiply(a: &Matrix, b: &Matrix) -> Matrix:
        const TILE_SIZE = 64  # Fits in L1 cache
        var result = Matrix::zeros(a.rows, b.cols)
        
        # Process in cache-friendly tiles
        # Parameter closure (fn[...]) inlined into tiling loop
        algorithm.tile[block_size=TILE_SIZE](
            a.rows, b.cols,
            fn[i_block: int, j_block: int]:
                # This 64x64 block stays in L1 cache
                for i in i_block..min(i_block + TILE_SIZE, a.rows):
                    for j in j_block..min(j_block + TILE_SIZE, b.cols):
                        result[i, j] = dot_product(&a[i], &b.col(j))
        )
        
        return result

Parameters:
  â€¢ block_size=N: Tile size (typically 32-128 for L1 cache)
  â€¢ block_size=auto: Compiler chooses based on target cache size

Why tiling matters:
  â€¢ L1 cache: ~50 KB, 4 cycles access
  â€¢ L2 cache: ~256 KB, 12 cycles access
  â€¢ L3 cache: ~8 MB, 40 cycles access
  â€¢ Main RAM: Unlimited, 200+ cycles access

Processing 64x64 tiles (32 KB for f64) keeps data in L1 = 50x faster than RAM.

Generated code (conceptual):

    const BLOCK = 64
    for i_block in range(0, rows, BLOCK):
        for j_block in range(0, cols, BLOCK):
            # Inner loops operate on BLOCKÃ—BLOCK tile
            for i in i_block..min(i_block + BLOCK, rows):
                for j in j_block..min(j_block + BLOCK, cols):
                    work_fn(i, j)

Common use cases:
  â€¢ Matrix operations (multiply, transpose, convolution)
  â€¢ Image processing (filters, transforms)
  â€¢ Stencil computations (fluid dynamics, heat transfer)
  â€¢ Any algorithm with O(nÂ²) or O(nÂ³) data access

Integration with profiling:
  â€¢ quarry perf shows cache misses per function
  â€¢ quarry tune suggests: "Add tiling to reduce cache misses"
  â€¢ Estimates improvement: "80% cache miss rate â†’ 5% with tile[64]"

Teaching path:
  1. **Beginner:** Write nested loops, profile shows slow
  2. **Intermediate:** Learn cache hierarchy, apply tile
  3. **Advanced:** Tune block_size based on target CPU
  4. **Expert:** Combine tile + vectorize + parallelize

Example: Matrix multiply speedup
  â€¢ Naive: 100% cache misses, 45 seconds
  â€¢ Tiled [64]: 5% cache misses, 3 seconds (15x faster)
  â€¢ Tiled + SIMD: 1 second (45x faster)
  â€¢ Tiled + SIMD + Parallel: 0.2 seconds (225x faster)

The tile helper is the missing piece between "scalar loops" and "production 
performance" for numeric code. Stable Release feature.

Future: Additional Loop Transforms
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Based on real-world demand, Stable Release may add:

  â€¢ algorithm.unswitch - Move loop-invariant conditionals outside loops
  â€¢ algorithm.fuse - Merge adjacent loops to reduce overhead
  â€¢ algorithm.split - Split loops for better parallelization
  â€¢ algorithm.peel - Peel first/last iterations for alignment

These follow the same philosophy: explicit, composable, inspectable. Only added 
if user feedback demonstrates need. Avoid premature complexity.

Teaching Path for Performance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The algorithmic helpers provide a gentle introduction to high-performance code:

1. **Beginner:** Write scalar loops
   
   for i in 0..data.len():
       data[i] = data[i] * 2.0

2. **Intermediate:** Add vectorize for SIMD
   
   # Parameter closure: fn[i: int] with square brackets (zero-cost)
   algorithm.vectorize[width=auto](data.len(), fn[i: int]:
       data[i] = data[i] * 2.0
   )

3. **Advanced:** Add parallelize for multi-core
   
   # Both parameter closures (nested, still zero allocation)
   algorithm.parallelize(chunks, fn[chunk_id: int]:
       algorithm.vectorize[...](fn[i: int]: ...)
   )

4. **Expert:** Drop to explicit std::simd for fine control
   
   const WIDTH = simd::preferred_width[f32]()
   while i + WIDTH <= len:
       let vec = simd::Vec[f32, WIDTH]::load(&data[i])
       ...

Each step is a natural progression, and the helpers desugar to explicit code 
that can be inspected (quarry expand can show generated code). The parameter 
closure syntax (fn[...]) signals "this is inlined, zero-cost" while runtime 
closures (fn(...)) signal "this may allocate, has runtime representation."

The teaching path for closures:

1. **Week 1-2 (Use without understanding):**
   Just write the body, don't think about closure mechanics
   
       algorithm.vectorize[width=auto](data.len(), fn[i: int]:
           data[i] *= 2.0
       )

2. **Week 3-4 (Learn runtime closures):**
   Understand closures as values for callbacks
   
       let filter = fn(x: int) -> bool: x > 10
       Thread.spawn(fn(): background_work())

3. **Week 5+ (Understand the distinction):**
   Learn that fn[...] is compile-time (free), fn(...) is runtime (may cost)
   
       # Zero-cost (parameter)
       algorithm.vectorize[width=8](n, fn[i: int]: ...)
       
       # May allocate (runtime)
       Thread.spawn(fn(): ...)
   
   See the difference in quarry cost output

4. **Expert (Choose appropriately):**
   Use parameter closures for performance-critical hot paths
   Use runtime closures when flexibility/escape needed

Autotuning: Machine-Specific Optimization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For performance-critical code, optimal parameters (SIMD width, tile size, unroll 
factors) vary by hardware. Pyrite provides autotuning as a TOOL that generates 
code, not runtime magic.

Design Philosophy
~~~~~~~~~~~~~~~~~

Autotuning in Pyrite is explicit and transparent:
  â€¢ NOT runtime adaptation (no overhead)
  â€¢ NOT hidden compiler heuristics (no magic)
  â€¢ YES codegen tool that outputs constants
  â€¢ YES checked-in configuration (reproducible)

This avoids the pitfalls that caused Mojo to deprecate their autotuning system--
Pyrite's approach has zero runtime cost and is fully inspectable.

Command Usage
~~~~~~~~~~~~~

    quarry autotune                     # Tune all autotunable functions
    quarry autotune matrix_multiply     # Tune specific function
    quarry autotune --profile=release   # Tune for release optimizations
    quarry autotune --target=avx512     # Tune for specific CPU features

How It Works
~~~~~~~~~~~~

1. **Identify tunable parameters:** Functions marked @autotune or using 
   algorithm helpers with parameter placeholders

2. **Generate variants:** Compile multiple versions with different parameters

3. **Benchmark:** Run microbenchmarks on target hardware

4. **Select optimal:** Choose parameters with best performance

5. **Generate code:** Output tuned constants to checked-in file

Example: Tuning Matrix Multiply
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Source code with tunable parameters:

    @autotune(params=["TILE_SIZE", "SIMD_WIDTH", "UNROLL_FACTOR"])
    fn matrix_multiply[M: int, N: int, K: int](
        a: &Matrix[M, K],
        b: &Matrix[K, N]
    ) -> Matrix[M, N]:
        # Use tuned parameters (will be generated)
        const TILE_SIZE = tuned::MATRIX_MULTIPLY_TILE_SIZE
        const SIMD_WIDTH = tuned::MATRIX_MULTIPLY_SIMD_WIDTH
        const UNROLL_FACTOR = tuned::MATRIX_MULTIPLY_UNROLL
        
        var result = Matrix[M, N]::zeros()
        
        # Parameter closures (fn[...]) are inlined and unrolled
        algorithm.tile[block_size=TILE_SIZE](M, N, fn[i: int, j: int]:
            algorithm.vectorize[width=SIMD_WIDTH, unroll=UNROLL_FACTOR](
                K, fn[k: int]:
                    result[i, j] += a[i, k] * b[k, j]
            )
        )
        
        return result

Run autotuner:

    $ quarry autotune matrix_multiply
    
    Autotuning: matrix_multiply
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Detected hardware:
      â€¢ CPU: Intel Core i9-12900K
      â€¢ L1 cache: 48 KB per core
      â€¢ L2 cache: 1.25 MB per core
      â€¢ L3 cache: 30 MB shared
      â€¢ SIMD: AVX2 (8-wide f32), AVX-512 (16-wide f32)
    
    Testing parameter combinations:
      â€¢ TILE_SIZE: [32, 64, 128, 256]
      â€¢ SIMD_WIDTH: [4, 8, 16]
      â€¢ UNROLL_FACTOR: [1, 2, 4, 8]
      â€¢ Total: 48 combinations
    
    Benchmarking (matrix size 1024Ã—1024):
    
      [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘] 80% (38/48 combinations tested)
      
      Best so far:
        TILE_SIZE=64, SIMD_WIDTH=8, UNROLL_FACTOR=4
        Time: 234ms (baseline: 892ms, 3.8x faster)
    
    âœ“ Optimal parameters found:
    
      TILE_SIZE = 64
        â€¢ Fits in L1 cache (32 KB for f32)
        â€¢ Minimizes cache misses
      
      SIMD_WIDTH = 8
        â€¢ AVX2 available (8-wide f32 vectors)
        â€¢ Better than AVX-512 for this workload (overhead not justified)
      
      UNROLL_FACTOR = 4
        â€¢ Balance: instruction-level parallelism vs code size
        â€¢ Diminishing returns beyond 4
    
    Performance:
      â€¢ Baseline (naive): 892ms
      â€¢ Tuned: 234ms
      â€¢ Speedup: 3.81x
    
    Generated: src/generated/tuned_params.pyr

Generated File
~~~~~~~~~~~~~~

Autotuner outputs human-readable, checked-in constants:

    # src/generated/tuned_params.pyr
    # AUTO-GENERATED by 'quarry autotune' on 2025-12-18
    # Hardware: Intel Core i9-12900K
    # DO NOT EDIT - Regenerate with 'quarry autotune'
    
    """
    Tuned parameters for optimal performance on target hardware.
    
    Benchmark results:
      â€¢ matrix_multiply: 234ms (3.81x faster than baseline)
      â€¢ Tested: 48 parameter combinations
      â€¢ Hardware: i9-12900K, AVX2, 48 KB L1
    """
    
    # Matrix multiplication tuning
    const MATRIX_MULTIPLY_TILE_SIZE: int = 64
    const MATRIX_MULTIPLY_SIMD_WIDTH: int = 8
    const MATRIX_MULTIPLY_UNROLL: int = 4
    
    # Image processing tuning
    const BLUR_KERNEL_TILE_SIZE: int = 128
    const BLUR_SIMD_WIDTH: int = 8
    
    # ... more tuned constants ...

Application code imports these:

    import generated::tuned
    
    fn matrix_multiply[M: int, N: int, K: int](...):
        const TILE_SIZE = tuned::MATRIX_MULTIPLY_TILE_SIZE  # 64
        # ... use tuned constant ...

Per-Platform Tuning
~~~~~~~~~~~~~~~~~~~

Different platforms need different parameters:

    # Tune for multiple targets
    quarry autotune --target=x86_64-linux
    quarry autotune --target=aarch64-linux
    quarry autotune --target=riscv64-linux

Generated files:

    src/generated/tuned_x86_64.pyr
    src/generated/tuned_aarch64.pyr
    src/generated/tuned_riscv64.pyr

Conditional compilation selects appropriate file:

    @cfg(target = "x86_64")
    import generated::tuned_x86_64 as tuned
    
    @cfg(target = "aarch64")
    import generated::tuned_aarch64 as tuned

Benefits of Tool-Based Autotuning
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Compared to runtime autotuning (Mojo's original approach, now deprecated):

Pyrite's tool-based approach:
  âœ“ Zero runtime cost (constants compiled in)
  âœ“ Fully inspectable (generated file is readable)
  âœ“ Reproducible (checked-in file, version controlled)
  âœ“ No hidden behavior (explicit import of tuned::*)
  âœ“ CI-friendly (deterministic builds)
  âœ“ Cross-compilation safe (tune per target)

Runtime autotuning pitfalls (why Mojo abandoned it):
  âœ— Runtime overhead (measure and adapt on every run)
  âœ— Non-deterministic (results vary by machine)
  âœ— Hidden behavior (parameters chosen invisibly)
  âœ— CI complexity (different results on different machines)
  âœ— Debugging nightmare (works on my machine!)

When to Use Autotuning
~~~~~~~~~~~~~~~~~~~~~~~

Use quarry autotune for:
  â€¢ Performance-critical kernels (hot paths)
  â€¢ Numeric code with hardware-dependent optimal parameters
  â€¢ Code shipping to diverse hardware (tune per target)
  â€¢ Libraries where 10-50% improvement justifies tuning effort

Don't use for:
  â€¢ Non-critical code (manual tuning good enough)
  â€¢ Code that's not performance-bottleneck
  â€¢ First optimization pass (profile first, tune later)

Workflow Integration
~~~~~~~~~~~~~~~~~~~~

Autotuning fits naturally into development:

1. **Develop:** Write code with algorithm.tile[block_size=64]
2. **Profile:** quarry perf shows it's a hot spot
3. **Autotune:** quarry autotune finds TILE_SIZE=128 is 20% faster
4. **Apply:** Import tuned constants, regenerate for production hardware
5. **CI:** Run quarry autotune in CI to detect performance regressions

Example production workflow:

    # Development: Use reasonable defaults
    const TILE_SIZE = 64
    
    # Production: Use tuned values
    const TILE_SIZE = tuned::MATRIX_MULTIPLY_TILE_SIZE
    
    # CI: Regenerate and verify
    quarry autotune --check  # Fail if tuned values are stale

Cost Transparency
~~~~~~~~~~~~~~~~~

Autotuning maintains Pyrite's explicitness:

    $ quarry cost --show-tuning
    
    Tuned Parameters in Use:
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    src/math.py:234 - matrix_multiply
      â€¢ TILE_SIZE: 64 (from tuned::MATRIX_MULTIPLY_TILE_SIZE)
      â€¢ Source: src/generated/tuned_params.pyr:15
      â€¢ Tuned on: 2025-12-18 (Intel i9-12900K)
      â€¢ Speedup vs baseline: 3.81x
    
    âš ï¸  Warning: Tuned for x86_64, running on aarch64
        Performance may be suboptimal
        Run 'quarry autotune --target=aarch64' to re-tune

This makes tuning decisions visible and auditable - no hidden magic.

Why This Approach Wins
~~~~~~~~~~~~~~~~~~~~~~~

Pyrite's tool-based autotuning provides:
  â€¢ Mojo's performance benefits (tuned parameters for hardware)
  â€¢ None of Mojo's runtime overhead (zero cost at runtime)
  â€¢ Full transparency (generated file is version controlled)
  â€¢ Reproducible builds (CI and prod use same tuned values)

This is "autotuning done right" - as a build tool, not language semantics.

CI Verification and Perf.lock Integration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Autotuning integrates with the performance lockfile (Perf.lock) to prevent 
regressions and ensure tuned parameters remain optimal:

    quarry autotune --verify                # Check tuned params vs current hardware
    quarry autotune --check                 # Fail CI if params are stale
    quarry autotune --update-lockfile       # Update Perf.lock with new params

Workflow with performance lockfile:

1. **Initial tuning:** Generate optimal parameters
   
       $ quarry autotune
       Generated: src/generated/tuned_params.pyr
       
       Optimal parameters found:
         â€¢ MATRIX_MULTIPLY_TILE_SIZE = 64
         â€¢ MATRIX_MULTIPLY_SIMD_WIDTH = 8
         â€¢ Speedup: 3.81x vs baseline

2. **Commit to version control:** Tuned parameters become reproducible baseline
   
       $ git add src/generated/tuned_params.pyr Perf.lock
       $ git commit --m "Add autotuned parameters for matrix_multiply"

3. **CI verification:** Ensure parameters remain valid on deployment hardware
   
       $ quarry autotune --verify
       
       Verifying tuned parameters on current hardware...
       
       âœ“ MATRIX_MULTIPLY_TILE_SIZE=64 validated (64 is still optimal)
       âœ“ MATRIX_MULTIPLY_SIMD_WIDTH=8 validated (AVX2 available)
       âœ“ Performance matches baseline: 234ms (3.81x)
       
       All tuned parameters remain optimal

4. **Detect staleness:** Warn if hardware changed significantly
   
       $ quarry autotune --verify
       
       âš ï¸  WARNING: Tuned parameters may be suboptimal
       
       Hardware mismatch:
         â€¢ Tuned for: Intel i9-12900K (AVX2, 48KB L1)
         â€¢ Running on: AMD Ryzen 9 7950X (AVX-512, 32KB L1)
       
       Benchmark results:
         â€¢ matrix_multiply: 189ms (was 234ms on tuned hardware)
         â€¢ Speedup vs baseline: 4.72x (better than original!)
       
       Recommendation:
         Hardware improved - consider re-tuning:
           quarry autotune --update-lockfile
         
         Expected improvements:
           â€¢ SIMD_WIDTH: 8 â†’ 16 (AVX-512 available)
           â€¢ Potential: 5.5x speedup (vs current 4.72x)

Integration with Perf.lock Format
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Autotuned parameters are recorded in Perf.lock alongside performance baselines:

    # Perf.lock (YAML format)
    version: "1.0"
    generated: "2025-12-18T10:30:00Z"
    build: "--release --lto=thin"
    platform: "x86_64-linux (Intel i9-12900K)"
    
    autotuned_parameters:
      matrix_multiply:
        tile_size: 64
        simd_width: 8
        unroll_factor: 4
        benchmark_time_ms: 234
        speedup_vs_baseline: 3.81
        tested_combinations: 48
        tuning_date: "2025-12-18T10:30:00Z"
      
      blur_kernel:
        tile_size: 128
        simd_width: 8
        benchmark_time_ms: 89
        speedup_vs_baseline: 2.34
        tested_combinations: 32
    
    hot_functions:
      - name: "process_data"
        time_ms: 1247
        uses_autotuned: true
        autotuned_params: ["matrix_multiply"]
        # ... rest of Perf.lock content ...

CI Pipeline Example
~~~~~~~~~~~~~~~~~~~

Complete CI workflow combining autotuning verification with performance regression 
detection:

    # .github/workflows/ci.yml
    name: Performance CI
    
    jobs:
      performance:
        steps:
          - name: Verify autotuned parameters
            run: quarry autotune --verify
            # Warns if hardware mismatch, fails if incompatible
          
          - name: Check performance baseline
            run: quarry perf --check --threshold=5%
            # Fails if any hot function regressed >5%
          
          - name: Re-tune if needed (on schedule)
            if: github.event.schedule  # Weekly scheduled run
            run: |
              quarry autotune
              quarry perf --baseline --update
              # Commit updated Perf.lock if improvements found

This ensures:
  â€¢ Autotuned parameters remain optimal (warn on hardware changes)
  â€¢ Performance doesn't regress (Perf.lock enforcement)
  â€¢ Parameters stay fresh (periodic re-tuning in CI)
  â€¢ Both checked into VCS (reproducible, reviewable)

Benefits of Combined Workflow
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Autotuning + Performance Lockfile together provide complete performance governance:

**Autotuning provides:** Machine-optimal parameters for hot functions
**Perf.lock provides:** Regression detection for all functions
**Together:** Optimal performance that stays optimal forever

Example impact:
  â€¢ Autotuning: 3.8x speedup from tuned parameters
  â€¢ Perf.lock: Catches when future changes break tuning assumptions
  â€¢ Result: Performance gains are permanent, not accidental

Without integration:
  - Tune parameters â†’ merge â†’ later commit breaks assumptions â†’ silent regression
  - Parameters chosen for old hardware â†’ deploy on new hardware â†’ suboptimal

With integration:
  - quarry autotune --verify warns: "Tuned for AVX2, deployed with AVX-512 available"
  - quarry perf --check fails: "matrix_multiply regressed 18% (SIMD width dropped)"
  - CI prevents both problems before production

This completes the "reproducible path to maximum performance" that Pyrite 
promises-tuning + verification + enforcement.

Implementation: Stable Release (after algorithmic helpers and profiling are mature)
           Requires: Stable benchmark framework, platform introspection, Perf.lock integration

Performance Cookbook: Stdlib as Learning Resource
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Beyond providing fast implementations, Pyrite's standard library serves as an 
interactive performance education system. Each stdlib algorithm becomes a teaching 
tool that demonstrates "why it's fast" with concrete, measurable examples.

Canonical Implementations with Explanations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Every performance-critical stdlib function includes:

1. **Documented performance characteristics:**
   
       """
       Sorts a slice in-place using pattern-defeating quicksort.
       
       Time complexity: O(n log n) average and worst-case
       Space complexity: O(log n) stack for recursion
       Allocations: 0 (sorts in-place, uses stack recursion)
       
       Performance:
         â€¢ 100 elements: ~2 microseconds
         â€¢ 10,000 elements: ~400 microseconds  
         â€¢ 1,000,000 elements: ~60 milliseconds
       
       Why it's fast:
         â€¢ Hybrid algorithm (quicksort + heapsort + insertion sort)
         â€¢ Branch prediction friendly (pattern-defeating)
         â€¢ Cache-aware partitioning
         â€¢ SIMD for small comparisons where applicable
       
       Benchmark: Run 'quarry bench std::sort' to verify on your hardware
       """
       fn sort[T: Ord](slice: &mut [T]):
           ...

2. **Inline performance notes in code:**
   
       fn sort[T: Ord](slice: &mut [T]):
           if slice.len() < 20:
               # Small arrays: insertion sort is fastest
               # Why: No branch mispredictions, excellent cache locality
               insertion_sort(slice)
           else:
               # Large arrays: pattern-defeating quicksort
               # Why: O(n log n) worst-case, cache-friendly partitioning
               pdqsort(slice)

3. **Comparison with alternatives:**
   
       """
       Alternative sorting algorithms in std::
       
       â€¢ sort() - General-purpose, fastest for most cases
         Use when: Default choice, no special requirements
         
       â€¢ sort_unstable() - May reorder equal elements, ~10% faster
         Use when: Don't care about stability, need maximum speed
         
       â€¢ sort_by_key() - Sort by extracted key, avoids comparison overhead
         Use when: Comparing complex types, key extraction is cheap
       
       Choosing the right algorithm:
         Run 'quarry bench --compare sort,sort_unstable,sort_by_key'
         to see performance on your workload
       """

Built-in Benchmark Harness
~~~~~~~~~~~~~~~~~~~~~~~~~~

Every performance-critical stdlib function has associated benchmarks:

    quarry bench std::sort                    # Benchmark sort implementation
    quarry bench std::map::insert             # Benchmark map insertion
    quarry bench std::json::parse             # Benchmark JSON parser
    
    # Compare implementations
    quarry bench --compare std::sort,std::sort_unstable
    
    # Profile specific workload
    quarry bench std::sort --size=10000       # Custom input size

Output shows concrete performance on user's hardware:

    $ quarry bench std::sort
    
    Benchmarking: std::sort
    =======================
    
    Hardware: Intel Core i9-12900K, 48KB L1, 1.25MB L2
    
    Results (sorted Vec<i32>):
      100 elements:      1.8 Î¼s  (55 ns/element)
      1,000 elements:    28 Î¼s   (28 ns/element)
      10,000 elements:   380 Î¼s  (38 ns/element)
      100,000 elements:  5.2 ms  (52 ns/element)
    
    Scaling: O(n log n) confirmed
    
    Memory:
      â€¢ Peak allocation: 0 bytes (in-place sort)
      â€¢ Stack usage: 892 bytes (recursion depth)
    
    Optimizations observed:
      â€¢ SIMD comparisons: AVX2 8-wide for integer types
      â€¢ Branch prediction: 94% accuracy (pattern-defeating)
      â€¢ Cache misses: 2.3% (excellent locality)
    
    Comparison with std::sort_unstable:
      â€¢ sort_unstable: 8% faster (340 Î¼s for 10k elements)
      â€¢ Trade-off: May reorder equal elements
      â€¢ Use when: Stability not required

This output teaches:
  â€¢ What performance to expect (not abstract O(n log n), but "380 Î¼s for 10k ints")
  â€¢ Why it's fast (SIMD, branch prediction, cache locality)
  â€¢ When to choose alternatives (sort_unstable for 8% speedup)

Canonical Examples with quarry cost Output
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Documentation includes complete examples with cost analysis:

    """
    Example: Efficient JSON parsing
    ================================
    
    This example demonstrates optimal JSON parsing patterns.
    """
    
    fn parse_config_file(path: &str) -> Result[Config, Error]:
        # Read entire file (one syscall, one allocation)
        let contents = try File.read_to_string(path)
        
        # Parse JSON (zero additional allocations with pre-sized arena)
        let json = try json::parse(&contents)
        
        # Extract config (moves data, zero copies)
        return Config::from_json(json)
    
    """
    Cost analysis (quarry cost):
      Allocations: 2 total
        â€¢ Line 7: File buffer (~4KB typical)
        â€¢ Line 10: JSON parse arena (size based on content)
      
      Copies: 0 (all moves and references)
      
      Syscalls: 1 (file read)
      
      Performance: ~500 Î¼s for typical 4KB config file
      
    Why it's fast:
      â€¢ Single allocation for file read (buffered)
      â€¢ Zero-copy JSON parsing (references string slices)
      â€¢ Move semantics avoid copies
      â€¢ Predictable memory usage
    
    Run 'quarry cost example.pyr' to analyze on your code
    """

Interactive Performance Learning
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

quarry learn includes performance-specific exercises:

    $ quarry learn performance
    
    ============================================================
    Performance Exercise: Optimize Hot Loop
    ============================================================
    
    The code below allocates in a loop. Optimize it:
    
    fn process_lines(lines: &[&str]) -> Vec<String>:
        var result = Vec[String].new()
        for line in lines:
            let processed = String.new()  # Allocates every iteration!
            processed.push_str("prefix: ")
            processed.push_str(line)
            result.push(processed)
        return result
    
    Tasks:
      1. Identify the allocation in the loop
      2. Pre-allocate to avoid reallocation
      3. Measure improvement with quarry perf
    
    Target: < 10 allocations for 1000 lines
    
    Hint: Use String::with_capacity() and result.with_capacity()
    [Try] [Hint] [Solution]

Cross-Linked Documentation
~~~~~~~~~~~~~~~~~~~~~~~~~~

Performance patterns reference each other:

    """
    std::algorithm::vectorize
    
    See also:
      â€¢ std::simd - Explicit SIMD operations (lower-level)
      â€¢ std::algorithm::parallelize - Combine for multi-core + SIMD
      â€¢ std::algorithm::tile - Add cache-awareness
      
    Examples:
      â€¢ Image processing: docs/examples/image_blur.pyr
      â€¢ Matrix operations: docs/examples/matrix_multiply.pyr  
      â€¢ Audio processing: docs/examples/fir_filter.pyr
    
    Each example includes:
      â€¢ Complete runnable code
      â€¢ quarry cost output (static analysis)
      â€¢ quarry perf output (runtime profiling)
      â€¢ Explanation of optimizations
      â€¢ Benchmark comparisons
    """

Cookbook Repository
~~~~~~~~~~~~~~~~~~~

Official performance cookbook (docs/cookbook/) with canonical implementations:

    docs/cookbook/
      â”œâ”€â”€ algorithms/
      â”‚   â”œâ”€â”€ matrix_multiply.pyr      # Tiled + SIMD + parallel
      â”‚   â”œâ”€â”€ fft.pyr                   # Fast Fourier transform
      â”‚   â””â”€â”€ sort_variants.pyr         # Comparison of sort algorithms
      â”œâ”€â”€ data_structures/
      â”‚   â”œâ”€â”€ cache_friendly_list.pyr   # Memory layout optimization
      â”‚   â”œâ”€â”€ lock_free_queue.pyr       # Concurrent data structure
      â”‚   â””â”€â”€ small_vec_usage.pyr       # Inline storage patterns
      â”œâ”€â”€ io/
      â”‚   â”œâ”€â”€ zero_copy_parsing.pyr     # Avoid allocation in parsers
      â”‚   â”œâ”€â”€ buffered_io.pyr           # Efficient file operations
      â”‚   â””â”€â”€ memory_mapped.pyr         # Memory-mapped file I/O
      â””â”€â”€ numerical/
          â”œâ”€â”€ dot_product.pyr           # SIMD inner product
          â”œâ”€â”€ convolution.pyr           # Cache-aware 2D convolution
          â””â”€â”€ tensor_ops.pyr            # Efficient tensor operations

Each cookbook entry:
  â€¢ Self-contained, runnable code
  â€¢ quarry cost output showing zero or minimal allocations
  â€¢ quarry perf benchmarks with expected performance
  â€¢ Explanation: "This is fast because..."
  â€¢ Alternatives: "Use X if Y condition"
  â€¢ Quiz: "Modify this to handle Z case"

Why Performance Cookbook Matters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This transforms the stdlib from "collection of functions" to "performance 
education system":

1. **Concrete examples beat abstract advice:**
   "See how matrix multiply achieves 15x speedup through tiling" is more valuable 
   than "consider cache locality"

2. **Verifiable performance claims:**
   Every claim ("this is zero-allocation") backed by quarry cost output that 
   readers can reproduce

3. **Benchmark-driven learning:**
   Run the benchmarks, see the numbers, understand why each optimization matters

4. **Canonical patterns:**
   No need to wonder "what's the fast way to parse JSON?" - the cookbook shows the 
   blessed implementation with explanations

5. **Performance becomes accessible:**
   Intermediate developers can copy proven patterns without deep expertise

Impact on Adoption
~~~~~~~~~~~~~~~~~~

The performance cookbook addresses the common complaint: "I know the language is 
fast, but I don't know how to make MY code fast."

With cookbook:
  â€¢ "Copy the pattern from docs/cookbook/matrix_multiply.pyr"
  â€¢ "Run quarry bench to verify on my hardware"
  â€¢ "Understand WHY it's fast from inline comments"
  â€¢ "Adapt the pattern to my problem"

This operationalizes "performance is possible" into "performance is the default" 
by providing concrete, proven templates.

Implementation: Stable Release
  â€¢ Initial: Core cookbook entries (10-15 canonical examples)
  â€¢ Expansion: 50+ examples covering all performance domains
  â€¢ Ongoing: Community contributions, benchmarked and reviewed

Cost Transparency Integration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Algorithmic helpers remain transparent, and the two-tier closure model makes 
cost analysis precise:

  â€¢ quarry cost shows:
    - "vectorize generates SIMD loop (width=8, closure inlined)"
    - "parallelize spawns 4 threads (parameter closure, zero allocation)"
    - "Thread::spawn allocates closure environment (32 bytes)"
  
  â€¢ quarry explain-type on closures distinguishes types:
    - Parameter closure: "Compile-time only, zero bytes, always inlined"
    - Runtime closure: "16 bytes + environment, may allocate, can escape"
  
  â€¢ Compiler warnings for inefficient usage:
    - "vectorize width=8 but array length=3 (no benefit)"
    - "parallelize workers=8 but only 100 items (overhead > benefit)"
    - "runtime closure in hot path (consider parameter closure)"

The helpers are ergonomic without being magical. The two-tier closure model makes 
cost explicit: parameter closures (fn[...]) are provably zero-cost, runtime 
closures (fn(...)) show their allocation in quarry cost. This maintains Pyrite's 
transparency principles while enabling powerful abstractions.

Why This Matters
~~~~~~~~~~~~~~~~~

Mojo demonstrated that algorithmic helpers are the "killer feature" for 
performance-oriented languages. They provide:

  â€¢ **Approachability:** Scalar logic â†’ SIMD performance
  â€¢ **Composability:** vectorize + parallelize = both benefits
  â€¢ **Safety:** Compiler enforces correctness (no data races)
  â€¢ **Transparency:** Still explicit, inspectable, understandable

Pyrite adopts this proven approach while maintaining its core philosophy: 
explicit is better than implicit, but ergonomic is better than tedious.

9.13 GPU Computing (Stable Release)
--------------------------------------------------------------------------------

For GPU-accelerated computing, Pyrite extends its performance contract system 
to heterogeneous computing with a kernel programming model that maintains the 
language's core philosophy: explicit, safe by default, with compiler-verified 
guarantees.

Design Philosophy
~~~~~~~~~~~~~~~~~

Pyrite's GPU support is designed around contracts and blame tracking - the same 
system that makes @noalloc/@cost_budget powerful on CPU extends naturally to 
GPU constraints.

Key principles:
  â€¢ Explicit kernel boundaries (no automatic offloading)
  â€¢ Contract-based restrictions (@kernel implies @noalloc, @no_panic, etc.)
  â€¢ Call-graph blame tracking shows why code can't run on GPU
  â€¢ Single-source: write once, target CPU or GPU with same safety guarantees

Kernel Programming Model
~~~~~~~~~~~~~~~~~~~~~~~~

The @kernel attribute marks functions eligible for GPU execution:

    @kernel
    fn saxpy[N: int](a: f32, x: &[f32; N], y: &mut [f32; N]):
        let idx = gpu::thread_id()
        if idx < N:
            y[idx] = a * x[idx] + y[idx]

Kernel contract (automatically enforced):
  â€¢ @noalloc - No heap allocation (GPU has no allocator)
  â€¢ @no_panic - No panic/abort (GPU can't print or terminate gracefully)
  â€¢ @no_recursion - No recursion (limited GPU stack)
  â€¢ @no_syscall - No system calls (GPU has no OS)

The compiler enforces these transitively - any function called from a kernel must 
also satisfy kernel contracts.

Call-Graph Blame for GPU
~~~~~~~~~~~~~~~~~~~~~~~~~

When kernel constraints are violated, blame tracking shows exactly why:

    @kernel
    fn process_data(input: &[f32], output: &mut [f32]):
        let result = expensive_compute(input[0])
        output[0] = result
    
    error[P0701]: kernel contract violation: heap allocation
      ----> src/gpu.py:45:18
       |
    43 | @kernel
    44 | fn process_data(input: &[f32], output: &mut [f32]):
    45 |     let result = expensive_compute(input[0])
       |                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^ allocation in kernel
       |
       = note: Call chain:
         1. process_data() [marked @kernel]
            â†’ calls expensive_compute()
         2. expensive_compute() at math.py:234
            â†’ allocates Vec[f64] for intermediate results [VIOLATES @kernel]
       
       = help: To fix this kernel violation:
         1. Rewrite expensive_compute to use fixed-size arrays:
            fn expensive_compute_kernel(input: f32, buffer: &mut [f64; 100]) -> f64
         
         2. Or move allocation to host side:
            let buffer = vec![0.0; 100]  # Host allocation
            gpu::launch(process_data, &input, &mut output, &buffer)
       
       = explain: Run 'pyritec --explain P0701' for kernel programming guide

This makes GPU programming teachable - the compiler explains *why* code can't run 
on GPU and *how* to fix it.

Multi-Backend Support
~~~~~~~~~~~~~~~~~~~~~

Pyrite targets multiple GPU APIs through a unified interface:

    # Compile for specific GPU backend
    quarry build --gpu=cuda      # NVIDIA GPUs (CUDA/PTX)
    quarry build --gpu=hip       # AMD GPUs (HIP/ROCm)
    quarry build --gpu=metal     # Apple GPUs (Metal)
    quarry build --gpu=vulkan    # Cross-vendor (Vulkan compute)

Backend selection is compile-time, not runtime. Single-source code targets 
different GPUs without API-specific code.

Launch API
~~~~~~~~~~

Explicit kernel launch from host code:

    import std::gpu
    
    fn main():
        let n = 1_000_000
        let a = 2.5
        let x = vec![1.0; n]
        let mut y = vec![2.0; n]
        
        # Launch kernel on GPU
        gpu::launch[threads=n, blocks=256](
            saxpy[n],
            a,
            &x,
            &mut y
        )
        
        # Synchronize (wait for GPU completion)
        gpu::sync()
        
        print("Result:", y[0])

Launch parameters:
  â€¢ threads: Total work items (like CUDA threads or OpenCL work-items)
  â€¢ blocks: Parallelism hint (GPU groups threads into blocks)
  â€¢ Compiler generates optimal grid/block configuration per backend

Memory Management
~~~~~~~~~~~~~~~~~

Explicit memory movement between host and device:

    # Allocate on device
    let d_data = gpu::alloc[f32](1000)?
    
    # Copy host â†’ device
    gpu::copy_to_device(&h_data, &d_data)
    
    # Launch kernel
    gpu::launch[threads=1000](process, &d_data)
    
    # Copy device â†’ host
    gpu::copy_to_host(&d_data, &mut h_data)
    
    # Free device memory
    gpu::free(d_data)

Or use RAII wrappers for automatic cleanup:

    let d_data = gpu::DeviceVec::from_host(&h_data)?
    gpu::launch[threads=1000](process, &d_data)
    let result = d_data.to_host()
    # Device memory freed automatically

Integration with Type System
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Device pointers are distinct types:

    type HostPtr[T] = &[T]       # Host memory
    type DevicePtr[T] = gpu::Ptr[T]  # Device memory
    
    @kernel
    fn kernel_func(data: DevicePtr[f32]):
        # Can only access device memory in kernels
    
    fn host_func(data: HostPtr[f32]):
        # Cannot pass host pointer to kernel directly
        # Must explicitly copy to device first

This prevents the common bug of passing host pointers to GPU kernels.

Why This Approach Wins
~~~~~~~~~~~~~~~~~~~~~~

Pyrite's GPU model differentiates through its contract system:

1. **Teachability:** Blame tracking explains GPU restrictions clearly
2. **Safety:** Type system prevents host/device pointer confusion
3. **Composability:** Same contracts work on CPU and GPU
4. **Debuggability:** Know exactly why code can't run on GPU
5. **Simplicity:** One language for CPU and GPU (not separate dialects)

Comparison to other approaches:
  â€¢ CUDA/HIP: Separate compilation, easy to mix host/device pointers
  â€¢ OpenCL: Verbose, runtime compilation, weak type safety
  â€¢ Mojo: Similar goals but less explicit about constraints

Pyrite makes GPU programming accessible while maintaining systems-level control.

Implementation Timeline
~~~~~~~~~~~~~~~~~~~~~~~

  â€¢ Stable Release: Design and specify kernel contracts + blame tracking
  â€¢ Initial: CUDA backend (NVIDIA GPUs, 80% market share)
  â€¢ Expansion: HIP (AMD), Metal (Apple), Vulkan (cross-vendor)

Rationale for late phase: GPU is a powerful differentiator, but requires 
stable CPU-side language first. Get ownership, contracts, and blame tracking 
rock-solid before adding GPU complexity.

This positions Pyrite as: "The systems language that scales from embedded to 
GPU, with the same safety guarantees everywhere."

9.14 Why "Batteries Included" Matters
--------------------------------------------------------------------------------

Developer Adoption Barrier
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Languages are often evaluated by: "Can I build X without hunting for 20 
dependencies?" 

Pyrite's comprehensive stdlib means:
  â€¢ CLI tool? âœ“ (args, file I/O, process)
  â€¢ Web scraper? âœ“ (HTTP client, regex, JSON)
  â€¢ Game? âœ“ (math, time, collections)
  â€¢ API service? âœ“ (HTTP server, JSON, database TBD)

First impressions matter. A newcomer building their first project shouldn't 
spend hours finding, vetting, and integrating dependencies for basic tasks.

Quality and Consistency
~~~~~~~~~~~~~~~~~~~~~~~~

Standard library advantages:
  â€¢ Single quality standard
  â€¢ Consistent error handling (all use Result)
  â€¢ Coherent documentation
  â€¢ Security auditing
  â€¢ Performance optimization
  â€¢ Long-term stability (semantic versioning)

Third-party ecosystem complements stdlib, doesn't replace fundamentals.

Zero-cost Abstractions
~~~~~~~~~~~~~~~~~~~~~~~

The stdlib APIs are designed not to introduce overhead beyond the cost of the 
operations they perform. For example, a sorting function in the library should be 
as efficient as one you would write by hand for a specific type. It will likely 
be generic (sorting any List[T] where T is Comparable), and the compiler will 
optimize it for the concrete type you use. Data structures like List and Map are 
implemented with performance comparable to their C++ or Java counterparts, but 
with safety (no buffer overruns, etc.).

No Hidden Allocations
~~~~~~~~~~~~~~~~~~~~~~

The standard library follows the language's ethos of not hiding costly operations. 
If a function allocates memory or spawns a thread, it's documented and often 
reflected in the API. For instance, pushing an element onto a List that has 
reached capacity will cause a reallocation of the internal array - this is an 
O(n) operation occasionally. The List API would document that append may realloc 
and that you can call reserve(n) to preallocate space if needed to ensure 
amortized constant time appends. By making this explicit, programmers can reason 
about performance. 

In many cases, the library design will require the caller to provide an allocator 
or pick a strategy so that it's never implicit. For example, if you create a 
large String by concatenation, that will allocate new memory; the library might 
provide a StringBuilder to make this efficient and clear. In short, if something 
is potentially expensive, the stdlib will either require you to opt-in (e.g. 
provide an allocator or call a special method) or will be clearly documented to 
avoid surprises.

Memory Safety
~~~~~~~~~~~~~

The standard library is written with the same safe-by-default approach. 
Internally, it might use some unsafe code to achieve performance (for example, 
the internal implementation of List might use pointer arithmetic to move elements 
during reallocation), but this is encapsulated. The public API of List is safe - 
you can't cause a buffer overflow by using it as documented. If a bug is present, 
it's in the stdlib implementation and can be fixed, but your application logic 
remains safe. 

The standard library uses the type system to enforce correctness, e.g., returning 
an Optional when a lookup might fail (instead of a null or an uninitialized 
value). Errors in I/O are returned as Result types, forcing the user to handle 
them.

Error Handling in stdlib
~~~~~~~~~~~~~~~~~~~~~~~~~

As mentioned, functions like opening files will return Result<File, IOError>. 
This encourages robust error handling in user code. If you forget to check an 
error, the compiler will warn that a Result value is unused. This is safer than 
ignoring return codes in C (a very common source of bugs). In Pyrite, it's harder 
to accidentally ignore an error because the type system nudges you.

Global Allocator and Custom Allocators
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pyrite will likely have a concept of a global default heap allocator (like 
malloc/free under the hood). The stdlib collections by default use this global 
allocator for simplicity. However, for more control, many data structure APIs 
allow passing a custom allocator. For instance, you could create a List with a 
specific allocator (say an arena allocator for many short-lived allocations). 

This pattern is borrowed from Zig, where most functions that allocate take an 
Allocator parameter. Pyrite may not require it on every single call (to keep 
basic usage simple), but it will provide the hooks for when you need them. This 
means you can use Pyrite's stdlib in OS/kernel or embedded contexts by providing 
a no-op or custom allocator, or avoid heap usage entirely if you never call those 
parts (thanks to compile-time, unused code can be eliminated).

Freestanding Support
~~~~~~~~~~~~~~~~~~~~

The standard library is likely split into layers, e.g., a core library that 
doesn't depend on OS (for use in kernels, embedded) and a full library with I/O 
and threads that requires OS support. Rust does this with core vs std. Zig 
similarly has an optional std. Pyrite wants to allow writing code with no stdlib 
if needed (like an OS), or linking only a minimal subset. This is in line with 
providing first-class support for no runtime. If you don't use something, it 
should not be in the binary. For example, a bare-metal program might use 
collections and math from the core library but not use file I/O or threads at 
all.

C Interop in stdlib
~~~~~~~~~~~~~~~~~~~

The stdlib will likely wrap or interface with common C APIs for things like file 
I/O (maybe using OS system calls or C's libc under the hood) and networking 
(sockets). Because Pyrite can call C easily, the implementation can lean on 
proven C libraries where appropriate but present a safer, modern interface.

Concurrency Primitives
~~~~~~~~~~~~~~~~~~~~~~

(Expanding on this important part of the stdlib in its own subsection below.)

9.15 Concurrency and Multithreading
--------------------------------------------------------------------------------

Pyrite is built to support concurrent and multi-threaded programming, as these 
are essential for modern software (from servers handling many requests to 
applications with background tasks). The language's memory safety extends to 
concurrency by statically preventing data races. Here's how Pyrite handles 
concurrency:

Threads
~~~~~~~

The stdlib provides a high-level API for threading. For example, there might be a 
Thread.spawn(fn) function or similar that takes a function or closure and runs it 
in a new OS thread. 

E.g.:

    Thread.spawn(fn():
        do_work()
    )

This would spawn a new thread to execute the do_work() function in parallel. 
Under the hood, this uses the OS threading API (like pthread_create on POSIX or 
CreateThread on Windows). The Thread API likely returns a handle or joiner that 
you can use to wait for the thread to finish or detach it.

Closure Capture and 'static Requirement
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you spawn a thread with a closure that captures variables from outside, the 
compiler ensures that either those variables are global or heap-allocated so that 
they outlive the new thread, or it forces you to clone/move them into the thread. 
This prevents scenarios like launching a thread that references a stack variable 
which goes out of scope.

For instance:

    var message = "Hello"
    Thread.spawn(fn():
        print(message)
    )

In this example, message is on the parent thread's stack. Pyrite's compiler would 
forbid capturing it by reference because the new thread could outlive the stack 
frame. It would likely demand that message be moved or be a 'static value. The 
user could do: 

    Thread.spawn(fn(msg=message):
        print(msg)
    )

(Imagining a syntax where we explicitly move message into the closure by value, 
making a copy of the string for the thread.) The compiler might also automatically 
move it if message is owned and not used after, similar to Rust's move closures. 
The key is, the compiler checks and prevents you from accidentally sharing 
something unsafe to share.

Data Race Prevention
~~~~~~~~~~~~~~~~~~~~

Pyrite will likely adopt Rust's concept of Send and Sync traits for types to 
indicate thread-safety. By default, owning a type in one thread and sending it to 
another requires that the type is Send (meaning it has no internal pointer or 
state that could be touched unsafely across threads). Most basic types (ints, 
structs of sendable fields, etc.) are Send. Types like Rc (reference-counted 
pointer without atomic) would not be Send because increasing/decreasing ref count 
from multiple threads would be unsafe. Similarly, having a raw pointer might not 
be Send unless you explicitly mark it. 

These traits (if implemented similar to Rust) are auto-derived by the compiler in 
most cases, or require unsafe impl when appropriate. When you attempt to share 
something between threads (either by moving it to a thread or by using a global 
that multiple threads might access), the compiler will enforce that only 
thread-safe types cross those boundaries.

Shared Mutable State and Synchronization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In safe Pyrite, you cannot have unsynchronized mutable access to shared data. If 
you want two threads to share an object, you typically would use a 
synchronization primitive from the stdlib:

â€¢ Mutex (mutual exclusion lock): e.g., a Mutex[T] that wraps a T and ensures 
  only one thread can access it at a time. The Mutex's API would provide a method 
  to lock and get a &mut T (or some guard object) and automatically unlock on 
  scope exit (RAII style).

â€¢ Atomic types: For simple cases like counters or flags, the stdlib will offer 
  atomic integers, atomic booleans, etc., which can be safely shared and updated 
  without a full mutex, using low-level atomic CPU instructions. Pyrite can wrap 
  C11 atomics or use inline assembly for this.

â€¢ Channels: A channel is a safe communication queue between threads. For instance, 
  Channel[T] might have a tx (sender) and rx (receiver) handle. One thread can 
  send values of type T into the channel, another can receive them. This is a 
  message-passing concurrency model that avoids sharing data by transferring 
  ownership through the channel. Pyrite's channel could be bounded or unbounded, 
  and internally uses locks or lock-free algorithms. Using a channel, you can 
  design your program to not share mutable memory at all, instead just send 
  messages.

Example of using a channel:

    let (tx, rx) = Channel[int].make()
    Thread.spawn(fn():
        tx.send(42)
    )
    let value = rx.receive()
    print(value)  # prints 42

In this example, one thread sends the integer 42 through the channel, and the 
main thread receives it. The Channel ensures safe handoff of data - 42 is moved 
from one thread to another in this case. Channels in many languages (Rust, Go) 
are a primary tool for concurrency because they sidestep the complexities of 
locking for many scenarios.

All these abstractions (Mutex, Channel, etc.) are built on top of lower-level 
primitives (atomic operations or OS synchronization). They encapsulate the 
unsafety. For example, internally a Mutex might use a raw OS mutex or an atomic 
spinlock, and that involves some unsafe code, but the user of Mutex only deals 
with a safe interface (lock, unlock or RAII guard) that ensures the rules are 
followed.

Because of the ownership and borrowing rules, Pyrite's compiler ensures that if 
two threads access the same memory, it must be through some synchronization 
object. If you try to share a plain &mut T reference or something across threads, 
it just won't compile. The type system plus trait bounds (Send/Sync) and maybe 
some static analysis achieve this guarantee. In Rust, this means safe Rust is 
free of data races. Pyrite aspires to the same: no data races in safe Pyrite 
code. (Data race meaning two threads accessing the same memory concurrently with 
at least one write, without synchronization - which is undefined behavior in 
languages like C/C++.)

No-cost if Not Used
~~~~~~~~~~~~~~~~~~~

If your program does not use threads or any concurrency primitives, none of that 
infrastructure is included in the binary. There is no background thread or 
runtime doing anything implicitly. You only pay for what you use (for example, if 
you never spawn a thread, the Thread code might not even be linked in). This is 
important for embedded or small systems where you might be single-threaded.

Async/Await with Structured Concurrency (Stable Release)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pyrite will include async/await for high-concurrency applications, but with a 
critical difference from Rust: **structured concurrency by default**.

Traditional async (Rust, JavaScript):

    async fn fetch_data():
        spawn(background_task())  # Fire and forget - task may outlive scope
        let result = await fetch_primary()
        return result
    # background_task may still be running - leak!

Pyrite's structured concurrency:

    async fn fetch_data():
        async with:
            spawn fetch_user()      # Spawned task tracked
            spawn fetch_orders()    # Spawned task tracked
        # BOTH tasks complete before this line
        # No leaked background tasks possible

How it works:

    async with:
        # Scope for structured concurrency
        let user = spawn fetch_user()      # Returns JoinHandle
        let orders = spawn fetch_orders()  # Returns JoinHandle
        # Implicit await for all spawned tasks at scope exit

    # Both complete here - guaranteed

Benefits:
  â€¢ **No leaked tasks:** All spawned work completes before scope exit
  â€¢ **Prevents common bugs:** Background tasks can't outlive their purpose
  â€¢ **Clear reasoning:** Scope shows when work completes
  â€¢ **Cancellation propagates:** If parent task cancelled, children cancelled

Comparison with Rust:

    // Rust: Manual tracking required
    async fn process() {
        let handle1 = tokio::spawn(task1());
        let handle2 = tokio::spawn(task2());
        // Easy to forget to await handles
        // Tasks may leak if function returns early
        handle1.await?;
        handle2.await?;
    }
    
    # Pyrite: Automatic tracking
    async fn process():
        async with:
            spawn task1()
            spawn task2()
        # Compiler ensures all complete

Fire-and-forget (opt-in):

If you truly need detached tasks, make it explicit:

    spawn_detached(background_task())  # Explicit: task outlives scope
    # Must use spawn_detached, not spawn

Why structured concurrency matters:

Rust's async is criticized for leaked tasks being a common bug source:
  â€¢ Forgot to await a handle â†’ silent task leak
  â€¢ Early return â†’ spawned tasks abandoned
  â€¢ Error propagation â†’ cleanup tasks not run

Swift and Kotlin already prove structured concurrency works. Pyrite adopts this 
proven pattern.

Integration with error handling:

    async fn process_all() -> Result[(), Error]:
        async with:
            spawn fetch_user()?   # Error propagates, cancels other tasks
            spawn fetch_orders()
        # If any task errors, all others cancelled

Teaching path:
  1. **Beginner:** Use async/await for I/O
  2. **Intermediate:** Learn async with for parallel tasks
  3. **Advanced:** Understand cancellation and propagation
  4. **Expert:** Use spawn_detached when truly needed

This makes Pyrite's async story: "Like Rust's zero-cost async, but without the 
footgun of leaked tasks." Stable Release feature that addresses Rust's main async 
criticism.

In summary, Pyrite's concurrency model is about giving you the tools to do 
multithreading safely: 
  - It prevents you from doing the unsafe things (sharing mutable data 
    willy-nilly). 
  - It provides high-level, easy-to-use abstractions like threads, channels, and 
    mutexes that internally handle the hard parts (locking, etc.) correctly. 
  - It has no garbage collector, but thanks to ownership, memory management in 
    threads is still safe. You can freely send ownership of an object to another 
    thread (if it's Send), and the receiver will free it when done, with no leaks 
    or double frees. 
  - The design mirrors Rust's because Rust has proven that this approach yields 
    fearless concurrency (developers can spawn threads and not worry about memory 
    corruption or data races, as long as they stick to safe code).

9.16 Concurrency Primitives (Standard Library)
--------------------------------------------------------------------------------

(Expanding on the concurrency section previously outlined in section 9.2, now 
renumbered appropriately. This remains unchanged from the original spec but 
repositioned for consistency.)

The stdlib provides thread-safe abstractions built on the ownership model:

  â€¢ Thread spawning with safe closure capture
  â€¢ Mutex[T] for exclusive access
  â€¢ RwLock[T] for reader-writer locks
  â€¢ Atomic types (AtomicInt, AtomicBool, etc.)
  â€¢ Channel[T] for message passing between threads
  â€¢ Barrier, Semaphore for synchronization

All concurrency primitives leverage the type system to prevent data races at 
compile time. See section 5 (Memory Management) for details on how Send/Sync 
traits enforce thread safety.

9.17 Built-In Observability (Stable Release)
--------------------------------------------------------------------------------

Production systems require visibility into runtime behavior. Pyrite's standard 
library provides first-class observability primitives (logging, tracing, metrics) 
that integrate seamlessly with the language's zero-cost philosophy and can be 
completely eliminated at compile time when not needed.

Design Philosophy
~~~~~~~~~~~~~~~~~

Pyrite's observability follows core principles:
  â€¢ **Zero cost when disabled:** Compile-time feature flags eliminate all 
    instrumentation in embedded builds
  â€¢ **Structured, not unstructured:** JSON-style structured logging, not printf
  â€¢ **OpenTelemetry-compatible:** Industry-standard distributed tracing
  â€¢ **Type-safe:** Strongly-typed log fields, metrics, and spans
  â€¢ **Composable:** Works with ownership system (no hidden allocations)

Structured Logging
~~~~~~~~~~~~~~~~~~

    import std::log
    
    fn api_handler(req: &Request) -> Result[Response, Error]:
        # Structured logging with typed fields
        log::info("request_received", {
            "method": req.method,
            "path": req.path,
            "user_id": req.user_id,
            "ip": req.remote_addr
        })
        
        let result = try db.query(req)?
        
        log::debug("query_completed", {
            "rows": result.len(),
            "duration_ms": result.duration.as_millis()
        })
        
        return Ok(Response::new(result))

Log Levels
~~~~~~~~~~

    log::trace("...")    # Extremely verbose, disabled by default
    log::debug("...")    # Debug information
    log::info("...")     # Informational
    log::warn("...")     # Warning
    log::error("...")    # Error occurred
    log::fatal("...")    # Fatal error, program will terminate

Configuration:

    # Quarry.toml
    [logging]
    level = "info"                   # Default level
    format = "json"                  # JSON or "pretty" text
    destination = "stderr"           # Where logs go

Distributed Tracing
~~~~~~~~~~~~~~~~~~~

OpenTelemetry-compatible distributed tracing:

    import std::trace
    
    fn process_order(order_id: &str) -> Result[(), Error]:
        # Create trace span
        with span = trace::span("process_order"):
            span.set_attribute("order_id", order_id)
            span.set_attribute("user_id", get_user_id())
            
            # Nested spans
            with db_span = span.child("database_query"):
                let items = try db.fetch_order_items(order_id)?
                db_span.set_attribute("items_count", items.len())
            
            with shipping_span = span.child("calculate_shipping"):
                let cost = try calculate_shipping(&items)?
                shipping_span.set_attribute("shipping_cost", cost)
            
            # Span automatically records duration on scope exit
            return Ok(())

Trace context propagation:

    # Trace context automatically propagates across:
    # - Function calls (via span.current())
    # - Thread boundaries (explicit propagation)
    # - HTTP requests (via headers)
    # - Message queues (via metadata)

Metrics
~~~~~~~

Type-safe metrics collection:

    import std::metrics
    
    fn handle_request(req: &Request) -> Response:
        # Counters
        metrics::counter("http.requests.total").increment()
        metrics::counter("http.requests.by_method")
            .label("method", req.method)
            .increment()
        
        let start = Instant::now()
        
        let response = process_request(req)
        
        # Histograms (distribution of values)
        metrics::histogram("http.request.duration_ms")
            .record(start.elapsed().as_millis())
        
        # Gauges (point-in-time value)
        metrics::gauge("http.active_connections")
            .set(get_active_count())
        
        return response

Metric types:
  â€¢ **Counter:** Monotonically increasing (request count, bytes sent)
  â€¢ **Gauge:** Point-in-time value (memory usage, queue depth)
  â€¢ **Histogram:** Distribution of values (latency, request size)
  â€¢ **Summary:** Like histogram with quantiles (p50, p95, p99)

Zero-Cost Elimination
~~~~~~~~~~~~~~~~~~~~~

When observability features are disabled, they're completely eliminated:

    # Embedded build with observability disabled
    $ quarry build --embedded --no-features=observability
    
    # All log, trace, metrics calls compiled to ZERO instructions
    # Binary size: Same as if observability code wasn't there
    # Runtime cost: Absolutely zero

Implementation uses compile-time feature flags:

    # This code:
    log::info("message", {"key": value})
    
    # Compiles to:
    #[cfg(feature = "observability")]
    log::info("message", {"key": value})
    #[cfg(not(feature = "observability"))]
    { /* no-op, optimized away */ }

Cost Transparency
~~~~~~~~~~~~~~~~~

When observability IS enabled, costs are visible:

    $ quarry cost --show-observability
    
    Observability Costs
    ===================
    
    Logging: 47 call sites
      â€¢ Allocations: 23 (string formatting)
      â€¢ Total cost: ~4 KB per 1000 log calls
    
    Tracing: 12 spans
      â€¢ Stack overhead: 48 bytes per span
      â€¢ Allocation: Only if spans exported (async)
    
    Metrics: 8 collection sites
      â€¢ Lock contention: Atomic operations (no lock)
      â€¢ Memory: 16 bytes per metric
      â€¢ Update cost: ~5 cycles (atomic increment)

Exporter Configuration
~~~~~~~~~~~~~~~~~~~~~~

Export telemetry to monitoring systems:

    import std::log::exporters
    
    fn main():
        # Configure exporters
        log::set_exporter(exporters::StdErr::new())
        trace::set_exporter(exporters::Jaeger::new("http://localhost:14268"))
        metrics::set_exporter(exporters::Prometheus::new(":9090"))
        
        # Application code
        run_server()

Built-in exporters:
  â€¢ Logs: StdOut, StdErr, File, Syslog, JSON
  â€¢ Traces: Jaeger, Zipkin, OpenTelemetry Protocol (OTLP)
  â€¢ Metrics: Prometheus, StatsD, OpenTelemetry

Custom exporters:

    struct CustomExporter:
        # Implement exporter trait
    
    impl log::Exporter for CustomExporter:
        fn export(&mut self, record: &LogRecord):
            # Send to custom destination

Why Built-In Observability Matters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Production readiness:**
  â€¢ **Cannot achieve widespread adoption without production features**

  â€¢ Logs, traces, metrics are non-negotiable for servers
  â€¢ Third-party solutions fragment ecosystem
  â€¢ Built-in = consistent, compatible, zero-setup

**Competitive landscape:**
  â€¢ **Go:** Excellent observability story (built-in + ecosystem)
  â€¢ **Rust:** Third-party crates (tracing, log) work well
  â€¢ **C/C++:** Manual instrumentation, no standard
  â€¢ **Pyrite:** First-class, zero-cost when disabled

**Real-world requirement:**
  â€¢ Kubernetes: Logs + metrics expected
  â€¢ Observability tools: Prometheus, Grafana, Jaeger standard
  â€¢ SRE teams: "How do I debug this in production?"
  â€¢ Without observability: "Pyrite is missing production features"

**Embedded vs server trade-off:**
  â€¢ Embedded: Observability compiled out (zero cost)
  â€¢ Server: Observability essential for debugging
  â€¢ Same language, different profiles
  â€¢ Feature flags make this seamless

**OpenTelemetry alignment:**
  â€¢ Industry standard (CNCF project)
  â€¢ Supported by: Datadog, New Relic, Honeycomb, AWS X-Ray
  â€¢ Compatible = works with existing infrastructure
  â€¢ No vendor lock-in

Use Cases
~~~~~~~~~

  â€¢ **Web servers:** Request logging, latency tracking, error rates
  â€¢ **Microservices:** Distributed tracing across service boundaries
  â€¢ **Batch processing:** Progress tracking, throughput metrics
  â€¢ **Embedded (opt-in):** Serial logging, performance counters
  â€¢ **Games:** Frame time metrics, event logging

Example Production Usage
~~~~~~~~~~~~~~~~~~~~~~~~

    import std::log
    import std::trace
    import std::metrics
    
    fn main():
        # Initialize observability
        observability::init(Config {
            log_level: Level::Info,
            trace_exporter: Jaeger::new("http://jaeger:14268"),
            metrics_exporter: Prometheus::new(":9090")
        })
        
        # Run application
        with span = trace::span("application_lifetime"):
            run_server()
    
    fn run_server():
        let listener = TcpListener::bind("0.0.0.0:8080")?
        log::info("server_started", {"port": 8080})
        
        for stream in listener.incoming():
            metrics::counter("connections.total").increment()
            
            with span = trace::span("handle_connection"):
                handle_client(stream)

This makes Pyrite credible for production server deployments. Without 
observability, Pyrite is "embedded + CLI tools only." With it, Pyrite is 
"full-stack systems language."

Implementation: Stable Release (after core stdlib is stable)
Priority: High (required for server/cloud adoption)
Complexity: Moderate (exporter integrations, OpenTelemetry compat)
Impact: High (unlocks server/cloud use cases)

================================================================================
10. PYRITE PLAYGROUND AND LEARNING EXPERIENCE
================================================================================

The Pyrite Playground is a first-class component of the ecosystem, designed to 
lower the barrier to entry and enable frictionless sharing of code examples.

10.1 Browser-Based Playground
--------------------------------------------------------------------------------

Features
~~~~~~~~

The Playground (aspirational: play.pyrite-lang.org) provides:

  â€¢ Zero-installation experimentation: Write and run Pyrite in the browser
  â€¢ Real compiler: Uses WebAssembly-compiled pyritec for authentic experience
  â€¢ Instant feedback: Compiler errors displayed inline as you type
  â€¢ Example library: Curated examples for common patterns and language features
  â€¢ Shareable links: Every playground session has a unique URL
  â€¢ Embedded mode: Playground can be embedded in documentation and tutorials
  â€¢ Output capture: Console output, compiler warnings, execution time

Example Workflow
~~~~~~~~~~~~~~~~

1. Visit (aspirational: play.pyrite-lang.org)
2. Write code:

   fn main():
       let numbers = List[int]([1, 2, 3, 4, 5])
       let sum = numbers.iter().fold(0, fn(acc, x): acc + x)
       print("Sum:", sum)

3. Click "Run" â†’ See output immediately
4. Click "Share" â†’ Get link: (aspirational: play.pyrite-lang.org/abc123)
5. Paste link in forum post, documentation, or chat

10.2 Integration with Documentation
--------------------------------------------------------------------------------

Every code example in official documentation is a live Playground link:

    ```pyrite
    fn factorial(n: int) -> int:
        if n <= 1:
            return 1
        return n * factorial(n - 1)
    
    fn main():
        print(factorial(5))  # Output: 120
    ```
    
    [â–¶ Run this example] â†’ Opens in Playground

Readers can modify examples and see results instantly, dramatically improving 
the learning experience.

10.3 Compiler Error Teaching
--------------------------------------------------------------------------------

The Playground is optimized for teaching through errors:

  â€¢ Error messages use the full diagnostic format (section 2)
  â€¢ Hover over error codes for inline explanations
  â€¢ "Explain" button opens detailed error documentation
  â€¢ "Suggest fix" applies automatic corrections when available
  â€¢ Multi-error display with prioritization (show blocking errors first)

Example error in Playground:

    let data = List[int]([1, 2, 3])
    process(data)
    print(data[0])  # â† Error highlighted here

    [!] error[P0234]: cannot use moved value 'data'
    
    'data' was moved on line 2 when passed to 'process'
    
    [Explain P0234] [Suggest fix] [Share this error]

10.4 Why Playground Matters for Adoption
--------------------------------------------------------------------------------

Learning Curve Reduction
~~~~~~~~~~~~~~~~~~~~~~~~

Playground eliminates friction for newcomers:

  â€¢ No installation required (toolchain setup often blocks beginners)
  â€¢ Instant gratification (see results in seconds)
  â€¢ Low stakes (experiment without breaking local environment)
  â€¢ Social proof (easily share working examples)

Advocacy Amplification
~~~~~~~~~~~~~~~~~~~~~~

When developers discover something cool in Pyrite, they share it:

  â€¢ Twitter/Reddit post with Playground link â†’ Readers can try immediately
  â€¢ Stack Overflow answer with Playground example â†’ Questioner runs it in-browser
  â€¢ Tutorial with embedded Playground â†’ Readers learn by doing

This creates a flywheel: better playground â†’ more sharing â†’ more discovery â†’ 
more adoption.

Community Support
~~~~~~~~~~~~~~~~~

Playground links are invaluable for support forums:

  â€¢ Bug reports include reproducible case (playground link)
  â€¢ Help requests show actual code context
  â€¢ Teaching examples are interactive, not passive screenshots

Rust Playground (play.rust-lang.org) is consistently cited as one of Rust's best 
features. Pyrite Playground delivers similar experience from day one.

================================================================================
11. FOREIGN FUNCTION INTERFACE (FFI) AND INTEROPERABILITY
--------------------------------------------------------------------------------

11.1 C Foreign Function Interface
--------------------------------------------------------------------------------

Pyrite is engineered to integrate seamlessly with existing C/C++ codebases. The 
FFI (foreign function interface) allows calling external C functions directly and 
vice versa (exporting Pyrite functions to C), since one of the goals is easy 
adoption and reuse of legacy code. 

For example:

    extern fn printf(fmt: *const u8, ...) -> int  # declare an external C function

You could declare extern functions in Pyrite that are defined in C libraries. 
Pyrite will ensure the calling convention matches (likely it uses the platform C 
ABI by default for extern). Similarly, Pyrite could mark functions to be exported 
with C ABI so that C code can call into Pyrite compiled code. Because Pyrite has 
no heavyweight runtime, calling into Pyrite from C is just like calling into any 
compiled library - you just need to initialize any needed Pyrite runtime (which 
might be minimal to nonexistent).

The build tooling for Pyrite will likely include: 
  - A package manager (like Cargo for Rust or Zig's build system) to manage 
    dependencies and building libraries/executables. This simplifies sharing code 
    and using third-party libraries. 
  - Cross-compilation support out of the box: being a systems language, Pyrite 
    aims to target many architectures and OSes easily (like Zig's mantra of 
    cross-compiling is easy). The build system can supply needed runtime stubs, 
    etc., for different targets.

In terms of ecosystem: 
  - Testing and documentation: The language and tooling will encourage writing 
    tests (possibly with an inline testing framework like Rust's #[test] 
    functions) and documentation (doc comments """ that can be extracted into 
    docs). The goal is a developer-friendly experience where writing correct code 
    is streamlined. 
  - Package registry: likely there will be an official repository for Pyrite 
    packages to easily add dependencies (similar to PyPI for Python or crates.io 
    for Rust).

All these ecosystem features make Pyrite not just a bare language, but a 
practical tool for real projects. Part of being "most loved" is having a smooth 
development experience, not just the language syntax and features.

11.2 Build System Integration
--------------------------------------------------------------------------------

The build tooling for Pyrite (Quarry) includes:

  â€¢ Package manager to manage dependencies and build libraries/executables
  â€¢ Cross-compilation support out of the box: target many architectures and OSes 
    easily (like Zig's cross-compilation philosophy)
  â€¢ Testing framework with inline tests
  â€¢ Documentation generation from doc comments
  â€¢ Official package registry for sharing code

See section 8 (Tooling: Quarry Build System) for comprehensive details.

11.3 Future: Automatic Binding Generation
--------------------------------------------------------------------------------

While the initial release focuses on manual FFI declarations, future versions 
will include quarry bindgen to automatically generate Pyrite bindings from C 
header files:

    quarry bindgen /usr/include/sqlite3.h --output=src/sqlite.pyrite

This eliminates tedious manual translation of large C APIs, similar to Rust's 
bindgen tool.

11.4 Python Interoperability (Future Release)
--------------------------------------------------------------------------------

After establishing Pyrite in embedded and server domains (Phases 1-3), Python 
interoperability becomes a strategic adoption wedge for numerical computing and 
data science applications. This is intentionally delayed to avoid complexity in 
early phases.

Why Python Interop Matters
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Python's ecosystem (NumPy, SciPy, Pandas, PyTorch, TensorFlow) represents decades 
of mature libraries and millions of existing projects. For Pyrite to compete in 
numerical/scientific computing (Stable Release), seamless Python integration provides:

  â€¢ **Adoption wedge:** Incremental migration from Python performance bottlenecks
  â€¢ **Ecosystem leverage:** Access to existing Python libraries until Pyrite equivalents mature
  â€¢ **Data science appeal:** Pyrite becomes "Python's performance layer"
  â€¢ **Teaching path:** Python developers try Pyrite for hot loops, expand usage gradually

Why Not Beta Release
~~~~~~~~~~~~~~~~~~

Python interop is intentionally delayed because:

1. **Wrong audience initially:**
   Beta Release targets embedded developers and systems programmers
   Neither needs Python FFI (embedded has no Python, systems devs avoid it)

2. **Adds significant complexity:**
   â€¢ GIL integration (Global Interpreter Lock)
   â€¢ Reference counting interop with Pyrite ownership
   â€¢ Type bridging (Python dynamic types â†” Pyrite static types)
   â€¢ Exception handling across boundary
   â€¢ Requires Python runtime present (conflicts with no-runtime philosophy)

3. **Conflicts with embedded-first strategy:**
   Embedded firmware can't depend on Python runtime
   @noalloc mode incompatible with Python's malloc-heavy runtime
   Target hardware (microcontrollers) can't run Python

4. **Mojo already owns this space:**
   Mojo's entire value proposition is "Python interop + performance"
   Competing there dilutes Pyrite's embedded/systems differentiation

Future Release Design Approach
~~~~~~~~~~~~~~~~~~~~~~~~~

When Python interop is added, it must maintain Pyrite's core principles:

**Explicit, not hidden:**
  â€¢ Python calls are clearly bounded
  â€¢ No hidden GIL acquisition or reference counting
  â€¢ Cost transparency: quarry cost shows Python calls as expensive

**Optional and isolated:**
  â€¢ Requires explicit import of std::python module
  â€¢ Python runtime is optional dependency (not required for core Pyrite)
  â€¢ Code without Python imports compiles with zero Python dependencies

**Type-safe boundaries:**
  â€¢ Explicit type conversion at boundaries
  â€¢ Pyrite ownership rules enforced (no implicit borrowing of Python objects)
  â€¢ Python exceptions converted to Result types

Example API sketch (Future Release):

    import std::python as py
    
    fn process_with_numpy(data: &[f64]) -> Result[Vec[f64], Error]:
        # Explicit Python GIL acquisition
        with gil = py::GIL::acquire():
            # Import Python module
            let np = try gil.import("numpy")
            
            # Convert Pyrite slice to NumPy array (zero-copy where possible)
            let py_array = try gil.from_slice(data)
            
            # Call Python function
            let result = try np.call("fft", [py_array])
            
            # Convert back to Pyrite (allocates)
            let pyrite_vec = try gil.to_vec[f64](result)
            
            return Ok(pyrite_vec)
        # GIL released automatically via defer

Cost transparency:

    warning[P1350]: Python GIL acquisition
      ----> src/compute.py:45:9
       |
    45 |     with gil = py::GIL::acquire():
       |              ^^^^^^^^^^^^^^^^^^^^ expensive operation
       |
       = note: GIL acquisition may block if Python is active
       = note: Python calls within GIL are 10-100x slower than native code
       = performance: Consider:
         1. Minimize GIL-protected regions
         2. Pre-convert data to avoid repeated conversions
         3. Use Pyrite-native libraries where available

Python Extension Module Generation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For the reverse direction (calling Pyrite from Python), provide tooling:

    quarry pyext                           # Generate Python extension module
    quarry pyext --name=fast_compute       # Custom module name

Generated extension exposes Pyrite functions to Python:

    # Pyrite code (src/compute.pyrite)
    @pyexport
    fn fast_matrix_multiply(a: &[f64], b: &[f64], rows: usize, cols: usize) -> Vec[f64]:
        # Pyrite implementation (fast, safe)
        ...
    
    # Generated Python module (build/fast_compute.so)
    # Python usage:
    import fast_compute
    
    result = fast_compute.fast_matrix_multiply(a, b, rows, cols)
    # Calls compiled Pyrite code (100x faster than Python)

Benefits for Python users:
  â€¢ Drop-in replacement for performance bottlenecks
  â€¢ Memory-safe (no segfaults from extension bugs)
  â€¢ Easy distribution (wheel packages)
  â€¢ Gradual migration path (replace hot functions one at a time)

Example migration path:

    # Week 1: Profile Python code, find bottleneck
    # Week 2: Rewrite bottleneck in Pyrite
    # Week 3: Generate extension module (quarry pyext)
    # Week 4: Import and use from Python (import my_fast_code)
    # Week 5-N: Migrate more hot paths incrementally

Type Bridging
~~~~~~~~~~~~~

Explicit type conversions at the boundary:

    # Pyrite â†’ Python
    gil.from_slice(&data)        # &[f64] â†’ numpy.ndarray (zero-copy)
    gil.from_vec(vec)            # Vec[T] â†’ list (copies)
    gil.from_str(&text)          # &str â†’ str (copies)
    
    # Python â†’ Pyrite
    gil.to_vec[f64](py_obj)      # list â†’ Vec[f64] (allocates, validates)
    gil.to_slice[f64](py_obj)    # ndarray â†’ &[f64] (borrows Python memory)
    gil.to_string(py_obj)        # str â†’ String (allocates)

All conversions are explicit and show their cost in quarry cost output.

Future Release Rationale
~~~~~~~~~~~~~~~~~~

By waiting until a future release, Python interop benefits from:
  â€¢ Mature ownership model (no surprises at boundaries)
  â€¢ Proven cost transparency (Python calls clearly expensive)
  â€¢ Strong stdlib (less need for Python deps)
  â€¢ Established identity (not "Python with performance," but "systems language that CAN interop")

Target use cases:
  â€¢ NumPy/SciPy bottleneck replacement
  â€¢ Data science pipeline acceleration
  â€¢ ML model inference (Pyrite replaces Python hot paths)
  â€¢ Scientific computing (leverage both ecosystems)

Not for:
  â€¢ Embedded (no Python runtime available)
  â€¢ Systems programming (Python runtime unwanted)
  â€¢ Real-time systems (GIL breaks determinism)

Timeline: Future Release (after numerical computing, tensor types, SIMD are stable)
Priority: Medium (valuable for market expansion, not critical for core identity)

This positions Python interop as: "Pyrite is systems language first, but can 
leverage Python ecosystem when needed" rather than "Pyrite is Python replacement."

================================================================================
12. MARKETING AND POSITIONING
================================================================================

While technical excellence is necessary, it's not sufficient for achieving "most 
admired" status. Pyrite needs clear positioning and messaging.

12.1 The Core Pitch
--------------------------------------------------------------------------------

30-Second Elevator Pitch
~~~~~~~~~~~~~~~~~~~~~~~~~

"Pyrite is what Python would be if it were a systems language-readable, 
explicit, and memory-safe by default, compiling to C-speed binaries with no 
runtime."

Extended Pitch (2 minutes)
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pyrite combines three best-in-class languages:

  â€¢ Python's readability: Clean syntax with significant whitespace, minimal 
    boilerplate, obvious semantics
  â€¢ Rust's safety: Ownership model prevents memory bugs and data races at 
    compile time, no garbage collector needed
  â€¢ Zig's transparency: No hidden allocations, no hidden control flow, explicit 
    costs for everything

Who should use Pyrite?

  â€¢ Systems programmers tired of segfaults and memory corruption
  â€¢ Application developers who need C-level performance without C-level pain
  â€¢ Teams building security-critical software (embedded, crypto, OS kernels)
  â€¢ Educators teaching systems programming without the C footguns
  â€¢ Anyone who wants Rust's guarantees with an easier learning curve

12.2 Differentiation Matrix
--------------------------------------------------------------------------------

vs C/C++
~~~~~~~~

Advantages:
  âœ“ Memory safety by default (no segfaults, no buffer overflows)
  âœ“ Modern syntax (readable, less boilerplate)
  âœ“ No undefined behavior in safe code
  âœ“ Built-in package manager and tooling
  âœ“ Fearless concurrency (data races prevented by compiler)

Trade-offs:
  â—‹ Comparable performance (zero-cost abstractions)
  â—‹ Can drop to manual control when needed (unsafe blocks)

vs Rust
~~~~~~~

Advantages:
  âœ“ Easier learning curve (Python-like syntax, gentler concepts)
  âœ“ More intuitive for beginners (indentation-based, no sigils)
  âœ“ Cost transparency attributes (@noalloc, @nocopy)
  âœ“ Explicit is better than implicit (Pythonic philosophy)

Trade-offs:
  â—‹ Equivalent safety guarantees (ownership model)
  â—‹ Equivalent performance (zero-cost abstractions)
  â—‹ Smaller ecosystem initially (Rust has 10+ years head start)

vs Python
~~~~~~~~~

Advantages:
  âœ“ 100x+ faster (compiled to machine code)
  âœ“ No runtime overhead (no GIL, no GC pauses)
  âœ“ Predictable performance (explicit allocation)
  âœ“ Compile-time error catching (type safety)
  âœ“ Systems programming capable (OS kernels, embedded)

Trade-offs:
  â—‹ Static typing required (but with inference)
  â—‹ Ownership model learning curve (prevents memory bugs)
  â—‹ Compilation step (but fast compile times are a goal)

vs Go
~~~~~

Advantages:
  âœ“ No garbage collector (deterministic performance)
  âœ“ Zero-cost abstractions (generics without runtime overhead)
  âœ“ Memory safety without runtime (compile-time guarantees)
  âœ“ Fine-grained control (manual memory management available)

Trade-offs:
  â—‹ Steeper learning curve (ownership vs GC simplicity)

vs Zig
~~~~~~

Advantages:
  âœ“ Memory safety by default (ownership model prevents bugs)
  âœ“ More familiar syntax for Python/mainstream programmers
  âœ“ Stronger type system (prevents more bugs at compile time)

Trade-offs:
  â—‹ Similar explicitness and transparency
  â—‹ Comparable C interop

vs Mojo
~~~~~~~

Advantages:
  âœ“ Explicit SIMD (no auto-vectorization magic, predictable performance)
  âœ“ Tool-based autotuning (zero runtime cost vs Mojo's deprecated runtime approach)
  âœ“ Parameter closures with provable zero-cost (fn[...] syntax makes cost explicit)
  âœ“ No hidden runtime behavior (Mojo has hidden allocations, Pyrite never does)
  âœ“ Embedded/systems-first (Mojo is Python/ML-first)
  âœ“ Supply-chain security built-in (audit, vet, sign)
  âœ“ Performance lockfile (Perf.lock) prevents regressions
  âœ“ General-purpose systems language (not ML-specific)

Trade-offs:
  â—‹ No Python interop initially (Mojo's flagship, Pyrite's future release)
  â—‹ Smaller ML ecosystem (Mojo targets PyTorch users, Pyrite targets embedded)
  â—‹ Later GPU support (Pyrite Stable Release, Mojo earlier)

Differentiation:
  â€¢ Mojo: "Python that's as fast as C++ for AI/ML"
  â€¢ Pyrite: "Systems language with Python syntax, embedded to GPU"

Target audience overlap:
  â€¢ Pyrite: Embedded developers, systems programmers, Rust-curious
  â€¢ Mojo: Python/ML developers, data scientists, AI engineers

Competition assessment:
  â€¢ Different domains: Minimal direct competition
  â€¢ Complementary: Pyrite for systems/embedded, Mojo for ML/Python ecosystem
  â€¢ Pyrite advantage: Explicit transparency, no magic, broader systems usage
  â€¢ Mojo advantage: Python interop from day one, AI/ML focus

Strategic note: Pyrite learns from Mojo's successes (parameter closures, 
algorithmic helpers, compile-time params) while avoiding their mistakes (runtime 
autotuning, hidden allocations, narrow ML focus). Pyrite positions as the 
general-purpose systems language; Mojo positions as Python's performance layer.

12.3 Target Audiences and Use Cases
--------------------------------------------------------------------------------

Primary Audiences
~~~~~~~~~~~~~~~~~

1. **Embedded Systems Developers**
   Pain point: C/C++ memory bugs in resource-constrained environments
   Pyrite solution: Memory safety, deterministic performance, no runtime

2. **Systems Programmers**
   Pain point: Tired of debugging segfaults and undefined behavior
   Pyrite solution: Compile-time safety catches bugs, fearless refactoring

3. **Security-Critical Software Teams**
   Pain point: CVEs from memory vulnerabilities
   Pyrite solution: Memory-safe by default, isolated unsafe auditing

4. **Rust Beginners Who Bounced Off**
   Pain point: Borrow checker confusion, complex syntax
   Pyrite solution: Same safety guarantees, gentler syntax and learning curve

5. **Python Developers Needing Performance**
   Pain point: Speed limitations, deployment complexity
   Pyrite solution: Familiar syntax, 100x faster, single binary deployment

Concrete Use Cases
~~~~~~~~~~~~~~~~~~

  â€¢ Operating system kernels (no runtime requirement)
  â€¢ Embedded firmware (AVR, ARM Cortex-M, RISC-V)
  â€¢ Game engines (predictable frame times, no GC pauses)
  â€¢ Database engines (memory safety, performance)
  â€¢ CLI tools (fast, single binary, easy distribution)
  â€¢ Web servers (high throughput, low latency, safe concurrency)
  â€¢ Cryptographic libraries (constant-time guarantees, auditable)
  â€¢ Network protocols (performance, safety)
  â€¢ Scientific computing (speed with safety)

Flagship Domain Strategy: Embedded-First
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

While Pyrite is a general-purpose systems language, the go-to-market strategy 
prioritizes **embedded/no-alloc as the flagship domain** for initial adoption.

Why Embedded-First?
~~~~~~~~~~~~~~~~~~~

1. **Most Differentiated:**
   
   Pyrite's unique advantages shine brightest in embedded:
     â€¢ @noalloc + quarry build --no-alloc = provable zero-heap âœ“
     â€¢ Call-graph blame tracking for allocation contracts âœ“
     â€¢ Zero runtime overhead (no GC, no runtime) âœ“
     â€¢ Cost transparency (every allocation, every copy visible) âœ“
   
   Competitors in embedded:
     â€¢ C/C++: Dominant but no memory safety
     â€¢ Rust: Tried but syntax too heavy for many embedded devs
     â€¢ Zig: Good transparency but no safety guarantees
   
   Pyrite occupies the unique position: "Memory-safe embedded without runtime 
   overhead, with tooling that teaches rather than frustrates."

2. **Underserved Market:**
   
   Web servers: Go, Rust, Elixir all competitive
   CLI tools: Rust, Go well-established
   Embedded: C/C++ still dominant due to inertia, no clear memory-safe winner
   
   Embedded developers WANT memory safety but can't justify Rust's learning 
   curve or ecosystem complexity. Pyrite's Pythonic syntax + teaching compiler + 
   zero-runtime story fills this gap.

3. **Trust Multiplier:**
   
   Once embedded developers trust Pyrite for firmware (where constraints are 
   harshest), they'll use it for everything:
     â€¢ "If it works for 32KB RAM microcontroller, it'll work for my server"
     â€¢ "If safety certification accepts it, our web app definitely can"
   
   Embedded is the hardest problem-solve it first, others become easier.

4. **Clear Success Metrics:**
   
   Embedded has concrete, measurable wins:
     â€¢ Zero CVEs from memory bugs (safety proof)
     â€¢ Certification for aerospace/medical (industry validation)
     â€¢ Adoption by Arduino, Raspberry Pi Pico ecosystems (community proof)
     â€¢ "Replaced C in our firmware, zero memory bugs in 2 years" (testimonial)

5. **Compounding Network Effects:**
   
   Embedded developers are evangelists:
     â€¢ Maker communities (Arduino, Adafruit)
     â€¢ Academic labs (research prototypes)
     â€¢ Startups (IoT, wearables, robotics)
     â€¢ Industrial (automotive, industrial automation)
   
   One success story â†’ conference talk â†’ 100 new projects

Initial Target: "Zero-Undefined-Behavior Embedded"
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Go-to-market message for embedded:

  "Write firmware with zero undefined behavior, zero allocations, zero runtime--
   provably. Pyrite's @noalloc contracts + quarry build --no-alloc guarantee 
   what C promises but never delivers: complete memory safety with no overhead."

Validation path:
  1. **Beta Release:** Core language and tooling for self-hosting
  2. **Stable Release:** Arduino/Pico libraries in Pyrite (LED blink, sensor drivers)
  3. **Production:** ESP32 WiFi stack (network + embedded)
  4. **Enterprise:** Real-time OS kernel (RTOS in Pyrite)
  5. **Certification:** Safety certification (aerospace or medical device)

Once certification succeeds, the message becomes: "The language that powers Mars 
rovers and medical implants-now for your web app."

Expansion Strategy
~~~~~~~~~~~~~~~~~~

After establishing core stability (Beta Release):

**Stable Release Expansion: High-Performance Servers**
  â€¢ Async/await with structured concurrency
  â€¢ Database drivers
  â€¢ Message: "Same safety that powers embedded, now 10x faster than Python"

**Stable Release Expansion: Numerical/Scientific**
  â€¢ Tensor type with shape checking
  â€¢ SIMD and algorithmic helpers
  â€¢ Message: "Safety + speed for scientific code"

**Future Release Expansion: GPU/ML**
  â€¢ @kernel with blame tracking
  â€¢ Multi-backend GPU support
  â€¢ Message: "Embedded to GPU, same safety everywhere"

Why Not Server-First or ML-First?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Server-first problems:
  â€¢ Rust and Go already excellent
  â€¢ Hard to differentiate ("another async web framework")
  â€¢ Success metrics vague ("we're faster sometimes")
  â€¢ Network effects favor incumbents

ML-first problems:
  â€¢ PyTorch/JAX dominance is entrenched
  â€¢ Requires GPU support (Stable Release, not Beta)
  â€¢ "Yet another ML framework" fatigue
  â€¢ Ecosystem lock-in (model formats, tooling)

Embedded-first advantages:
  â€¢ Clear differentiation (@noalloc + teaching compiler)
  â€¢ Underserved (no memory-safe winner)
  â€¢ Concrete success (zero memory bugs, certification)
  â€¢ Trust multiplier (hardest problem â†’ easier problems follow)

Summary: Domain Prioritization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Priority 1 (Beta Release): **Core Language + Self-Hosting**
  â€¢ Language features for compiler implementation
  â€¢ Traits, generics, Result/Option types
  â€¢ FFI for LLVM bindings
  â€¢ File I/O and string manipulation
  â€¢ 100% test coverage, cross-platform stability

Priority 2 (Stable Release): **Embedded/Advanced Features**
  â€¢ SIMD and algorithmic helpers
  â€¢ Advanced performance tooling
  â€¢ Observability and production features
  â€¢ Community ecosystem tools

Priority 3 (Future Release): **GPU/Numerical Computing**
  â€¢ Natural extension of contract system
  â€¢ Opens ML/HPC markets
  â€¢ "Embedded to GPU" positioning

This strategy maximizes early differentiation while building toward broad 
adoption. Win embedded â†’ gain trust â†’ expand to all domains.

12.4 The Complete Feature Integration Story
--------------------------------------------------------------------------------

Pyrite's design represents a comprehensive approach to developer experience by
addressing every critical gap identified through competitive analysis. Here's how 
all the features work together as a complete system:

The Interactive Learning Loop
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  REPL (instant experimentation)
    â†’ Hit ownership error with real-time visualization
    â†’ Run quarry fix --interactive (see numbered options)
    â†’ Choose fix, learn pattern
    â†’ Try in REPL again with :ownership visualization
    â†’ Pattern clicks through repetition
    â†’ Graduate to quarry learn exercises
    â†’ Complete with native language error messages

Result: **Fastest ownership learning curve** of any systems language.

The Complete Transparency Story
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  Write code
    â†’ quarry cost shows allocations/copies (static)
    â†’ quarry perf shows hot spots (runtime)
    â†’ quarry alloc shows allocation stacks (runtime)
    â†’ quarry bloat shows binary size (embedded)
    â†’ quarry energy shows power consumption (unique)
    â†’ quarry tune synthesizes all data, suggests fixes
    â†’ Apply fixes, measure improvement
    â†’ quarry perf --baseline commits to Perf.lock
    â†’ CI prevents regression with root cause (assembly diff)

Result: **Performance visibility** across every dimension-time, memory, size, 
energy-with CI enforcement.

The Supply-Chain Trust System
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  Declare dependencies (Quarry.toml)
    â†’ quarry audit scans CVEs
    â†’ quarry vet reviews dependency tree
    â†’ quarry license-check verifies legal compatibility
    â†’ quarry build --deterministic produces reproducible binary
    â†’ quarry verify-build proves "binary from source"
    â†’ quarry sign creates cryptographic signature
    â†’ quarry sbom generates compliance manifest
    â†’ Publish with proof of provenance

Result: **Complete supply-chain security** - audit, review, reproduce, verify, 
sign, document. Required for aerospace, medical, government.

The Production Deployment Path
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  Develop with observability
    â†’ log::info, trace::span, metrics::counter in code
    â†’ quarry build --release builds with observability
    â†’ quarry build --embedded strips all instrumentation (zero cost)
    â†’ Deploy with OpenTelemetry-compatible telemetry
    â†’ Monitor in production with Prometheus/Jaeger
    â†’ quarry energy optimizes for data center efficiency

Result: **Production-ready from development to deployment** - observability 
built-in, not bolted-on.

The Global Accessibility Story
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  Developer in Beijing
    â†’ pyritec --language=zh (Chinese error messages)
    â†’ quarry learn (exercises in Chinese)
    â†’ Community dashboard shows adoption in China
    â†’ Contribute to ecosystem
    â†’ Package appears on global metrics

Result: **Worldwide accessibility** - language barriers removed, contributions 
from everywhere.

The Certification Path
~~~~~~~~~~~~~~~~~~~~~~

  Write safety-critical firmware
    â†’ Use Core Pyrite (--core-only mode)
    â†’ Add contracts (@requires, @ensures)
    â†’ quarry build --no-alloc verifies zero-heap
    â†’ quarry fuzz tests edge cases
    â†’ quarry sanitize --asan checks runtime safety
    â†’ Formal semantics enables verification tools
    â†’ Submit for DO-178C Level A certification

Result: **Highest safety certification levels** - complete verification story.

Why This Matters
~~~~~~~~~~~~~~~~~

Every feature **multiplies the value** of other features:

  â€¢ REPL makes ownership teaching interactive (not just compiler errors)
  â€¢ Deterministic builds complete security story (audit + vet + sign + reproduce)
  â€¢ Energy profiling differentiates (no competitor has this)
  â€¢ Dashboard provides evidence (claims become data)
  â€¢ Internationalization multiplies community (60% more developers accessible)
  â€¢ Observability enables production (servers + cloud credible)
  â€¢ Contracts enable certification (formal correctness)
  â€¢ Incremental compilation makes learning faster (instant feedback)

This comprehensive feature set transforms Pyrite from an excellent technical 
language into a complete developer platform with no gaps.

This is the difference between "admired by systems programmers" and "admired by 
everyone from beginners to aerospace engineers to sustainability advocates."

12.5 Messaging Principles
--------------------------------------------------------------------------------

Avoid Overpromising
~~~~~~~~~~~~~~~~~~~

Never claim Pyrite "solves all problems" or "makes programming easy." Focus on:
  â€¢ Specific, verifiable advantages (compile-time safety, predictable 
    performance)
  â€¢ Honest trade-offs (learning curve exists, smaller ecosystem initially)
  â€¢ Real-world validation (benchmarks, case studies)

Lead with Developer Experience
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Technical features matter, but developer happiness matters more:
  â€¢ "Write systems code with confidence" (safety)
  â€¢ "Performance you can understand" (transparency)
  â€¢ "Compiler teaches you as you learn" (great diagnostics)
  â€¢ "Build and ship in minutes, not hours" (great tooling)

Community Over Competition
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Positioning Pyrite as complementary, not combative:
  â€¢ Rust paved the way for memory-safe systems languages
  â€¢ C/C++ have decades of proven libraries (Pyrite interops)
  â€¢ Python proved readability matters (Pyrite honors that)
  â€¢ Zig showed transparency is possible (Pyrite adopts it)

"Standing on the shoulders of giants" creates goodwill and attracts contributors.

================================================================================
13. SECURITY AND RELIABILITY
================================================================================

Because Pyrite emphasizes memory safety and simplicity by default, it is 
inherently a secure-by-design language. Many classes of vulnerabilities that 
plague C/C++ programs - buffer overflows, dangling pointer dereferences, double 
frees, uninitialized memory usage, data races - are eliminated at compile time in 
Pyrite's safe code. 

This aligns with the growing industry recognition (even at the national security 
level) that moving to memory-safe languages is crucial for software security. 
Pyrite offers these safety guarantees without the usual runtime overhead often 
associated with safe languages.

For critical systems (such as OS kernels, embedded controllers, or cryptographic 
modules), Pyrite's lack of hidden behavior and ability to work without a runtime 
make it suitable. You can predict worst-case execution times more easily - 
there's no garbage collector that might pause at an inopportune moment, and no 
JIT compilation hiccups. Allocation is deterministic in the sense that you control 
when and where it happens (or you avoid it entirely in critical sections). 

Pyrite avoids features that introduce non-deterministic performance, like 
unbounded recursion in templates or unpredictable exception handling. For example, 
exceptions can unwinding the stack taking varying amounts of time, but Pyrite's 
error handling is explicit and local (no unwinding unless you explicitly code it). 
This deterministic nature is important for real-time and high-assurance systems. 
In such systems, you also often need to avoid dynamic memory altogether; Pyrite 
allows that by using stack allocation and fixed-size structures, or custom 
allocators, and by not forcing any allocation behind the scenes.

On the reliability front, Pyrite's strong typing and ownership model mean many 
bugs are caught early in development (at compile time). Null pointer errors 
become compile-time issues (you can't use a value that could be None without 
handling it). Race conditions are prevented by design. The result is that Pyrite 
programs are more robust and require fewer security patches for memory safety 
issues. Major tech companies and even government cyber agencies have advocated 
for using memory-safe languages for new code, and Pyrite directly serves that 
agenda.

Furthermore, Pyrite's optional unsafe escape hatch means that if a security audit 
is performed, the focus can be on the isolated unsafe blocks of code. It's easier 
to review a small section of code that does manual memory handling than to worry 
about the entire codebase. For organizations, this provides confidence: as long 
as the safe code compiles, it's free of certain vulnerabilities, and only the 
unsafe parts need deep scrutiny. This compartmentalization of risk is a huge win 
for reliability.

In summary, Pyrite helps developers build secure and reliable software by 
construction. It dramatically reduces the chance of introducing the classic C 
bugs that often lead to exploits. And it does so while maintaining C-like 
performance and control, which means there's little excuse not to use it for 
systems programming from a security standpoint (one cannot argue "I need C for 
speed" - Pyrite gives you speed and safety).

================================================================================
14. IMPLEMENTATION ROADMAP
================================================================================

To achieve widespread developer adoption, Pyrite must deliver on both technical
excellence and developer experience. This section outlines the prioritized 
roadmap focused on reaching Beta Release (self-hosting capability).

14.1 Alpha Release: MVP (Minimum Viable Product)
--------------------------------------------------------------------------------

Goal: Prove the core concept with a usable, safe systems language

**Status:** âœ… COMPLETE (446 passing tests, December 2025)

Core Language:
  âœ“ Ownership and borrowing system (memory safety)
  âœ“ Basic types (integers, floats, bool, arrays, structs, enums)
  âœ“ Control flow (if, loops, match, functions)
  âœ“ Module system (imports, visibility)
  âœ“ Error handling (Result types, try operator)

Compiler:
  âœ“ Error messages with WHAT/WHY/HOW format (section 2)
  âœ“ Basic --explain system for common errors
  âœ“ Ownership error diagnostics with timeline views
  âœ“ Compile to native code (LLVM or MLIR backend)
      - LLVM: Proven, mature optimization infrastructure
      - MLIR: Better for heterogeneous computing (CPU+GPU)
      - Decision: Implementation detail, doesn't affect user-visible semantics
      - Both paths viable: Choose based on compiler team expertise and GPU timeline

Note on Compiler Backend Choice
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The choice between LLVM and MLIR is an **implementation question**, not a 
language design question:

**LLVM (Traditional Path):**
  Pros:
    â€¢ Mature optimization passes (proven in Clang, Rust, Swift)
    â€¢ Extensive documentation and community support
    â€¢ Excellent code generation for CPU targets
    â€¢ Well-understood by compiler engineers
  
  Cons:
    â€¢ GPU support requires more infrastructure work
    â€¢ Less flexible for custom dialects
    â€¢ Hetereogeneous computing requires more glue code

**MLIR (Modern Path):**
  Pros:
    â€¢ Built for heterogeneous computing (CPU+GPU unified)
    â€¢ Mojo proves it works for systems + ML domains
    â€¢ Better for custom optimization passes
    â€¢ Reusable infrastructure for domain-specific compilers
    â€¢ Natural fit for tensor operations and GPU kernels
  
  Cons:
    â€¢ Steeper learning curve for compiler implementation
    â€¢ Smaller community than LLVM
    â€¢ May constrain future flexibility in unexpected ways

**Recommendation:**
  â€¢ **For Beta Release:** LLVM (simpler, proven, sufficient for self-hosting)
  â€¢ **For GPU (Stable Release):** Consider MLIR if heterogeneous computing is priority
  â€¢ **If uncertain:** Start with LLVM, migrate later if needed

**Key insight:** This choice affects **how** the compiler is built, not **what** 
Pyrite looks like to users. All language features, semantics, and guarantees 
remain identical regardless of backend.

User-visible behavior (ownership, contracts, SIMD, blame tracking) is defined 
by Pyrite's semantics, not by LLVM or MLIR. The backend is an engineering 
decision for the compiler team, not a language design constraint.

**For this specification:** We assume LLVM for examples (proven path), but MLIR 
is equally valid if compiler team has expertise or GPU support is accelerated.

Tooling:
  âœ“ quarry new/build/run (basic workflow)
  âœ“ pyrite run (script mode - single file compilation with caching)  HIGH IMPACT
      - Zero-configuration single-file workflow
      - Automatic compilation and caching
      - Shebang support (#!/usr/bin/env pyrite)
      - Critical for "first 60 seconds" experience
  âœ“ quarry test (inline testing)
  âœ“ quarry fmt (official formatter, zero config)

Standard Library (Core):
  âœ“ Collections: List, Map, Set
  âœ“ String and StringBuilder
  âœ“ File I/O (read/write)
  âœ“ Basic math

Timeline: COMPLETE

Note: Script mode was prioritized in Alpha because it addresses the biggest 
adoption friction point for Python developers. The "first impression" workflow 
must feel familiar and frictionless.

14.2 Beta Release: Self-Hosting Capability
--------------------------------------------------------------------------------

Goal: Enable Pyrite compiler to be rewritten in Pyrite itself, eliminating Python dependency

**Critical Requirements for Beta:**
  1. Language features sufficient for compiler implementation
  2. 100% automated test coverage with all tests passing
  3. Cross-platform stability (Windows, Mac, Linux)
  4. No critical bugs in any environment

Language Features (CRITICAL for self-hosting):
  âœ“ Result[T, E] and Option[T] types  BLOCKING
      - Full error handling for compiler error reporting
      - try operator for error propagation
      - Pattern matching on Result/Option
      - Essential for all compiler passes
  âœ“ Traits and implementations  BLOCKING
      - Interface definitions (Display, Clone, Debug)
      - Method dispatch for compiler abstractions
      - Generic trait bounds (T: Display)
      - Required for extensible architecture
  âœ“ Advanced generics  BLOCKING
      - Associated types for collections
      - Where clauses for complex bounds
      - Higher-kinded types (if needed)
      - Compiler uses heavily for type representations
  âœ“ Full FFI support  BLOCKING
      - extern fn declarations for LLVM API
      - Struct layout compatibility with C
      - Function pointer types for callbacks
      - Required for LLVM bindings
  âœ“ defer and with statements  CORE PYRITE
      - Guaranteed cleanup for resource management
      - Python-familiar with statement
      - Required for file handling in compiler
  âœ“ Compile-time parameterization  ALREADY IMPLEMENTED
      - [N: int] syntax working
      - Function specialization working
      - Used for fixed-size buffers
  âœ“ Two-tier closure model  FOUNDATIONAL
      - Parameter closures (fn[...]) for algorithmic helpers
      - Runtime closures (fn(...)) for callbacks
      - Enables zero-cost abstractions

Compiler Enhancements:
  âœ“ Incremental compilation  CRITICAL FOR PRODUCTIVITY
      - Module-level caching with dependency tracking
      - 15-27x faster rebuilds essential for large codebase
      - Rewriting compiler will be ~10K lines, needs fast iteration
  âœ“ Cost transparency warnings  HELPFUL
      - Heap allocations, large copies
      - Performance lints
  âœ“ Expanded --explain coverage  QUALITY
      - All error codes documented
  âœ“ Enhanced ownership visualizations  NICE TO HAVE
      - Flow diagrams with --visual flag

Language Features (REQUIRED for compiler implementation):
  âœ“ Full Result[T, E] and Option[T] implementation  BLOCKING
      - Error handling throughout compiler
      - try operator for error propagation
      - Comprehensive pattern matching
      - Integration with ownership system
  âœ“ Traits and trait implementations  BLOCKING
      - Define interfaces (Display, Clone, Debug, Iterator)
      - Implement traits for all AST node types
      - Method dispatch for polymorphic code
      - Trait bounds in generics (T: Display)
  âœ“ Advanced generics  BLOCKING
      - Generic structs for compiler data structures
      - Generic enums for Result/Option
      - Associated types (Iterator::Item)
      - Where clauses for complex bounds
  âœ“ Improved FFI support  BLOCKING
      - extern fn with full ABI control
      - Opaque types for C pointers
      - Function pointers for LLVM callbacks
      - Required for LLVM bindings throughout compiler
  âœ“ String manipulation  BLOCKING
      - String formatting (format!() macro or equivalent)
      - String concatenation and slicing
      - UTF-8 handling for source code
      - Required for error messages and code generation
  âœ“ File I/O (enhanced)  BLOCKING
      - Read/write source files
      - Directory traversal for module system
      - Path manipulation for cross-platform
      - Required for multi-file compilation
  âœ“ HashMap/Dictionary  BLOCKING
      - Symbol tables (name â†’ type mapping)
      - Type environment (variable â†’ ownership state)
      - Module registry (path â†’ AST)
      - Essential for all compiler passes
  âœ“ defer and with statements  ALREADY IMPLEMENTED
      - Scope-based cleanup (~95% complete)
      - File resource management (with files)
      - Part of Core Pyrite
  âœ“ Compile-time parameters  ALREADY IMPLEMENTED
      - [N: int] syntax functional
      - Used for compiler optimizations
  âœ“ Two-tier closures  FOUNDATIONAL
      - Parameter closures working
      - Runtime closures functional
      - Required for compiler abstractions

Compiler Quality:
  âœ“ Incremental compilation  CRITICAL
      - Fast rebuilds for 10K+ line compiler codebase
      - Module-level caching required
  âœ“ Enhanced error messages  MAINTAINED
      - WHAT/WHY/HOW format continues
      - All error codes documented
  âœ“ Full test coverage  100% REQUIRED
      - Every compiler pass tested
      - All language features covered
      - Cross-platform test suite (Windows, Mac, Linux)
      - Zero failing tests before beta

Tooling (ESSENTIAL for development):
  âœ“ quarry build/run/test/fmt/clean  WORKING
      - Core workflow already functional
  âœ“ quarry cost  HELPFUL
      - Identify performance issues in new compiler
  âœ“ quarry bloat  HELPFUL
      - Track compiler binary size
  âœ“ quarry fix --interactive  NICE TO HAVE
      - Auto-fix for common errors
  âœ“ quarry explain-type  HELPFUL
      - Type introspection during development

Standard Library (REQUIRED for compiler):
  âœ“ Collections: List, Map, Set  ALREADY IMPLEMENTED
      - Compiler uses extensively
  âœ“ String and StringBuilder  ALREADY IMPLEMENTED
      - Source code manipulation
  âœ“ File I/O (enhanced)  NEEDS WORK
      - Multi-file reading
      - Directory operations
      - Path handling
  âœ“ JSON/TOML parsing  OPTIONAL
      - For Quarry.toml if needed
  âœ“ HashMap[K, V]  CRITICAL
      - Symbol tables
      - Type environments

Timeline: 6-12 months after Alpha (focus on self-hosting requirements)

14.3 Stable Release: Advanced Features and Ecosystem
--------------------------------------------------------------------------------

Goal: Production-ready for all domains + advanced optimization features

Advanced Language Features:
  âœ“ Cost transparency attributes (@noalloc, @nocopy, @nosyscall)
  âœ“ @cost_budget attribute (performance contracts)
      - Compile-time enforcement of cycles/allocs/stack budgets
      - Critical for real-time and safety-critical systems
      - Transforms requirements into compiler-verified contracts
  âœ“ @noalias attribute for non-aliasing assertions
      - Enables aggressive compiler optimizations
      - Checked at compile-time when possible, runtime in debug
      - 5-15% speedups for memory-bound operations
      - Expert-level feature for performance-critical code
  âœ“ Compile-time execution (const functions, comptime evaluation)
  âœ“ Async/await with structured concurrency
      - async with blocks ensure no leaked tasks
      - Addresses Rust's main async criticism
      - Inspired by Swift/Kotlin structured concurrency
  âœ“ Design by Contract support
      - @requires, @ensures, @invariant for logical correctness
      - Compile-time verification when provable
      - Runtime checking in debug builds (zero cost in release)
      - Integration with SMT solvers (Z3, CVC5)
      - Required for highest safety certification levels

Advanced Tooling:
  âœ“ pyrite repl  ADOPTION ACCELERATOR
      - Interactive development for learning
      - Ownership visualization
      - Instant feedback
  âœ“ quarry learn  LEARNING SYSTEM
      - Interactive exercises
      - Progressive tutorials
  âœ“ quarry bench (benchmarking framework)
  âœ“ quarry audit (security vulnerability scanning)
  âœ“ quarry bindgen (C header â†’ Pyrite bindings)
      - Automatic binding generation from C headers
      - Zig-style header parsing (no manual declarations)
      - Critical for ecosystem bootstrapping
      - Enables effortless use of existing C libraries
  âœ“ quarry energy  UNIQUE DIFFERENTIATOR
      - Energy profiling and battery-life optimization
      - Sustainability awareness (no competitor has this)
      - Critical for mobile, IoT, green computing
      - Platform-specific power measurement APIs
  âœ“ quarry dev --hot-reload  DEVELOPER JOY
      - Live code updates without restart
      - State preservation across reloads
      - Accelerates iteration for games, web, data processing
  âœ“ quarry perf with Perf.lock  PERFORMANCE TRACKING
      - Flamegraph profiling
      - Regression detection
      - Assembly diffs
  âœ“ quarry tune  OPTIMIZATION HELPER
      - Intelligent suggestions
      - Automated fixes
  âœ“ quarry fix --interactive  UX ENHANCEMENT
      - Numbered fix selection
      - Learning through fixing
  âœ“ quarry layout / explain-aliasing  ADVANCED INTROSPECTION
      - Cache-line analysis
      - Padding visualization
      - Aliasing assumptions
  âœ“ Community metrics dashboard (aspirational: quarry.dev/metrics)
      - Public, real-time performance and adoption data
      - Makes developer adoption metrics measurable
      - Evidence-based advocacy enabler
  âœ“ Internationalized error messages
      - Native language compiler diagnostics
      - Chinese, Spanish, Hindi, Japanese, and more
      - Professional translations by native speakers
  âœ“ LSP with full hover/goto/completion
  âœ“ Debugger integration (lldb/gdb support)

Standard Library Expansion:
  âœ“ JSON and TOML parsing
      - Configuration file handling
  âœ“ HTTP client
      - Network operations
  âœ“ Basic HTTP server
      - Simple server applications
  âœ“ DateTime and Duration
      - Time handling
  âœ“ Regex support
      - Pattern matching
  âœ“ CLI argument parsing
      - Command-line tools
  âœ“ Async runtime with structured concurrency
      - High-concurrency applications
  âœ“ Observability (logs, traces, metrics)
      - Production monitoring
  âœ“ Database drivers (common DBs)
      - Data persistence
  âœ“ Compression (gzip, etc.)
      - Data compression
  âœ“ Cryptography (hashing, encryption)
      - Security primitives
  âœ“ Tensor type for numerical computing (std::numerics)
      - Compile-time shape checking (Tensor[T, (M, N), Layout])
      - Explicit layout control (RowMajor, ColMajor, Strided)
      - Zero-cost slicing through borrowing (TensorView)
      - Integration with SIMD, tiling, and parallelization
      - Fixed-size (stack) and dynamic-size (heap) variants
      - Not a full ML framework - memory layout + indexing foundation
  âœ“ SIMD module (std::simd)
      - Portable vector types (Vec2, Vec4, Vec8, Vec16)
      - Platform-specific SIMD width detection
      - Explicit SIMD operations (no auto-vectorization magic)
      - Integration with compile-time parameterization
      - CPU multi-versioning (@multi_version attribute)
        â€¢ Ship single binary, run fast everywhere
        â€¢ Automatic runtime dispatch (SSE2/AVX2/AVX-512/NEON)
        â€¢ 2-4x speedup on modern CPUs vs baseline
        â€¢ Extended beyond SIMD: POPCNT, BMI2, AES-NI, etc.
        â€¢ Cross-architecture support (x86-64, ARM64, RISC-V)
        â€¢ Feature-specific optimization variants
  âœ“ Algorithmic helpers (std::algorithm)
      - vectorize[width=auto] - Ergonomic SIMD loop generation
      - parallelize[workers=auto] - Structured parallel execution
      - tile[block_size=auto] - Cache-aware blocking
      - All use parameter closures (fn[...]) for verified zero-cost abstraction
      - Compose naturally (parallel + SIMD + tiling)
      - Maintains cost transparency (still explicit, now with guaranteed zero-cost)
  âœ“ Inline storage containers
      - SmallVec[T, N] - Stack-allocated up to N elements
      - SmallString[N] - Small string optimization (SSO)
      - InlineMap[K, V, N] - Inline hash map for small collections

Ecosystem:
  âœ“ Pyrite Playground (aspirational: play.pyrite-lang.org)
      - Browser-based experimentation
  âœ“ 1000+ packages on Quarry Registry
      - Community contributions
  âœ“ Real-world case studies
      - Production deployments
  âœ“ Corporate adoption stories
      - Enterprise validation
  âœ“ Supply-chain security tooling
      - quarry audit - Vulnerability scanning with CVE database
      - quarry vet - Dependency review workflow
      - quarry sign/verify - Cryptographic package signing
      - quarry sbom - Software Bill of Materials

Formal Methods:
  âœ“ Formal semantics specification (Section 15)
      - Operational semantics for ownership and memory model
      - Axiomatic semantics for verification
      - Memory safety theorem and proof sketch
      - Undefined behavior catalog
      - Required for DO-178C Level A, Common Criteria EAL 7
      - Enables external verification tools (Coq, Isabelle, Lean)
      - Academic research foundation

Timeline: 12-24 months after Beta Release

14.4 Future Release: Heterogeneous Computing (GPU Support)
--------------------------------------------------------------------------------

Goal: Extend Pyrite's "fast everywhere" promise from CPU to GPU

Language Features:
  âœ“ @kernel attribute for GPU-executable functions  FLAGSHIP FEATURE
      - Kernel contract enforcement (@noalloc, @no_panic, @no_recursion)
      - Call-graph blame tracking for GPU constraints
      - Single-source CPU/GPU code with same safety guarantees
      - Explicit kernel boundaries (no automatic offloading)
  âœ“ GPU memory management primitives
      - Explicit host â†” device memory transfer
      - Device pointer types (distinct from host pointers)
      - RAII wrappers for automatic cleanup
  âœ“ GPU launch API
      - Explicit kernel launch with thread/block configuration
      - Synchronization primitives
      - Multi-GPU support

Compiler:
  âœ“ GPU backend: CUDA (NVIDIA)  PRIORITY 1
      - 80% GPU market share
      - PTX code generation via LLVM
      - CUDA runtime integration
  âœ“ GPU backend: HIP (AMD)  PRIORITY 2
      - ROCm support for AMD GPUs
      - Shared implementation with CUDA where possible
  âœ“ GPU backend: Metal (Apple)  PRIORITY 3
      - Apple Silicon GPU support
      - Metal Shading Language generation
  âœ“ GPU backend: Vulkan Compute  CROSS-VENDOR
      - Portable compute shaders
      - Runs on any Vulkan-capable GPU

Tooling:
  âœ“ quarry build --gpu=cuda/hip/metal/vulkan
  âœ“ GPU profiling integration (nvprof, rocprof, Instruments)
  âœ“ Kernel performance analysis tools

Standard Library:
  âœ“ std::gpu module
      - Launch primitives
      - Memory management
      - Device pointer types
      - Multi-GPU abstractions
  âœ“ Python interoperability (std::python)  STRATEGIC
      - Call Python from Pyrite with explicit GIL boundaries
      - Generate Python extension modules (quarry pyext)
      - Type-safe conversions at boundaries (explicit, cost-transparent)
      - Zero-copy where possible (NumPy array â†” Pyrite slice)
      - Optional dependency (Python runtime not required for core Pyrite)
      - Adoption wedge for numerical computing (migrate Python bottlenecks)

Why Future Release (Not Beta/Stable)
~~~~~~~~~~~~~~~~~~~~~~~~~~

GPU support is delayed because:
  1. Requires stable CPU-side language (ownership, contracts, blame tracking)
  2. Needs mature tooling (quarry cost, quarry perf, error diagnostics)
  3. GPU APIs are complex-rushing leads to poor design
  4. CPU-only Pyrite already serves 90%+ of use cases
  5. Not required for self-hosting compiler

By waiting, Pyrite can apply lessons learned from CPU development to GPU:
  - Contract system is proven on CPU â†’ extends naturally to GPU
  - Blame tracking teaches CPU restrictions â†’ same for GPU restrictions
  - Performance tooling is mature â†’ adapts to GPU profiling

The result: GPU support that feels like "more Pyrite" rather than "different 
language on GPU."

Impact
~~~~~~

GPU support positions Pyrite as: "The systems language that scales from 
embedded microcontrollers to GPU computing, with the same safety and explicitness 
everywhere."

This capability opens use cases not addressed in Beta/Stable:
  - Deep learning inference/training
  - Scientific computing at scale
  - Real-time graphics
  - Cryptographic acceleration
  - Financial modeling

Timeline: 24+ months after Stable Release (CUDA first, others follow)

14.5 Success Metrics
--------------------------------------------------------------------------------

Tracking Progress Toward "Most Admired"
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Quantitative:
  â€¢ Stack Overflow survey ranking (target: top 10 admired within 5 years)
  â€¢ GitHub stars (target: 50k+ within 3 years)
  â€¢ Quarry Registry packages (target: 10k+ within 5 years)
  â€¢ Active contributors (target: 500+ within 3 years)
  â€¢ Production deployments (target: 1000+ companies within 5 years)

Qualitative:
  â€¢ "Compiler errors taught me ownership" (learning indicator)
  â€¢ "Moved from Rust because syntax is clearer" (differentiation success)
  â€¢ "Built a kernel in Pyrite, no memory bugs" (safety validation)
  â€¢ "Faster than C, safer than Rust, easier than both" (positioning achieved)

Critical Success Factors
~~~~~~~~~~~~~~~~~~~~~~~~~

The difference between "interesting experiment" and widespread developer adoption is:

1. **Compiler diagnostics quality** (highest ROI)
   Poor errors = frustration, abandonment
   Great errors = learning, delight, advocacy

2. **Tooling excellence** (Quarry must match Cargo)
   Bad tooling = friction, slow adoption
   Great tooling = "it just works," viral growth

3. **Standard library breadth** (batteries included)
   Missing stdlib = dependency hell, decision fatigue
   Complete stdlib = ship real projects immediately

4. **Documentation and learning path** (Playground, tutorials)
   Hard to learn = niche adoption, experts only
   Easy to learn = broad adoption, beginners welcome

5. **Performance delivery** (match C in real benchmarks)
   Slow code = "safety has a cost" narrative loses
   Fast code = "no compromises" narrative wins

All five must be excellent. Excellence in four of five yields "good language." 
Excellence in all five yields a language that achieves widespread developer adoption.

================================================================================
15. HIGH-ROI FEATURES SUMMARY
================================================================================

Based on analysis of what makes languages successful and widely adopted, Pyrite incorporates
these highest-impact features that differentiate it from competitors:

15.1 Predictability: Fully-Defined Evaluation Order
--------------------------------------------------------------------------------

Unlike C/C++, Pyrite guarantees left-to-right evaluation order for all 
expressions and function arguments. This eliminates undefined behavior, makes 
debugging predictable, and removes cognitive load - at zero performance cost.

Impact: Reduces "mystery" and friction immediately for all developers.

15.2 Teaching Compiler: Interactive quarry fix
--------------------------------------------------------------------------------

The compiler doesn't just diagnose problems-it presents numbered, selectable 
fixes with quarry fix --interactive. Each option explains when to use it, what 
the trade-offs are (e.g., "allocates and copies"), and how it affects program 
behavior. Non-interactive mode applies the safest/cheapest fix automatically.

Impact: Fastest learning-curve reduction for ownership concepts. The interactive 
selection teaches patterns organically - after fixing 20 borrow errors with option 
1 ("use a reference"), the pattern clicks. This is what makes tools feel 
"magical" (like rust-analyzer, Elm, go fmt).

15.3 Cost Transparency: Multi-Level quarry cost
--------------------------------------------------------------------------------

Progressive cost reporting (--level=beginner/intermediate/advanced) teaches 
performance intuition gradually. Beginner mode shows only "this allocates" and 
"this copies 4KB" without overwhelming detail. Intermediate adds counts and loop 
multiplication. Advanced shows full call chains, dispatch sites, and syscalls. 
Machine-readable JSON output enables IDE integration and CI/CD gates.

Impact: Completes the transparency story - from gentle hints for beginners to deep 
analysis for experts. The progressive disclosure prevents overwhelm while 
enabling mastery. IDE integration makes costs visible inline ("ðŸ’° 1KB allocation").

15.4 Guaranteed Cleanup: defer Statement
--------------------------------------------------------------------------------

First-class defer statement ensures cleanup code runs at scope exit, regardless 
of error paths. Complements RAII for procedural code, C FFI resources, and 
explicit cleanup sequences.

Impact: Beginners love "cleanup always runs" guarantee. Fills gap where RAII is 
awkward without adding complexity.

15.5 Visual Learning: Enhanced Ownership Flow Diagrams
--------------------------------------------------------------------------------

Compiler errors for ownership/borrowing include ASCII art flow diagrams showing 
exactly when values move, when borrows start/end, and where conflicts occur. 
Enhanced --visual mode provides interactive diagrams.

Impact: Transforms the hardest concept (ownership) from abstract to concrete. 
"See the flow" beats "imagine the flow."

15.6 Ecosystem Acceleration: quarry bindgen (Stable Release)
--------------------------------------------------------------------------------

Automatic Pyrite binding generation from C headers (Zig-style). Parse headers 
and generate safe wrappers without manual FFI declarations.

Impact: Critical for ecosystem bootstrapping. Instant access to existing C 
libraries removes the "no libraries yet" adoption barrier.

15.7 Type Introspection: quarry explain-type
--------------------------------------------------------------------------------

The quarry explain-type command displays standardized "type badges" ([Stack], 
[Heap], [Copy], [Move], [MayAlloc], etc.) and memory characteristics in plain 
language. Shows size, alignment, behavior, and performance costs for any type.

Impact: Operationalizes "Intuitive Memory Model for Learners" (section 1.5). 
Every type becomes self-documenting. Beginners understand WHERE data lives, HOW 
it behaves, and WHAT it costs through concrete examples rather than abstract 
theory.

15.8 Beginner-Friendly Aliases: Text and Bytes
--------------------------------------------------------------------------------

Optional type aliases (type Text = &str, type Bytes = &[u8]) make borrowed 
views more intuitive for newcomers without fragmenting the type system.

Impact: Lowers barrier for Python/JavaScript developers. Zero cost, purely 
pedagogical.

15.9 Compile-Time Parameterization
--------------------------------------------------------------------------------

Functions and types can accept compile-time parameters in square brackets 
([N: int]), enabling specialization for specific constant values. The compiler 
generates optimized versions per parameter (loop unrolling, dead code 
elimination, SIMD). Inspired by Mojo's parameter system but with Pyrite's 
explicit syntax.

Impact: Enables C++-level performance optimizations with clear, teachable syntax. 
Critical for high-performance libraries (math, graphics, crypto) without 
sacrificing readability. Beta Release feature.

15.10 Runtime Profiling Suite
--------------------------------------------------------------------------------

Comprehensive profiling commands that complement static cost analysis:

  â€¢ quarry perf - Integrated flamegraph profiling across platforms
  â€¢ quarry alloc - Heap allocation profiling with call stacks
  â€¢ quarry pgo - One-command profile-guided optimization
  â€¢ quarry tune - Correlates static + runtime data for actionable suggestions

Impact: Closes the loop from "what could be expensive" (static) to "what IS 
expensive" (runtime). The quarry tune command synthesizes all data and suggests 
specific fixes with estimated improvements. This transforms performance 
optimization from art to science-mechanical, measurable, teachable. Beta Release 
feature.

15.11 Two-Tier Closure Model: Parameter vs Runtime
--------------------------------------------------------------------------------

Explicit distinction between compile-time and runtime closures:

  â€¢ **Parameter closures** (fn[...]) - Compile-time, always-inline, zero-alloc, 
    used for vectorize/parallelize/tile
  â€¢ **Runtime closures** (fn(...)) - First-class values, can escape, can allocate, 
    used for callbacks/threads

Syntax makes cost explicit:

    # Zero-cost (parameter closure)
    algorithm.vectorize[width=8](n, fn[i: int]: data[i] *= 2.0)
    
    # May allocate (runtime closure)  
    Thread.spawn(fn(): process_background())

Benefits:
  â€¢ Verifiable --no-alloc mode (parameter closures don't allocate)
  â€¢ quarry cost precision (distinguish zero-cost from potential-cost)
  â€¢ Teaching clarity ("square brackets = free, parentheses = may cost")
  â€¢ Zero-cost abstractions are provably zero-cost (not optimization-dependent)

Impact: Fills the biggest gap identified in current spec. Enables bulletproof 
"zero-cost abstraction" claims for algorithmic helpers. Makes --no-alloc 
verification complete (no hidden allocation through closures). Inspired by Mojo's 
parameter closures but with Pyrite's explicit syntax philosophy. Beta Release flagship 
feature (foundation for verifiable performance).

15.12 Algorithmic SIMD Helpers
--------------------------------------------------------------------------------

Mojo-inspired ergonomic helpers for SIMD and parallelism, powered by parameter 
closures:

  â€¢ std::algorithm::vectorize[width=auto] - Scalar logic â†’ SIMD performance
  â€¢ std::algorithm::parallelize[workers=auto] - Safe structured parallelism
  â€¢ Compose naturally (parallel + SIMD = maximum hardware utilization)
  â€¢ Parameter closures (fn[...]) guarantee zero allocation

Impact: Makes high-performance programming accessible to intermediate developers 
without sacrificing transparency. Helpers desugar to explicit std::simd code 
(inspectable with quarry expand). Parameter closure foundation ensures truly 
zero-cost abstractions-not "usually optimized," but "provably zero-cost." The 
"pit of success" for performance: write clear scalar logic, get SIMD speed. 
Beta Release feature (after parameter closures are stable).

15.13 Performance Cookbook: Stdlib as Learning Resource
--------------------------------------------------------------------------------

Standard library serves as interactive performance education with canonical 
implementations, benchmarks, and "why it's fast" explanations:

  â€¢ Every performance-critical function documented with complexity, allocation 
    counts, and benchmark numbers
  â€¢ Built-in benchmark harness (quarry bench std::sort verifies on your hardware)
  â€¢ Complete examples with quarry cost + quarry perf output shown
  â€¢ Cookbook repository (docs/cookbook/) with 50+ canonical patterns
  â€¢ Cross-linked documentation (matrix multiply â†’ tiling â†’ SIMD â†’ caching)

Example documentation quality:

    """
    std::sort - O(n log n) pattern-defeating quicksort
    Allocations: 0 (in-place), Stack: O(log n)
    Performance: 380 Î¼s for 10k i32 on i9-12900K
    Why fast: SIMD comparisons, branch prediction, cache locality
    Benchmark: quarry bench std::sort
    """

Impact: Transforms "performance is possible" into "performance is the default" by 
providing concrete, proven templates. Answers "how do I make MY code fast?" with 
runnable examples and verifiable benchmarks. Makes intermediate developers 
productive in high-performance domains without deep expertise. Stable Release 
implementation (core examples initially, expand over time).

15.14 Inline Storage Containers
--------------------------------------------------------------------------------

Stdlib containers that avoid heap allocation for common small cases:

  â€¢ SmallVec[T, N] - Stack-allocated up to N elements
  â€¢ SmallString[N] - Small string optimization (SSO)
  â€¢ InlineMap[K, V, N] - Inline hash maps for small collections

Impact: Addresses the "most collections are small" reality. Profiling shows 90% 
of lists have < 10 items - SmallVec[T, 8] eliminates 90% of allocations with zero 
API changes. Integrated with quarry tune for automatic suggestions ("median 
size=3 â†’ use SmallVec[T, 6]"). Proven pattern from Rust (smallvec crate) and 
C++ (llvm::SmallVector). Stable Release feature.

15.15 Interactive Learning System
--------------------------------------------------------------------------------

Built-in Rustlings-style interactive exercises via quarry learn:

  â€¢ Structured learning path (basics â†’ ownership â†’ concurrency â†’ SIMD)
  â€¢ Hands-on exercises with immediate feedback
  â€¢ Progressive hints (never stuck, never spoiled)
  â€¢ Synthesis projects to apply concepts
  â€¢ Integrated with compiler errors (error suggests relevant exercises)

Impact: Transforms the learning curve from "insurmountable wall" to "guided 
path." Research suggests that interactive practice is more effective than passive 
reading for systems concepts. Rustlings is consistently praised as 
Rust's best learning resource - Pyrite adopts this proven approach as first-class 
feature. Critical adoption accelerator. Stable Release feature.

15.16 With Statement - Python Familiarity
--------------------------------------------------------------------------------

Resource management syntax sugar that desugars to try + defer:

    with file = try File.open("config.txt"):
        for line in file.lines():
            process(line)
    # Automatically closes via defer

Zero cost, familiar to Python/JavaScript developers, teaches underlying 
mechanisms through quarry expand.

Impact: Removes friction for 80%+ of beginners who know Python's with statement. 
Zero new semantics (pure sugar over existing defer), but massive ergonomic win. 
Alpha Release feature.

15.17 Views-by-Default API Convention
--------------------------------------------------------------------------------

Hard stdlib rule: APIs take borrowed views (&T, &str, &[T]) by default; 
ownership-taking requires explicit documentation and @consumes annotation.

    # Standard (90% of APIs)
    fn parse(content: &str) -> Result[Data, Error]
    fn process(items: &[Item]) -> Summary
    
    # Rare (ownership justified and documented)
    @consumes
    fn take_ownership(data: List[int]) -> ProcessedData

Impact: Prevents the #1 beginner frustration ("why did this move?"). Makes 
borrowing the default, ownership transfer the exception. Single convention 
eliminates entire class of confusion. Alpha Release stdlib design principle.

15.18 Zero-Allocation Mode
--------------------------------------------------------------------------------

Build flag that makes heap allocation a compile error:

    quarry build --no-alloc
    
    error: heap allocation in no-alloc mode
      ----> src/main.pyr:42
       | let v = List[int].new()  # <---- forbidden

Provable no-heap for embedded systems, safety-critical software, and 
certification requirements. Works seamlessly with parameter closures (which are 
guaranteed zero-allocation).

Impact: Makes Pyrite credible for aerospace, medical devices, microcontrollers. 
Differentiates from Rust (which has no similar mode). Required for safety 
certification in many industries. Two-tier closure model makes --no-alloc 
verification complete (no hidden allocation through closures). Beta Release feature.

15.19 Performance Budget Contracts
--------------------------------------------------------------------------------

@cost_budget attribute enforces compile-time performance limits:

    @cost_budget(cycles=100, allocs=0, stack=1024)
    fn parse_packet(data: &[u8]) -> Result[Packet, Error]:
        # Compiler enforces budget or compilation fails

Transforms "performance requirements" from documentation into compiler-verified 
contracts.

Impact: Critical for real-time systems, safety-critical software, and 
high-frequency trading. Performance becomes correctness. Enables certification. 
Beta Release feature.

15.20 Cache-Aware Tiling
--------------------------------------------------------------------------------

algorithm.tile helper for cache-friendly blocking using parameter closures:

    algorithm.tile[block_size=64](rows, cols, fn[i_block: int, j_block: int]:
        # Process 64x64 tile that fits in L1 cache
        ...
    )

Typical 15x speedup for matrix/numeric operations by keeping data in L1 cache. 
Parameter closure ensures zero allocation overhead.

Impact: Closes the "scalar loops â†’ production performance" gap for numeric code. 
Teaches cache hierarchy naturally through use. Composable with vectorize + 
parallelize. Stable Release feature.

15.21 CPU Multi-Versioning
--------------------------------------------------------------------------------

@multi_version attribute generates multiple SIMD variants with automatic runtime 
dispatch:

    @multi_version(baseline="sse2", targets=["avx2", "avx512"])
    fn process_pixels(data: &mut [f32]):
        # Compiler generates 3 versions, runtime picks best

Ship one binary that runs optimally on all CPUs (2-4x speedup on modern 
hardware vs baseline).

Impact: "Fast everywhere" without user configuration. Critical deployment 
advantage. No other systems language makes this so easy. Stable Release flagship 
feature.

15.22 Structured Concurrency for Async
--------------------------------------------------------------------------------

async with blocks ensure all spawned tasks complete before scope exit:

    async fn process():
        async with:
            spawn fetch_user()
            spawn fetch_orders()
        # Both complete here - no leaked tasks

Prevents common async bug: fire-and-forget tasks that outlive their purpose.

Impact: Addresses Rust's main async criticism (easy to leak tasks). Swift and 
Kotlin prove this pattern works. Makes Pyrite async: "Zero-cost like Rust, but 
safe by default." Stable Release feature.

15.23 Call-Graph Blame Tracking
--------------------------------------------------------------------------------

Performance contract violations show complete call chain and identify which 
function caused the violation:

    error[P0601]: @noalloc violation
      |
      = note: Allocation chain:
        1. safe_compute() [your code, marked @noalloc]
           â†’ calls math::advanced_sqrt() [external]
        2. advanced_sqrt() at lib.py:89
           â†’ allocates Vec[f64] [VIOLATES @noalloc]

Shows exactly where in the call graph contracts are violated, across module and 
crate boundaries.

Impact: Transforms performance contracts from "catch violations" to "understand 
why and fix at the source." Makes @noalloc/@cost_budget practical for large 
codebases. The killer feature that makes performance contracts composable. Beta Release 
flagship feature.

15.24 IDE Hover Integration
--------------------------------------------------------------------------------

Rich tooltips in IDE show ownership state, memory layout, and performance costs 
in real-time:

    let data = List[int]([1, 2, 3])
         ^^^^
         Hover: [Heap] [Move] [MayAlloc]
                Stack: 24 bytes, Heap: 12 bytes
                Owner: 'data', Not moved, Not borrowed
                âš ï¸  Passing will move (use &data to borrow)

After move, hover shows: "MOVED on line 5 to 'process', Cannot be used."

Impact: Makes abstract concepts (ownership, memory layout, cost) visible while 
coding. Research suggests that visual feedback can accelerate learning (specific studies show 
improvements in the 40-60% range, though results vary by study methodology). The missing 
piece in teaching systems programming. Beta Release high-impact feature.

15.25 Explicit Loop Unrolling
--------------------------------------------------------------------------------

@unroll attribute provides explicit control over loop unrolling with compiler--
enforced safety limits:

    @unroll(factor=4)
    fn process_array(data: &mut [f32]):
        for i in 0..data.len():
            data[i] = data[i] * 2.0 + 1.0

Compiler warns on excessive unroll factors and prevents code size explosion. 
Integrates with compile-time parameters and @simd.

Impact: Fills gap between compiler auto-optimization and manual assembly. Provides 
explicit control for performance-critical kernels while maintaining safety. Beta Release 
performance feature.

15.26 LTO and Peak Performance Mode
--------------------------------------------------------------------------------

First-class Link-Time Optimization with simple flags:

    quarry build --release --lto=thin    # Fast builds, good optimization
    quarry build --peak                  # Maximum: LTO + PGO automatically

--peak mode automates the complete optimization pipeline (thin LTO + PGO training + 
full LTO + PGO final) in one command.

Impact: Makes "absolute best performance" a one-command operation. LTO adds 15-25% 
improvement, combined with PGO yields 30-50% total improvement vs plain release. 
Eliminates tedium while delivering maximum performance. Beta Release high-impact feature.

15.27 Extended CPU Multi-Versioning
--------------------------------------------------------------------------------

@multi_version supports arbitrary CPU features beyond SIMD:

    @multi_version(
        baseline="x86-64",
        targets=["x86-64-v2", "x86-64-v3", "x86-64-v4"]
    )
    fn hash_data(data: &[u8]) -> u64:
        # Uses POPCNT, BMI2, AES-NI when available

Automatic runtime dispatch for POPCNT, BMI2, AES-NI, and other CPU extensions. 
Cross-architecture support (x86-64, ARM64, RISC-V).

Impact: Extends "fast everywhere" beyond SIMD to general CPU features. Single 
binary optimized for baseline and modern CPUs. No other systems language makes 
this so comprehensive. Stable Release flagship enhancement.

15.28 Safe Accessors with Optimizer Elision
--------------------------------------------------------------------------------

Collection APIs provide bounds-checked accessors that optimize to zero cost when 
compiler proves safety:

    arr.get(i)       # Returns Optional[T], never panics (Core default)
    arr[i]           # Bounds-checked, optimizer elides when provable
    
    for i in 0..arr.len():
        arr[i]       # Bounds check ELIDED (loop proves safety)

quarry cost shows which bounds checks remain vs. elided.

Impact: Beginners learn safe patterns (Optional handling) while understanding that 
safe and fast aren't opposed-compiler makes safe code fast when it can prove 
correctness. Teaches performance intuition organically. Alpha/Beta stdlib design 
principle.

15.29 Learning Profile Mode
--------------------------------------------------------------------------------

One-command beginner-friendly setup:

    quarry new --learning my_project

Packages existing features into a "beginner bundle":
  â€¢ Enables --core-only mode (rejects advanced features)
  â€¢ Sets beginner lint level
  â€¢ Forbids unsafe by default
  â€¢ Includes extra diagnostics and hover help

Natural graduation path as skills advance - all code remains valid Pyrite.

Impact: "Pyrite has a beginner mode" is a powerful marketing message for Python 
developers. Zero new language complexity (just configuration). One command removes 
onboarding friction. Stable Release feature (after core compiler is stable).

15.30 Tensor Type for Numerical Computing
--------------------------------------------------------------------------------

First-class tensor type with compile-time shape checking:

    let image = Tensor[f32, (1024, 768), RowMajor]::zeros()
    let transformed = matmul(&weights, &image)  # Shapes checked at compile time

Features:
  â€¢ Compile-time shape verification (prevents dimension mismatch bugs)
  â€¢ Explicit layout control (RowMajor, ColMajor, Strided)
  â€¢ Zero-cost slicing through borrowing (TensorView[T, Shape])
  â€¢ Integration with SIMD, tiling, and parallelization (using parameter closures)
  â€¢ Fixed-size (stack) and dynamic-size (heap) variants

Impact: Makes Pyrite credible for numerical computing, scientific computing, and 
ML inference without becoming "yet another ML framework." Provides the foundation 
(memory layout + indexing); libraries build on top. Fills gap between "write loops" 
and "use heavyweight framework." Stable Release feature (after SIMD and algorithmic 
helpers).

15.31 Noalias/Restrict Semantics
--------------------------------------------------------------------------------

Expert-level optimization for asserting non-aliasing:

    @noalias
    fn process(a: &mut [f32], b: &mut [f32]):
        # Compiler assumes a and b don't overlap
        # Enables aggressive optimizations

Checked at compile-time when possible, runtime in debug builds. Enables:
  â€¢ More aggressive vectorization
  â€¢ Elimination of redundant loads
  â€¢ Advanced loop transformations
  â€¢ 5-15% speedups for memory-bound operations

Impact: Fills niche gap for cases where ownership can't prove disjointness 
(multiple immutable refs, FFI pointers). Expert optimization with explicit contract. 
quarry cost shows when @noalias would help. Stable Release feature (expert-only, after 
ownership is solid).

15.32 GPU Computing Support
--------------------------------------------------------------------------------

Extend Pyrite's contract system to GPU kernels:

    @kernel
    fn saxpy[N: int](a: f32, x: &[f32; N], y: &mut [f32; N]):
        let idx = gpu::thread_id()
        if idx < N:
            y[idx] = a * x[idx] + y[idx]

Kernel contracts automatically enforced:
  â€¢ @noalloc - No heap allocation (GPU has no allocator)
  â€¢ @no_panic - No panic/abort
  â€¢ @no_recursion - No recursion
  â€¢ @no_syscall - No system calls

Call-graph blame tracking shows *why* code can't run on GPU and *how* to fix it.

Multi-backend support:
  â€¢ CUDA (NVIDIA, 80% market share) - Priority 1
  â€¢ HIP (AMD) - Priority 2
  â€¢ Metal (Apple) - Priority 3
  â€¢ Vulkan Compute (cross-vendor) - Portable

Impact: Opens entirely new use cases (ML, scientific computing, graphics, crypto). 
Differentiates Pyrite as "embedded to GPU, same safety everywhere." Positions as 
Mojo competitor but with better teachability (blame tracking explains GPU 
restrictions). Future Release feature (after CPU-side language is rock-solid).

15.33 Performance Lockfile - Enforced "Fast Forever"
--------------------------------------------------------------------------------

Performance regression prevention through baseline tracking and CI enforcement:

    quarry perf --baseline              # Write Perf.lock
    quarry perf --check                 # Fail CI if regressed
    quarry perf --diff-asm function     # Show why it regressed

The lockfile stores:
  â€¢ Hot function timings and call counts
  â€¢ Allocation sites and sizes
  â€¢ SIMD width used (4, 8, 16)
  â€¢ Inlining decisions (which functions inlined where)
  â€¢ Optimization choices (loop unrolling, vectorization)

When regressions occur, pinpoints root cause:
  â€¢ "SIMD width changed: 8 â†’ 4 (alignment broke)"
  â€¢ "Inlining stopped: function grew beyond threshold"
  â€¢ "New allocation added at line 241"
  â€¢ Assembly/IR diff shows exact code generation changes

Impact: This is the HIGHEST-LEVERAGE addition. It transforms cost transparency 
from measurement to enforcement. Without lockfile: performance decays 2-3% per 
month ("death by 1000 cuts"). With lockfile: performance regressions impossible 
to merge. Turns "Pyrite is fast" into "Pyrite stays fast forever." 

This is how Google maintains Chrome performance, how LLVM prevents regressions. 
Pyrite makes it one command instead of custom infrastructure.

Beta Release flagship feature (after quarry perf is stable).

15.34 Enhanced Layout and Aliasing Introspection
--------------------------------------------------------------------------------

Deep visibility into memory layout and compiler optimization assumptions:

Commands:
  â€¢ quarry layout TypeName - Field offsets, padding, alignment
  â€¢ quarry layout TypeName --cache-analysis - Cache-line implications
  â€¢ quarry explain-aliasing - When compiler assumes noalias
  â€¢ quarry explain-aliasing function - Aliasing limits for specific function

Shows concrete performance implications:
  â€¢ "3 bytes padding (12.5% overhead) - reorder fields to eliminate"
  â€¢ "Spans 15,626 cache lines - consider tiling for random access"
  â€¢ "Aliasing assumption prevents vectorization - estimated 15% slower"
  â€¢ "Add @noalias for 12-18% speedup if inputs provably disjoint"

Impact: Completes the "intuitive memory model" story. Makes invisible concepts 
(cache lines, padding, aliasing) visible and actionable. Teaches performance 
optimization systematically: profile â†’ inspect layout â†’ understand constraints â†’ 
apply fixes â†’ measure improvement.

Pairs perfectly with existing quarry explain-type. Together they answer:
  â€¢ explain-type: WHAT is this type?
  â€¢ layout: HOW is it arranged in memory?
  â€¢ explain-aliasing: WHEN can compiler optimize?

Stable Release enhancement (extends existing type introspection).

15.35 Fuzzing and Sanitizers - Runtime Verification
--------------------------------------------------------------------------------

Industry-standard runtime verification catches bugs that static analysis misses:

Commands:
  â€¢ quarry fuzz - Coverage-guided fuzzing (finds edge cases)
  â€¢ quarry sanitize --asan - AddressSanitizer (memory errors)
  â€¢ quarry sanitize --tsan - ThreadSanitizer (data races)
  â€¢ quarry sanitize --ubsan - UndefinedBehaviorSanitizer (UB)
  â€¢ quarry sanitize --msan - MemorySanitizer (uninitialized memory)
  â€¢ quarry miri - Interpreter-based exhaustive UB detection (future)

Real-world impact:
  â€¢ Chromium: ASan found 3,000+ bugs code review missed
  â€¢ Rust: Miri caught soundness bugs in stdlib
  â€¢ Industry: Sanitizers non-negotiable for security-critical code

Example fuzzing output:
  â€¢ "Found integer overflow: u32::max + 1 at line 234"
  â€¢ "Saved crash input to fuzz/crashes/crash-1.bin"
  â€¢ "Generated 1,247 regression test cases"

Example sanitizer output:
  â€¢ "ThreadSanitizer: data race on counter (line 156)"
  â€¢ "Fix: Use AtomicU64 or Mutex<u64>"

Impact: This is TABLE-STAKES for "serious systems language." Every mature 
systems language has sanitizers (C++, Rust, Go). Without them, Pyrite would 
seem "less rigorous." With them, Pyrite signals: "We care about correctness 
beyond what the compiler can prove."

Zero runtime cost (test builds only), high confidence multiplier. Required for:
  â€¢ Security-critical software certification
  â€¢ Safety-critical software (aerospace, medical)
  â€¢ Open-source trust building ("we fuzz our stdlib")
  â€¢ Corporate adoption (compliance requirements)

Beta Release feature (fuzzing + ASan/TSan/UBSan integration).
Stable Release feature (Miri-equivalent interpreter).

15.36 Autotuning as Codegen Tool
--------------------------------------------------------------------------------

Machine-specific parameter optimization without runtime cost:

    quarry autotune                     # Run microbenchmarks
    # Outputs: src/generated/tuned_params.pyr
    
    const MATRIX_MULTIPLY_TILE_SIZE = 64   # Measured optimal for L1 cache
    const BLUR_SIMD_WIDTH = 8              # Best for this CPU
    const UNROLL_FACTOR = 4                # ILP sweet spot

Application code imports generated constants:

    import generated::tuned
    
    fn matrix_multiply(...):
        const TILE_SIZE = tuned::MATRIX_MULTIPLY_TILE_SIZE  # 64
        # Parameter closure uses tuned constant (still zero-cost)
        algorithm.tile[block_size=TILE_SIZE](M, N, fn[i: int, j: int]:
            ...
        )

Benefits vs runtime autotuning:
  âœ“ Zero runtime cost (constants compiled in)
  âœ“ Fully inspectable (generated file is human-readable)
  âœ“ Reproducible (checked into version control)
  âœ“ No hidden behavior (explicit import)
  âœ“ CI-friendly (deterministic builds)
  âœ“ Cross-compilation safe (tune per target)

Workflow:
  1. Developer writes code with reasonable defaults
  2. Profile shows hot path
  3. quarry autotune benchmarks parameter space
  4. Generated constants checked in
  5. Production builds use tuned values (10-50% faster)

Impact: Provides Mojo's autotuning benefits (machine-optimal parameters) without 
Mojo's pitfalls (runtime overhead, non-determinism, hidden behavior). This is 
"autotuning done right" - as a build tool, not language semantics. Addresses why 
Mojo deprecated their original autotuning system. Works seamlessly with parameter 
closures for verified zero-cost abstractions.

Typical improvements:
  â€¢ TILE_SIZE tuning: 15-30% speedup (cache-aware blocking)
  â€¢ SIMD_WIDTH tuning: 2-4x speedup (optimal vector width)
  â€¢ Combined tuning: 3-8x speedup for numeric kernels

Stable Release feature (after algorithmic helpers and benchmarking are mature).

Why These Features Matter
----------------------------------------------------

These additions address the specific pain points identified in developer surveys 
as friction points for systems languages:

  1. **Predictable behavior** â†’ Fewer "why did this happen?" moments
  2. **Interactive fix selection** â†’ Faster learning, less frustration
  3. **Progressive cost reporting** â†’ Performance intuition at your level
  4. **Type introspection** â†’ Memory model becomes tangible
  5. **Guaranteed cleanup** â†’ Confidence in resource management
  6. **Visual learning** â†’ Ownership clicks faster
  7. **C interop** â†’ Ecosystem unlocked immediately
  8. **Friendly names** â†’ Gentle onboarding
  9. **Compile-time params** â†’ High performance without complexity
  10. **Runtime profiling suite** â†’ Performance debugging becomes mechanical
  11. **Two-tier closures** â†’ Zero-cost abstractions are provably zero-cost 
  12. **Algorithmic helpers** â†’ SIMD/parallel for intermediate devs
  13. **Performance cookbook** â†’ Stdlib teaches "why it's fast" with examples 
  14. **Inline storage containers** â†’ Fast path is easy path
  15. **Interactive learning** â†’ Guided practice beats passive reading
  16. **With statement** â†’ Python familiarity, zero friction
  17. **Views by default** â†’ Eliminates "why did this move?" confusion
  18. **Zero-alloc mode** â†’ Embedded/safety-critical credibility
  19. **Performance budgets** â†’ Requirements become guarantees
  20. **Cache tiling** â†’ Numeric performance accessible
  21. **CPU multi-versioning** â†’ Fast everywhere, zero config
  22. **Structured async** â†’ No leaked tasks by default
  23. **Call-graph blame** â†’ Understand why contracts fail, fix at source
  24. **IDE hover metadata** â†’ Ownership/cost visible while coding
  25. **Explicit unrolling** â†’ Control for kernels, safety for sanity
  26. **Peak performance mode** â†’ One-command maximum optimization
  27. **Extended multi-versioning** â†’ Beyond SIMD to all CPU features
  28. **Safe accessors** â†’ Bounds-checked by default, optimized when provable
  29. **Learning Profile** â†’ One-command beginner setup, zero friction
  30. **Tensor type** â†’ Numerical computing foundation
  31. **Noalias semantics** â†’ Expert optimization for disjoint data
  32. **GPU computing** â†’ Heterogeneous computing with same safety guarantees
  33. **Performance lockfile** â†’ CI-enforced "fast forever" with regression root cause 
  34. **Enhanced layout introspection** â†’ Cache-line analysis, padding visualization
  35. **Fuzzing and sanitizers** â†’ Catch bugs static analysis misses, industry standard
  36. **Autotuning as codegen** â†’ Machine-optimal parameters, zero runtime cost 
  37. **Supply-chain security** â†’ quarry audit/vet/sign for trust and compliance 
  38. **Argument convention aliases** â†’ Optional borrow/inout/take keywords for teaching
  39. **Enhanced PGO workflow** â†’ Manual generate/optimize steps for full control
  40. **Python interop roadmap** â†’ Future Release strategic expansion for numerical computing
  41. **Interactive REPL** â†’ Instant experimentation with ownership visualization  CRITICAL
  42. **Binary size profiling** â†’ quarry bloat for embedded code size optimization  EMBEDDED
  43. **Deterministic builds** â†’ Bit-for-bit reproducible, verifiable binaries  TRUST
  44. **Incremental compilation** â†’ 15-27x faster rebuilds for productivity  ESSENTIAL
  45. **Internationalized errors** â†’ Native language diagnostics for global adoption  GLOBAL
  46. **Built-in observability** â†’ Logs, traces, metrics for production systems  PRODUCTION
  47. **Energy profiling** â†’ Battery and sustainability optimization  UNIQUE
  48. **Hot reloading** â†’ Live updates for rapid iteration  DEVELOPER JOY
  49. **Community dashboard** â†’ Transparent metrics make success measurable  EVIDENCE
  50. **Formal semantics** â†’ Mathematical specification for certification  VERIFICATION
  51. **Dead code analysis** â†’ Find and remove unused code for size optimization  MAINTAINABILITY
  52. **License compliance** â†’ Automated legal compatibility checking  ENTERPRISE
  53. **Feature flags/cfg** â†’ Structured conditional compilation  CROSS-PLATFORM

**Summary: Complete Excellence Across All Dimensions**

These 54 features represent a comprehensive approach to achieving widespread developer 
adoption:

**Beta Release (Critical for Self-Hosting):**
  â€¢ Result/Option types: Error handling-BLOCKING
  â€¢ Traits and generics: Compiler abstractions-BLOCKING
  â€¢ Full FFI: LLVM bindings-BLOCKING
  â€¢ HashMap: Symbol tables-BLOCKING
  â€¢ File I/O (enhanced): Multi-file compilation-BLOCKING
  â€¢ Incremental compilation (#44): Fast rebuilds for 10K+ line compiler-CRITICAL
  â€¢ REPL (#41): Development tool-HELPFUL
  â€¢ Deterministic builds (#43): Reproducibility-important for testing
  â€¢ Test framework: 100% coverage - REQUIRED

**Stable Release (Advanced Features):**
  â€¢ Energy profiling (#47): UNIQUE-no competitor has this
  â€¢ Community dashboard (#49): Evidence-based advocacy-makes success measurable
  â€¢ Formal semantics (#50): Certification-highest safety levels
  â€¢ Design by Contract: Logical correctness-beyond memory safety
  â€¢ Observability (#46): Production readiness-servers and cloud
  â€¢ Internationalized errors (#45): Global accessibility-60% of developers
  â€¢ SIMD and algorithmic helpers: High-performance computing

**Future Release (Specialized Domains):**
  â€¢ GPU computing: Heterogeneous computing
  â€¢ Python interop: ML/numerical computing bridge
  â€¢ Hot reloading (#48): Developer joy-rapid iteration
  â€¢ Advanced performance tuning: Production optimization

The result: **No friction points, no missing features, no "figure it out 
yourself."** Every dimension of developer experience is addressed with 
first-class, integrated solutions.

This is what achieving widespread developer adoption requires: not just technical excellence,
but **comprehensive excellence** - safety, performance, learning, productivity, 
transparency, security, production, global reach, quality, and joy. Pyrite 
delivers on all of them.

15.37 Supply-Chain Security and Trust
--------------------------------------------------------------------------------

Comprehensive supply-chain security as first-class features:

Commands:
  â€¢ quarry audit - Vulnerability scanning against CVE database
  â€¢ quarry vet - Dependency review workflow with organization trust sharing
  â€¢ quarry sign/verify - Cryptographic package signing
  â€¢ quarry sbom - Software Bill of Materials generation (SPDX, CycloneDX)

Example workflows:

    # Continuous vulnerability monitoring
    $ quarry audit
    Found 2 CRITICAL vulnerabilities in dependencies
    Run 'quarry audit --fix' to update to patched versions
    
    # Organization dependency review
    $ quarry vet
    4 unvetted dependencies require review
    Import audits: quarry vet import-audits --from=mozilla
    
    # Package signature verification
    $ quarry verify --all
    âœ“ All dependencies cryptographically verified
    âœ“ No tampering detected

Benefits:
  â€¢ **Reduces supply-chain risk:** Every dependency explicitly reviewed
  â€¢ **Industry compliance:** SBOM for government contracts, healthcare, finance
  â€¢ **Trust multiplier:** Makes Pyrite "enterprise-ready from day one"
  â€¢ **Required for certification:** Aerospace (DO-178C), medical (IEC 62304)
  â€¢ **Zero language complexity:** Implementation time only, no semantic changes

Impact: This is a **love multiplier** that costs implementation effort but makes 
Pyrite feel "serious" and "production-ready" to organizations evaluating systems 
languages for critical infrastructure. For embedded-first strategy, supply-chain 
security is **table stakes** in aerospace, medical, and industrial domains.

Without this: "Interesting language but lacks enterprise features"
With this: "Complete, production-ready platform with security baked in"

Real-world validation:
  â€¢ Rust: cargo-vet adoption growing in Mozilla, Google, security-focused orgs
  â€¢ npm: Added signature verification after multiple supply-chain attacks
  â€¢ Go: Added SBOM support in response to Executive Order 14028
  â€¢ Pyrite: Ships with all of this as first-class features, not afterthoughts

Stable Release implementation (after core package registry is stable).

15.38 Argument Convention Aliases
--------------------------------------------------------------------------------

Optional teaching keywords that desugar to standard reference syntax:

    # Standard syntax (always works)
    fn process(data: &Config):       # Immutable borrow
    fn update(data: &mut Config):    # Mutable borrow
    fn consume(data: Config):        # Takes ownership
    
    # Optional teaching aliases (desugar to standard)
    fn process(borrow data: Config):  # â†’ data: &Config
    fn update(inout data: Config):    # â†’ data: &mut Config
    fn consume(take data: Config):    # Semantic marker (no desugaring)

Keywords:
  â€¢ borrow â†’ &T (read-only access)
  â€¢ inout â†’ &mut T (mutable access)
  â€¢ take â†’ semantic marker for ownership transfer (self-documenting)

Benefits:
  â€¢ Zero runtime cost (pure syntax sugar, desugars during parsing)
  â€¢ Optional (never required, &T works everywhere)
  â€¢ Teaching-focused (intent explicit for Python/JS developers)
  â€¢ Self-limiting (learners see &T in errors, transition naturally)

Use cases:
  â€¢ Teaching materials for absolute beginners
  â€¢ Introductory tutorials and workshops
  â€¢ Educational institutions with Python-first students
  â€¢ Code review where intent needs extra clarity

Configuration:

    # Allow in learning mode
    [learning]
    allow-argument-aliases = true
    
    # Standardize in production
    quarry fmt --normalize-syntax  # Convert to &T

Impact: Low-cost, high-teaching-value syntax sugar inspired by Mojo's argument 
conventions. Makes intent crystal clear without fragmenting ecosystem (stdlib 
standardizes on &T). Optional adoption-educators can choose based on student 
background.

Pyrite's approach: Provide the tool, let educators decide when to use it.

Beta Release implementation (parser-level sugar, trivial to add).

15.39 Enhanced PGO Workflow
--------------------------------------------------------------------------------

Manual profile-guided optimization with full control:

    # Step 1: Generate instrumented binary
    $ quarry pgo generate
    Built: target/pgo-instrument/myapp
    
    # Step 2: Run training workload(s)
    $ ./target/pgo-instrument/myapp --benchmark
    $ ./target/pgo-instrument/myapp --workload=typical
    Profile data: target/pgo-data/*.profraw
    
    # Step 3: Optimize with collected profiles
    $ quarry pgo optimize
    Built: target/release/myapp (15-30% faster)

Supports multiple training runs:
  â€¢ Collect profiles from different workloads
  â€¢ Merge all profiles for comprehensive optimization
  â€¢ Weight different scenarios (50% web, 30% batch, 20% interactive)

Advantages over one-command quarry pgo:
  â€¢ Full control over training data
  â€¢ Multiple workload profiles (web + batch + interactive)
  â€¢ Custom training scripts (complex setup/teardown)
  â€¢ Reproducible training (version-controlled training scripts)

Integration with automation:

    # CI/CD pipeline
    quarry pgo generate
    ./run_comprehensive_training_suite.sh
    quarry pgo optimize
    quarry bench --compare-to=baseline

Impact: Complements one-command quarry pgo with power-user workflow. Similar to 
cargo-pgo ergonomics (generate/optimize split). Makes PGO accessible (automated 
workflow) and flexible (manual control when needed).

Typical improvement: 10-30% performance gain for real-world workloads.

Beta Release implementation (extends existing PGO support).

15.40 Interactive REPL with Ownership Visualization
--------------------------------------------------------------------------------

Interactive Read-Eval-Print Loop that makes ownership tangible and enables 
instant experimentation:

    pyrite repl --explain
    
    >>> let data = List[int]([1, 2, 3])
    â”Œâ”€â”€â”€â”€â”€â”
    â”‚data â”‚ â† OWNER (owns heap allocation)
    â””â”€â”€â”€â”€â”€â”˜
    
    >>> process(data)
    data â”€â”€[MOVED]â”€â”€> process()
    
    >>> data.length()
    error: Cannot use moved value

Features:
  â€¢ Real-time ownership state visualization
  â€¢ :cost command shows session allocations
  â€¢ :type command runs quarry explain-type inline
  â€¢ :ownership command shows borrow graph
  â€¢ Multi-line editing, session save/load
  â€¢ Import support for stdlib and packages

Impact: **CRITICAL MISSING FEATURE** - Python developers expect REPL as core 
feature. Without it, "Pythonic systems language" claim is incomplete. REPL makes 
ownership learning interactive ("try it, see what happens") rather than abstract 
("read about it, compile, see error"). This is 50% of Python's appeal - instant 
gratification.

Beta Release implementation (high priority).

15.41 Binary Size Profiling
--------------------------------------------------------------------------------

Embedded-critical tooling for flash memory optimization:

    quarry bloat
    
    Total: 47 KB (73% of 64 KB budget)
    
    Largest contributors:
      1. std::fmt (12 KB, 25%) â†’ Use core::fmt_minimal (2 KB)
      2. Panic handler (8 KB, 17%) â†’ Use --panic=abort (-6 KB)
      3. Unused generics (5 KB, 11%) â†’ Run --gc-sections

Features:
  â€¢ Per-function and per-section size breakdown
  â€¢ Dependency size attribution
  â€¢ Optimization suggestions for size reduction
  â€¢ Size budget enforcement in CI
  â€¢ Comparison across versions

Impact: **ESSENTIAL FOR EMBEDDED-FIRST STRATEGY** - embedded developers evaluate 
binary size first thing. "How big is Hello World?" is the first question. 
Without quarry bloat, Pyrite's embedded positioning lacks credibility. With it, 
demonstrates understanding of embedded constraints at tooling level.

Must match Rust's cargo bloat for competitive parity.

Beta Release implementation (critical for embedded credibility).

15.42 Deterministic and Reproducible Builds
--------------------------------------------------------------------------------

Bit-for-bit identical binaries from identical sources:

    quarry build --deterministic
    
    Binary hash: 7d5e9c8b3a4f2d1e...
    
    quarry verify-build --hash=7d5e9c8b3a4f2d1e...
    âœ“ Binary reproducible from declared sources

Features:
  â€¢ Fixed timestamps, stable symbol tables
  â€¢ Content-addressable build artifacts
  â€¢ BuildManifest with source hashes
  â€¢ Verification workflow
  â€¢ CI enforcement

Impact: **COMPLETES SUPPLY-CHAIN SECURITY** - quarry audit/vet/sign (Section 8.17) 
is incomplete without reproducibility. Can't verify "this binary came from this 
source" without deterministic builds. Required for: Debian packaging, security 
audits, government contracts. SolarWinds attack and XZ backdoor demonstrate why 
this is non-negotiable.

Table-stakes for security-critical deployments.

Beta Release implementation (critical for supply-chain security).

15.43 Incremental Compilation
--------------------------------------------------------------------------------

Fast rebuilds through module-level caching:

    Full build: 28s
    Incremental: 1.8s (15x faster)

Features:
  â€¢ Module dependency tracking
  â€¢ Interface fingerprinting (recompile dependents only when API changes)
  â€¢ Intelligent cache invalidation
  â€¢ 15-27x speedup for typical projects

Impact: **ESSENTIAL FOR DEVELOPER EXPERIENCE** - slow rebuilds significantly impact 
productivity. Google data shows 30% productivity increase with faster builds (citation 
needed). Rust's incremental compilation strongly correlates with satisfaction. Without 
this, even fast compiler feels slow on large projects. 

"Instant feedback" requires <5s rebuilds - incremental compilation delivers this.

Beta Release implementation (critical for productivity).

15.44 Internationalized Error Messages
--------------------------------------------------------------------------------

Native language compiler diagnostics for global adoption:

    pyritec --language=zh            # Chinese errors
    pyritec --language=es            # Spanish errors
    pyritec --language=hi            # Hindi errors

Features:
  â€¢ Professional native-speaker translations
  â€¢ 10+ languages (Chinese, Spanish, Hindi, Japanese, etc.)
  â€¢ Community translation workflow
  â€¢ IDE integration (auto-detect system language)

Impact: **REQUIRED FOR GLOBAL ADOPTION** - 60% of programmers are non-native English 
speakers. Language barriers prevent understanding ownership concepts (hard even in 
native language). Educational institutions in China, India, Latin America need local 
language support. Research suggests faster concept mastery in first language (specific 
studies show approximately 2x improvement).

Few compilers provide comprehensive multilingual error support - distinctive 
differentiator.

Stable Release implementation (high impact, moderate complexity).

15.45 Built-In Observability
--------------------------------------------------------------------------------

Production observability with zero-cost elimination:

    import std::log, std::trace, std::metrics
    
    with span = trace::span("process_request"):
        log::info("request", {"user_id": id})
        metrics::counter("requests").increment()

Features:
  â€¢ Structured logging (typed fields, JSON output)
  â€¢ Distributed tracing (OpenTelemetry-compatible)
  â€¢ Metrics collection (counters, gauges, histograms)
  â€¢ Zero cost when disabled (compile-time feature flags)

Impact: **PRODUCTION READINESS FOR SERVERS** - without observability, Pyrite is 
"embedded + CLI only." With it, credible for web services, microservices, cloud 
applications. Go's excellent observability is significant adoption driver. 
Built-in (not third-party) ensures consistency and quality.

Required for server/cloud use cases.

Stable Release implementation (required for production deployments).

15.46 Energy Profiling
--------------------------------------------------------------------------------

Power consumption visibility and battery-life optimization:

    quarry energy
    
    Total: 45.2 joules over 30s
    Average: 1.51 watts
    
    Hot spots:
      1. matrix_multiply: 18.2 J (AVX-512 high power)
         â†’ Battery mode: Use AVX2 (saves 40% energy)

Features:
  â€¢ Platform-specific power measurement (RAPL, powermetrics, ETW)
  â€¢ Battery-life estimation
  â€¢ Energy budget enforcement
  â€¢ Optimization for low-power mode

Impact: **UNIQUE DIFFERENTIATOR** - NO systems language has built-in energy 
profiling. Growing importance for: sustainability (green software movement), 
mobile (battery life), IoT (coin-cell batteries), data centers (cooling costs). 

Marketing: "The energy-aware systems language."

Forward-thinking differentiation with practical value.

Stable Release implementation (unique positioning).

15.47 Hot Reloading for Development
--------------------------------------------------------------------------------

Live code updates without restart:

    quarry dev
    
    [10:30:15] File changed: src/renderer.pyrite
    [10:30:16] âœ“ Hot reloaded in 847ms
    [10:30:16] Application state preserved

Features:
  â€¢ Function body updates without restart
  â€¢ State preservation across reloads
  â€¢ Safety: Only compatible changes allowed
  â€¢ Accelerates game dev, web dev, data processing

Impact: **DEVELOPER JOY MULTIPLIER** - saves 5-30 seconds per iteration. 100 
iterations = 8-50 minutes saved. Game developers tweak gameplay without losing 
session. Web developers see changes instantly. Competitive parity with modern 
languages (Rust-analyzer, Erlang hot code swapping, JavaScript HMR).

Productivity enhancement for certain workflows.

Stable Release implementation (developer experience).

15.48 Community Transparency Dashboard
--------------------------------------------------------------------------------

Public metrics that make developer adoption measurable:

    (aspirational: quarry.dev/metrics)
    
    Performance: 98.3% of C speed (1,247 benchmarks)
    Safety: 0 memory CVEs (C: 892, C++: 654, Rust: 0)
    Learning: 82% complete ownership (Rust: 64%)
    Compile time: 1.2s average (Rust: 8.4s)

Features:
  â€¢ Real-time performance, safety, learning metrics
  â€¢ User-submitted benchmarks
  â€¢ CVE tracking vs competitors
  â€¢ Ecosystem health (package count, maintainers)
  â€¢ Public API for embedding

Impact: **MAKES SUCCESS MEASURABLE** - transforms developer adoption goals from aspiration to

evidence. "Just look at (aspirational: quarry.dev/metrics)" becomes standard advocacy response. 
Objective data beats subjective claims. Competitive positioning with proof. 
Trust multiplier through transparency.

No competitor has comprehensive public dashboard - unique transparency.

Stable Release implementation (advocacy multiplier).

15.49 Design by Contract
--------------------------------------------------------------------------------

Logical correctness verification through contracts:

    @requires(n >= 0, "n non-negative")
    @ensures(result > 0, "factorial positive")
    fn factorial(n: int) -> int:
        # Contract checked at compile-time when provable
        # Runtime check in debug builds, zero cost in release

Features:
  â€¢ Preconditions, postconditions, invariants
  â€¢ Compile-time verification when provable
  â€¢ Call-graph contract propagation
  â€¢ Integration with SMT solvers (Z3)
  â€¢ Composable with @cost_budget

Impact: **CORRECTNESS + CERTIFICATION** - ownership prevents memory bugs, contracts 
prevent logic bugs. Required for DO-178C Level A, IEC 62304 medical devices. 
Ada/SPARK prove formal methods enable certification. Pyrite makes contracts 
accessible (not just academic).

Bridges gap: "memory-safe but logically incorrect" is still wrong.

Stable Release implementation (certification requirement).

15.50 Formal Semantics Specification
--------------------------------------------------------------------------------

Mathematical specification of language behavior:

**Memory Safety Theorem:**
  âˆ€ program p, well-typed(p) âˆ§ no-unsafe(p) âŸ¹ memory-safe(p)

**Data-Race-Freedom Theorem:**
  âˆ€ program p, well-typed(p) âŸ¹ Â¬data-race(p)

Features:
  â€¢ Operational semantics (execution rules)
  â€¢ Axiomatic semantics (verification)
  â€¢ Memory model with happens-before
  â€¢ Undefined behavior catalog
  â€¢ Mechanization in Coq/Isabelle

Impact: **REQUIRED FOR HIGHEST CERTIFICATION** - DO-178C Level A, Common Criteria 
EAL 7 require formal specification. Academic research needs rigorous foundation. 
Verification tools need precise semantics. Enables: formal proofs, certified 
compilation (CompCert-style), academic adoption.

Differentiates Pyrite as "verifiable, not just safe."

Stable Release implementation (certification, research).

15.51 Dead Code Analysis and Elimination
--------------------------------------------------------------------------------

Comprehensive unused code detection for binary size optimization:

    quarry deadcode
    
    Found 23 unused items (3,456 bytes in binary)
    
    Unused functions (18):
      â€¢ old_algorithm() [234 bytes]
      â€¢ parse_legacy_format() [567 bytes]
    
    Total savings: 3.456 KB (7.3% of binary)

Features:
  â€¢ Unused function detection
  â€¢ Unused type detection
  â€¢ Unused generic instantiation detection
  â€¢ Automatic removal (quarry deadcode --remove)
  â€¢ Link-time elimination (--gc-sections)
  â€¢ CI enforcement (--threshold flag)

Impact: **BINARY SIZE + CODE QUALITY** - embedded systems benefit from size 
reduction (every byte matters). Code quality improves (dead code is technical 
debt). Maintenance simplified (clear signal what's safe to remove). Integration 
with quarry bloat completes size optimization story.

Beta Release implementation (valuable for embedded + maintainability).

15.52 Dependency License Compliance
--------------------------------------------------------------------------------

Automated legal compatibility checking for enterprise adoption:

    quarry license-check
    
    âœ“ Compatible: 44 packages
    âš ï¸  Requires Review: 2 packages (LGPL-2.1)
    âœ— INCOMPATIBLE: 1 package (GPL-3.0 conflicts with MIT)

Features:
  â€¢ License compatibility verification
  â€¢ Configurable allowed/denied licenses
  â€¢ CI enforcement
  â€¢ SBOM integration (include license info)
  â€¢ Compliance report generation

Impact: **ENTERPRISE ADOPTION ENABLER** - legal departments require license audits. 
GPL contamination prevents commercial use. Built-in checking (not third-party) 
reduces friction. Makes Pyrite "enterprise-ready" for regulated industries.

Trust signal: Pyrite understands legal requirements, not just technical.

Stable Release implementation (extends SBOM work).

15.53 Configuration Attributes and Feature Flags
--------------------------------------------------------------------------------

Structured conditional compilation without textual preprocessing:

    @cfg(target_os = "windows")
    fn platform_specific():
        # Windows implementation
    
    @cfg(feature = "gpu")
    import std::gpu
    
    # Quarry.toml
    [features]
    default = ["json", "http"]
    gpu = ["cuda"]
    minimal = []

Features:
  â€¢ Platform conditionals (OS, architecture, pointer width)
  â€¢ Feature flags for optional dependencies
  â€¢ Build configuration (debug_assertions, release)
  â€¢ Type-safe conditions (no preprocessor pitfalls)
  â€¢ IDE-friendly (grayed-out disabled code)

Impact: **ESSENTIAL FOR CROSS-PLATFORM** - platform-specific code clearly marked. 
Optional features reduce binary size (embedded: disable everything unnecessary). 
Type-checked (not textual substitution) prevents preprocessor bugs. Already 
mentioned in specification but fully detailed now.

Alpha Release implementation (essential for portability).

15.54 Python Interop Roadmap
--------------------------------------------------------------------------------

Strategic expansion for numerical/data science markets (Future Release):

Capabilities:
  â€¢ Call Python from Pyrite (with explicit GIL boundaries)
  â€¢ Generate Python extension modules (quarry pyext)
  â€¢ Type-safe conversion at boundaries
  â€¢ Cost transparency (Python calls marked expensive)

Example:

    import std::python as py
    
    fn process_with_numpy(data: &[f64]) -> Result[Vec[f64], Error]:
        with gil = py::GIL::acquire():
            let np = try gil.import("numpy")
            let result = try np.call("fft", [gil.from_slice(data)])
            return Ok(try gil.to_vec[f64](result))

Why Future Release (not Beta/Stable):
  â€¢ **Wrong audience initially:** Embedded/systems devs don't need Python
  â€¢ **Significant complexity:** GIL, reference counting, type bridging
  â€¢ **Conflicts with embedded-first:** Can't run Python on microcontrollers
  â€¢ **Mojo owns this space:** Competing dilutes differentiation
  â€¢ **Not required for self-hosting:** Compiler doesn't need Python interop

When to add:
  â€¢ After establishing Pyrite in embedded + servers (Beta + Stable)
  â€¢ When entering numerical computing domain (Stable Release expansion)
  â€¢ As adoption wedge for data science (migrate Python bottlenecks incrementally)

Strategic value:
  â€¢ Access to Python ecosystem until Pyrite equivalents mature
  â€¢ Gradual migration path for existing Python projects
  â€¢ "Pyrite for performance, Python for ecosystem" hybrid approach

Must maintain Pyrite principles:
  â€¢ Explicit (GIL acquisition visible, not hidden)
  â€¢ Optional (Python runtime not required for core Pyrite)
  â€¢ Isolated (clear boundaries, cost transparency)
  â€¢ Type-safe (explicit conversions, ownership rules enforced)

Impact: Medium priority. Valuable for market expansion (numerical computing, 
data science) but not critical for core identity (embedded/systems). Addresses 
ecosystem gap after Pyrite's native numerical libs mature.

Future Release feature (intentionally delayed to avoid early complexity).

Each feature delivers maximum impact relative to implementation cost-the 
definition of high ROI. Together they form a comprehensive developer experience package:

technical excellence + exceptional developer experience + accessible performance 
optimization + structured learning path + Python familiarity + embedded/safety--
critical viability + composable performance contracts + real-time cost visibility + 
GPU acceleration with teachable constraints + enforced performance stability + 
runtime verification for reliability + supply-chain trust + flexible PGO workflows + 
strategic ecosystem bridges.

16. FORMAL SEMANTICS AND VERIFICATION (Stable Release)
================================================================================

For safety-critical applications requiring formal verification and certification, 
Pyrite provides a mathematically rigorous specification of its semantics. This 
enables external verification tools, academic research, and certification 
processes that require formal methods.

16.1 Formal Memory Model
--------------------------------------------------------------------------------

The memory model defines when memory operations are valid and what constitutes 
undefined behavior. This is specified using operational semantics and happens--
before relationships.

Ownership Rules (Formal)
~~~~~~~~~~~~~~~~~~~~~~~~~

**Axiom 1: Unique Ownership**
  âˆ€ value v, âˆƒ exactly one owner o at any point in program execution
  
  Ownership transfer (move):
    owner(v, tâ‚€) = oâ‚ âˆ§ move(v, oâ‚ â†’ oâ‚‚, tâ‚) âŸ¹ owner(v, tâ‚) = oâ‚‚ âˆ§ invalid(oâ‚)

**Axiom 2: Exclusive Mutable Access**
  âˆ€ value v, âˆƒ at most one &mut reference at time t
  âˆ€ value v, (&mut v exists at t) âŸ¹ (no &v exists at t)

**Axiom 3: Lifetime Containment**
  âˆ€ reference r to value v, lifetime(r) âŠ† lifetime(v)
  
  Cannot create dangling reference:
    lifetime(r) âŠˆ lifetime(v) âŸ¹ compile-time error

Happens-Before Relationships
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Sequencing rules that define program order:

1. **Sequential consistency within thread:**
   Statement sâ‚ happens-before sâ‚‚ if sâ‚ textually precedes sâ‚‚ in same block

2. **Synchronization edges:**
   - Write to Mutex âŸ¹ happens-before subsequent Lock
   - Send to Channel âŸ¹ happens-before Receive
   - Thread::spawn âŸ¹ happens-before first statement in thread
   - Thread::join âŸ¹ all thread statements happen-before join return

3. **Memory ordering for atomics:**
   - Acquire-Release: Write with Release âŸ¹ happens-before Read with Acquire
   - Sequentially Consistent: Total global order

Data Race Definition
~~~~~~~~~~~~~~~~~~~~

**Data Race (Undefined Behavior):**
  Two accesses aâ‚, aâ‚‚ to location â„“ where:
    â€¢ At least one is write
    â€¢ Â¬(aâ‚ happens-before aâ‚‚) âˆ§ Â¬(aâ‚‚ happens-before aâ‚)
    â€¢ No synchronization between aâ‚ and aâ‚‚

**Theorem: Safe Pyrite is Data-Race-Free**
  âˆ€ program p, well-typed(p) âˆ§ no-unsafe(p) âŸ¹ Â¬data-race(p)

Proof sketch:
  â€¢ Ownership rules prevent unsynchronized mutable aliasing
  â€¢ Borrowing rules enforce exclusive-or-shared access
  â€¢ Send/Sync traits prevent unsafe sharing across threads
  â€¢ Type system rejects programs that could have data races

Memory Safety Theorem
~~~~~~~~~~~~~~~~~~~~~~

**Theorem: Well-Typed Programs Are Memory-Safe**

For all programs p:
  well-typed(p) âˆ§ no-unsafe(p) âŸ¹
    â€¢ Â¬use-after-free(p)
    â€¢ Â¬double-free(p)
    â€¢ Â¬null-dereference(p)
    â€¢ Â¬buffer-overflow(p) [with bounds-checking]
    â€¢ Â¬data-race(p)

Proof approach:
  â€¢ Operational semantics define valid execution steps
  â€¢ Type soundness: well-typed expressions don't get stuck
  â€¢ Progress: well-typed program either steps or is a value
  â€¢ Preservation: stepping preserves types
  â€¢ Ownership invariant: each value has exactly one owner
  â€¢ Borrowing invariant: exclusive mutable or shared immutable access

Operational Semantics (Selected Rules)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Variable binding:

    âŸ¨let x = e, Ïƒ, Î·âŸ© â‡’ âŸ¨skip, Ïƒ[x â†¦ v], Î·âŸ©
    
    where:
      â€¢ Ïƒ is the ownership store (who owns what)
      â€¢ Î· is the value store (memory)
      â€¢ v is the result of evaluating e

Move semantics:

    owner(v, Ïƒ) = x âˆ§ assign(y, x, Ïƒ) âŸ¹
      Ïƒ' = Ïƒ[y â†¦ v, x â†¦ âŠ¥]
    
    where:
      â€¢ âŠ¥ represents "moved/invalid"
      â€¢ Subsequent use of x is compile-time error

Borrow creation:

    owner(v, Ïƒ) = x âˆ§ create-borrow(&v, 'a, Shared) âŸ¹
      Ïƒ' = Ïƒ[r â†¦ &'a v]
      âˆ§ borrows(x, Ïƒ') = borrows(x, Ïƒ) âˆª {r}
      âˆ§ no &mut borrows of x exist

Unsafe Semantics
~~~~~~~~~~~~~~~~~

Undefined behavior is formally specified:

**Undefined Behavior Catalog:**
  1. Dereferencing null, dangling, or unaligned pointer
  2. Data race on non-atomic memory
  3. Reading uninitialized memory
  4. Violating pointer aliasing assumptions (@noalias)
  5. Integer overflow in non-wrapping context
  6. Calling function with mismatched ABI
  7. Violating safety contracts in unsafe code

Programs exhibiting UB have **no defined semantics** - anything can happen.

Safe Pyrite guarantees: âˆ€ safe program p, Â¬undefined-behavior(p)

Verification Integration
~~~~~~~~~~~~~~~~~~~~~~~~

The formal semantics enables external verification tools:

**Static analyzers:**
  â€¢ Prove absence of memory errors
  â€¢ Verify contracts hold (@requires, @ensures)
  â€¢ Check performance contracts (@cost_budget)

**Model checkers:**
  â€¢ Exhaustively verify small, critical functions
  â€¢ Prove invariants across all possible inputs
  â€¢ Integration with TLA+, Spin, CBMC

**SMT solvers:**
  â€¢ Prove preconditions imply postconditions
  â€¢ Verify loop invariants automatically
  â€¢ Integration with Z3, CVC5

Example verification workflow:

    quarry verify --tool=z3 function_name
    
    # Attempts to prove:
    # - All preconditions are satisfied at call sites
    # - Postconditions hold for all execution paths
    # - Invariants maintained throughout execution

Axiomatic Semantics
~~~~~~~~~~~~~~~~~~~

For verification, each operation has axiomatic specification:

    # Specification for List::push
    @requires(self is valid)
    @ensures(self.len() == old(self.len()) + 1)
    @ensures(self[self.len() - 1] == item)
    @ensures(âˆ€i < old(self.len()): self[i] == old(self[i]))
    fn push(&mut self, item: T)

Verification tools can reason about list operations using these axioms without 
analyzing implementation.

16.2 Certification Support
--------------------------------------------------------------------------------

The formal semantics document enables Pyrite's use in safety-critical systems 
requiring certification:

DO-178C (Aerospace Software)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Level A requirements:
  â€¢ Formal specification of language semantics âœ“
  â€¢ Compiler qualification (prove compiler correctness)
  â€¢ Tool qualification for quarry build, quarry test
  â€¢ Verification of generated code

Pyrite support:
  â€¢ Formal semantics document (this section)
  â€¢ Subset of language for Level A (Core Pyrite)
  â€¢ Qualified compiler configuration
  â€¢ Traceability from source to binary (quarry verify-build)

IEC 62304 (Medical Device Software)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Requirements:
  â€¢ Language with defined semantics âœ“
  â€¢ Memory safety verification âœ“
  â€¢ Risk analysis for language features
  â€¢ Tool qualification

Pyrite support:
  â€¢ Formal proof of memory safety (for safe code)
  â€¢ Design by Contract for logical correctness
  â€¢ Hazard analysis: unsafe code must be justified
  â€¢ quarry vet for dependency security

ISO 26262 (Automotive Functional Safety)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ASIL D requirements:
  â€¢ Formal specification âœ“
  â€¢ Freedom from interference (memory safety) âœ“
  â€¢ Deterministic behavior âœ“
  â€¢ Verified compiler

Pyrite support:
  â€¢ Memory safety by construction (ownership)
  â€¢ Deterministic builds (reproducible)
  â€¢ No undefined behavior in safe code
  â€¢ Formal semantics for verification tools

16.3 Academic and Research Applications
--------------------------------------------------------------------------------

The formal semantics enables academic research:

Research Opportunities
~~~~~~~~~~~~~~~~~~~~~~

  â€¢ Prove new optimization passes preserve semantics
  â€¢ Verify third-party libraries meet contracts
  â€¢ Extend formal model with new features
  â€¢ Compare memory models (Pyrite vs Rust vs C++)
  â€¢ Automated theorem proving for Pyrite programs

Example Publications:
  â€¢ "Formal Verification of Pyrite's Ownership System"
  â€¢ "Mechanized Proof of Memory Safety for Pyrite Core"
  â€¢ "Certified Compilation from Pyrite to Machine Code"

Mechanization in Proof Assistants
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Formalization targets:
  â€¢ **Coq:** Mechanized proof of type soundness
  â€¢ **Isabelle/HOL:** Verification of compiler passes
  â€¢ **Lean:** Proof-carrying code for Pyrite
  â€¢ **F*:** Verification-oriented Pyrite subset

Example (Coq pseudocode):

    Theorem pyrite_memory_safety:
      forall (p : Program),
        well_typed p -->
        no_unsafe p -->
        forall (t : Trace), exec p t -> memory_safe t.

Why Formal Semantics Matter
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Safety certification:**
  â€¢ DO-178C Level A: Formal methods required
  â€¢ Common Criteria EAL 7: Formal verification required
  â€¢ IEC 62304 Class C: Formal specification beneficial
  â€¢ Without formal semantics: Cannot achieve highest certification levels

**Academic credibility:**
  â€¢ Research community needs formal foundation
  â€¢ Publications require mathematical specification
  â€¢ Tool development needs precise language definition
  â€¢ Universities teach from formal specs

**Compiler correctness:**
  â€¢ Test compiler against formal specification
  â€¢ Prove optimization passes preserve semantics
  â€¢ CompCert-style verified compilation future path

**Competitive differentiation:**
  â€¢ Rust: Informal but well-documented
  â€¢ C/C++: ISO standard (complex, has UB everywhere)
  â€¢ Zig: Informal specification
  â€¢ **Pyrite: Formal, verifiable, certification-ready**

Implementation: Stable Release (after language is stable)
Priority: Medium-High (required for highest certification levels)
Complexity: Very High (requires formal methods expertise)
Impact: High (enables verification, certification, academic adoption)

================================================================================
17. CONCLUSION
================================================================================

Pyrite is the amalgamation of the best ideas from decades of programming language 
development. From Python, it inherits approachability and readability - the 
philosophy that code should be intuitive and clear. A beginner can read Pyrite 
code and roughly understand what it does, thanks to its clean syntax and 
straightforward semantics, and experienced developers appreciate the lack of 
boilerplate and clutter (no extraneous semicolons, braces, or verbose type 
annotations for every variable). 

From Rust, it adopts rigorous compile-time safety (the ownership and borrowing 
model) and modern abstractions like traits, pattern matching, and generics, 
enabling "fearless concurrency" and memory safety without a garbage collector. 
This means you can write low-level code in Pyrite with confidence: the compiler 
is your ally, catching memory errors, data races, and even logical exhaustiveness 
errors at compile time. 

And from Zig, Pyrite embraces simplicity in implementation and explicit control 
over memory. There are no hidden allocations or hidden control flow transfers in 
Pyrite - if something allocates or jumps, you wrote it that way (or it's clearly 
documented). This transparency ensures that performance characteristics are easy 
to reason about; you won't be surprised by a sudden slow-down due to an implicit 
heap allocation or an unexpected exception throw, because those things don't 
happen implicitly in Pyrite's world.

The result is a language that feels high-level when you're writing it, but 
produces low-level efficient executables. You can often write code that looks 
like pseudocode or Python in its clarity, and yet the compiler turns it into tight 
machine code competitive with a hand-written C program. Pyrite encourages good 
coding practices naturally: immutability by default leads to fewer bugs; error 
handling is enforced by the type system so you don't forget it; resources are 
managed automatically in a RAII style so leaks are avoided. All of this happens 
in a way that doesn't burden the programmer with runtime costs or overly complex 
syntax.

In short, Pyrite aims to be a language that developers love using - one that is 
fun and productive for everything from microcontroller firmware to high-performance
servers. By meeting the needs of low-level systems programming without the usual 
pain (no more mysterious segfaults, no more endless debugging of memory corruption) 
and by providing a delightful, Pythonic development experience, Pyrite is poised 
to become not just a tool you use out of necessity, but one you admire and desire 
to use. These are exactly the qualities that have made Rust highly regarded in 
recent developer surveys, and Pyrite strives to reach that pinnacle by lowering 
the learning curve and keeping the experience enjoyable. 

Ultimately, Pyrite invites you to imagine a programming language where you can 
have it all: the speed of C, the safety of Rust, and the simplicity of Python - 
and then it delivers on that vision. 

But Pyrite's true ambition extends beyond technical features. The path to 
achieving widespread developer adoption requires excellence across the complete
developer experience:

**World-Class Compiler Diagnostics** (Section 2, 14.1, 14.2, 14.5, 14.44)
  Every error becomes a teaching moment. The compiler doesn't just reject code -
  it explains what happened, why it's a problem, and how to fix it. Ownership 
  errors include timeline visualizations. Every error code has detailed 
  explanations accessible via pyritec --explain. Internationalized error 
  messages in 10+ languages make systems programming accessible globally, 
  removing the language barrier that prevents 60% of developers from fully 
  grasping complex concepts like ownership.

**Cost Transparency Tooling** (Section 4.5, 8.13, 15.3, 15.10, 15.31, 15.41, 15.42, 15.46)
  Attributes like @noalloc, @nocopy, and @nosyscall transform Pyrite's "no 
  hidden costs" philosophy from documentation into enforceable contracts. The 
  compiler and linter warn about heap allocations, large copies, and expensive 
  operations. Beyond static analysis, runtime profiling (quarry perf, quarry 
  alloc, quarry pgo) shows actual hot spots. The quarry tune command synthesizes 
  static + runtime data to suggest specific, actionable optimizations with 
  estimated improvements - making performance debugging mechanical rather than 
  mystical. Performance lockfile workflow (Perf.lock) commits baselines to 
  version control and fails CI on regression with root cause analysis (SIMD 
  width changes, inlining decisions, assembly diffs) - transforming "Pyrite is 
  fast" into "Pyrite stays fast forever." Binary size profiling (quarry bloat) 
  extends transparency to code size - critical for embedded systems where every 
  byte matters. Energy profiling (quarry energy) makes power consumption visible - 
  a unique differentiator addressing sustainability, mobile battery life, and IoT 
  constraints. Together, these tools provide complete visibility: performance, 
  memory, size, and energy.

**Frictionless Tooling** (Section 7, 14.2, 14.3, 14.40-14.48)
  Quarry delivers a Cargo-level experience: one command for every task (new, 
  build, run, test, doc, fmt, lint, fix, cost), reproducible builds by default, 
  zero-config formatting, and first-class cross-compilation. Auto-fix applies 
  compiler suggestions instantly. The interactive REPL (pyrite repl) enables 
  instant experimentation with real-time ownership visualization - essential for 
  the "Pythonic" promise. Incremental compilation ensures 15-27x faster rebuilds, 
  making large projects feel responsive. Hot reloading (quarry dev) allows live 
  code updates for game and web development. Deterministic builds enable 
  verifiable supply-chain security. Binary size profiling (quarry bloat) makes 
  embedded constraints visible. Energy profiling (quarry energy) optimizes for 
  sustainability and battery life. The community dashboard (aspirational: quarry.dev/metrics) 
  provides transparent, real-time evidence of Pyrite's performance, safety, and 
  learning characteristics. Great tooling transforms Pyrite from "technically 
  impressive" to "joy to use daily."

**Batteries-Included Standard Library** (Section 8, 14.11, 14.12, 14.13, 14.36, 14.45)
  Ship complete applications using only built-in functionality: collections, 
  JSON/TOML, HTTP client/server, file I/O, networking, time, regex, CLI parsing, 
  and production observability (structured logging, distributed tracing, metrics). 
  Performance-oriented additions include inline storage containers (SmallVec, 
  SmallString, InlineMap) that avoid heap allocation for common small cases, and 
  Mojo-inspired algorithmic helpers (vectorize, parallelize, tile) powered by 
  parameter closures that guarantee zero-cost abstraction. The two-tier closure 
  model (fn[...] compile-time vs fn(...) runtime) makes cost explicit in syntax 
  rather than optimization-dependent, enabling verifiable --no-alloc mode and 
  bulletproof performance claims. Tool-based autotuning (quarry autotune) generates 
  machine-optimal parameters as checked-in constants - achieving Mojo's performance 
  benefits without runtime overhead. Built-in observability (std::log, std::trace, 
  std::metrics) makes Pyrite production-ready for servers and cloud applications, 
  with zero-cost elimination for embedded builds. The stdlib serves as a 
  performance cookbook with "why it's fast" explanations, built-in benchmarks 
  (quarry bench std::sort), and canonical examples showing optimal patterns. 
  Developers evaluate languages by building real projects - Pyrite's comprehensive 
  stdlib enables that evaluation immediately, from embedded firmware to cloud 
  services.

**Learn-By-Doing Ecosystem** (Section 8.14, Section 10, 15.13, 15.40)
  Zero-installation experimentation via (aspirational: play.pyrite-lang.org). Every documentation 
  example is a live playground link. Share code via URL. See compiler errors in 
  real-time with inline explanations. The interactive REPL (pyrite repl) provides 
  local, instant experimentation with ownership visualization - making abstract 
  concepts tangible through direct interaction. Beyond the playground, quarry 
  learn provides Rustlings-style interactive exercises with progressive hints and 
  synthesis projects - transforming the learning curve from "insurmountable wall" 
  to "guided path." The combination of Playground (browser), REPL (local), and 
  quarry learn (structured) creates a complete learning ecosystem that meets 
  developers wherever they are. This accelerates learning, enables social sharing, 
  and amplifies advocacy.

**Runtime Verification and Reliability** (Section 8.9, 15.33, 15.49, 15.50)
  Coverage-guided fuzzing (quarry fuzz) automatically discovers edge cases that 
  unit tests miss, generating regression test cases from crash-inducing inputs. 
  Sanitizers (AddressSanitizer, ThreadSanitizer, UndefinedBehaviorSanitizer, 
  MemorySanitizer) catch memory errors and data races at runtime with zero cost 
  in production builds. Future Miri-equivalent interpreter will provide 
  exhaustive undefined behavior detection for auditing unsafe code. Design by 
  Contract (@requires, @ensures, @invariant) extends verification from memory 
  safety to logical correctness, enabling compile-time and runtime checking of 
  preconditions and postconditions. Formal semantics specification provides 
  mathematical rigor for certification processes (DO-178C Level A, IEC 62304, 
  ISO 26262) and enables external verification tools. This multi-layered 
  verification approach - compile-time ownership, runtime sanitizers, contract 
  checking, and formal verification - makes Pyrite suitable for the most 
  demanding safety-critical applications. This is table-stakes for serious 
  systems languages - Chromium found 3,000+ bugs with ASan that code review 
  missed. Pyrite makes these industry-standard tools first-class citizens while 
  adding unique correctness guarantees through contracts and formal methods.

**Clear Positioning** (Section 11)
  "Pyrite is what Python would be if it were a systems language-readable, 
  explicit, and memory-safe by default, compiling to C-speed binaries with no 
  runtime." Clear messaging attracts the right users and sets honest 
  expectations.

These elements - compiler UX, tooling, stdlib, learning resources, and 
positioning - are what separate languages that are admired in theory from languages 
that are loved in practice. Rust proved this formula: technical excellence + 
exceptional developer experience = language that achieves widespread adoption.

Pyrite follows the same roadmap with an additional advantage: learning from 
Rust's decade of success, Python's readability principles, Zig's transparency 
ethos, and Mojo's performance innovations - taking the best and avoiding their 
pitfalls. By combining proven approaches and addressing their learning curve 
challenges, Pyrite is positioned to achieve its ultimate goal: becoming the 
widely adopted and desired systems programming language of its era.

**The "Most Loved + Most Performant" Formula**

Building on Pyrite's existing strengths (tool-enforced Core subset, 
performance-as-correctness with blame tracking, Mojo-style algorithmic helpers), 
the specification includes these critical pieces for sustained excellence:

1. **Performance Lockfile (Perf.lock):** Prevents the "death by 1000 cuts" 
   performance decay that plagues long-lived projects. CI enforcement with root 
   cause analysis (assembly diffs, SIMD width changes, inlining decisions) means 
   regressions can't merge. This transforms cost transparency from measurement 
   to enforcement - the difference between "Pyrite is fast" and "Pyrite stays fast 
   forever."

2. **Runtime Verification (fuzzing + sanitizers):** Complements compile-time 
   safety with runtime bug detection. Catches edge cases (fuzzing), memory 
   errors (ASan), data races (TSan), and undefined behavior (UBSan) that static 
   analysis cannot. This is table-stakes for serious systems languages and 
   required for safety certification.

3. **Enhanced Introspection (layout + aliasing):** Makes invisible performance 
   concepts (cache lines, padding, aliasing assumptions) concrete and actionable. 
   Completes the "intuitive memory model" story by showing not just WHAT types 
   are, but HOW they're arranged and WHEN the compiler can optimize.

4. **Tool-Based Autotuning:** Achieves Mojo's machine-optimal parameter benefits 
   without runtime overhead. Generates checked-in constants (zero magic, fully 
   inspectable), avoiding the pitfalls that caused Mojo to deprecate their 
   runtime autotuning.

5. **Embedded-First Strategy:** Prioritizes the most differentiated domain where 
   Pyrite's advantages (provable zero-heap, blame tracking, zero runtime) shine 
   brightest. Establishes trust in the hardest environment first, then expands 
   to servers â†’ numerical â†’ GPU.

These additions - all high ROI, all aligned with Pyrite's core philosophy of 
explicitness and transparency - complete the formula for sustained excellence. 
Not just "admired today" but "admired forever" through enforced performance 
stability, runtime verification, and systematic optimization pathways.

With Pyrite, we believe we've forged a tool that empowers programmers to build 
more ambitious, reliable software with a smile on their face - and keeps that 
software fast and correct for the long haul.

**Comprehensive Excellence: The Complete Vision**

This specification represents the complete vision for achieving widespread developer 
adoption, with all critical pieces for developer excellence:

**Interactive Learning** (REPL + Playground + quarry learn)
  The interactive REPL fills the most glaring gap - Python developers expect 
  instant experimentation, and the REPL delivers with real-time ownership 
  visualization. Combined with the browser Playground and structured quarry learn 
  exercises, Pyrite offers the most comprehensive learning ecosystem of any 
  systems language.

**Complete Transparency** (Cost + Size + Energy + Dashboard)
  Beyond runtime performance, Pyrite makes binary size visible (quarry bloat for 
  embedded), energy consumption visible (quarry energy for sustainability), and 
  ecosystem health visible (community dashboard with real-time metrics). No 
  systems language offers this level of transparency across all dimensions.

**Uncompromising Security** (Audit + Vet + Sign + Reproducible + Formal)
  Supply-chain security is complete: vulnerability scanning, dependency review, 
  cryptographic signing, reproducible builds, and formal semantics for 
  verification. This addresses the critical trust requirements for aerospace, 
  medical, government, and enterprise deployments.

**Production Readiness** (Observability + Incremental + Hot-Reload)
  Built-in observability makes Pyrite credible for production servers. 
  Incremental compilation makes large projects feel fast. Hot reloading makes 
  iteration delightful. These are table-stakes for modern development - Pyrite 
  delivers them all.

**Global Accessibility** (Internationalization + Evidence)
  Internationalized error messages break down language barriers for 60% of the 
  world's developers. The public metrics dashboard provides objective evidence of 
  Pyrite's claims, making advocacy data-driven rather than subjective.

**Correctness at All Levels** (Ownership + Contracts + Formal Methods)
  Memory safety through ownership, logical correctness through contracts, and 
  mathematical rigor through formal semantics. Pyrite is the first systems 
  language to integrate all three verification approaches seamlessly.

The result is not just a programming language, but a **complete developer 
platform** that excels across every dimension:

  âœ“ Safety (ownership + contracts + formal verification)
  âœ“ Performance (zero-cost abstractions + profiling + optimization)
  âœ“ Learning (REPL + Playground + interactive exercises + native language errors)
  âœ“ Productivity (incremental builds + hot reload + great diagnostics)
  âœ“ Transparency (cost + size + energy + public metrics)
  âœ“ Security (audit + vet + sign + reproducible + formal semantics)
  âœ“ Production (observability + testing + debugging + certification)
  âœ“ Global (internationalization + accessibility + evidence)

Every feature compounds the others: interactive learning accelerates adoption, 
transparency builds trust, security enables certification, production features 
enable real deployments, global reach multiplies community, formal methods enable 
verification. This represents a comprehensive approach to achieving widespread developer 
adoption - not just technical excellence, but **complete excellence across the entire developer 
experience and lifecycle.**

From the first "Hello World" in the REPL to certified deployment in safety-
critical systems, from hobby projects to hyperscale cloud services, from 
32-kilobyte microcontrollers to GPU-accelerated data centers, Pyrite delivers 
consistent safety, predictable performance, and delightful developer experience.

Pyrite provides the tools and features needed to build reliable, performant systems 
software with confidence.